{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a738ee4d-62e5-4e1d-aee5-68dc7f494617",
   "metadata": {},
   "source": [
    "# Snowline performance assessment\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022/2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d809a61d-618f-44f7-a82c-7946104a000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import geopandas as gpd\n",
    "import glob\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import rioxarray as rxr\n",
    "import rasterio as rio\n",
    "from scipy import stats\n",
    "from shapely import wkt\n",
    "from ast import literal_eval\n",
    "from shapely.geometry import Point, MultiLineString, LineString, shape, MultiPolygon, Polygon\n",
    "from shapely.ops import split, unary_union, polygonize, nearest_points\n",
    "import skimage.io\n",
    "from skimage import feature\n",
    "import sys\n",
    "import wxee as wx\n",
    "import xarray as xr\n",
    "import rioxarray as rxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc578f-bec3-4e6f-930d-633b083db83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to glacier-snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "# path to study-sites\n",
    "study_sites_path = '/Volumes/LaCie/raineyaberle/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission'\n",
    "# path to snowline-package/\n",
    "snowlines_obs_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/manually-digitized-snowlines/' \n",
    "usgs_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS'\n",
    "\n",
    "# names of study sites\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "# path for output figures\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "\n",
    "# add path to functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import pipeline_utils as f\n",
    "\n",
    "# load dataset dictionary\n",
    "dataset_dict = json.load(open(os.path.join(base_path, 'inputs-outputs/datasets_characteristics.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2870334a-841b-4aa2-8faa-1e1178f97ffc",
   "metadata": {},
   "source": [
    "## PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9853ff3d-0006-46c9-8f89-3d2a7ae6734d",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Check if output file already exists\n",
    "results_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_PlanetScope.csv')\n",
    "if os.path.exists(results_fn):\n",
    "    print('Performance stats already exist in file, loading...')\n",
    "    results_df = pd.read_csv(results_fn)\n",
    "else:\n",
    "\n",
    "    # -----Load trained classifier and feature columns\n",
    "    clf_fn = os.path.join(base_path, 'inputs-outputs', 'PlanetScope_classifier_all_sites.joblib')\n",
    "    clf = load(clf_fn)\n",
    "    feature_cols_fn = os.path.join(base_path, 'inputs-outputs', 'PlanetScope_feature_columns.json')\n",
    "    feature_cols = json.load(open(feature_cols_fn))\n",
    "    dataset = 'PlanetScope'\n",
    "\n",
    "    # -----Loop through sites\n",
    "    results_df = pd.DataFrame()\n",
    "    for i, site_name in enumerate(site_names):    \n",
    "    \n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "        \n",
    "        # define path to raw images\n",
    "        im_path = os.path.join(snowlines_obs_path, site_name, 'images')\n",
    "    \n",
    "        # load observed snow line shapefile names\n",
    "        sl_obs_fns = glob.glob(os.path.join(snowlines_obs_path, site_name , 'snowlines', '*.shp'))\n",
    "        sl_obs_fns = sorted(sl_obs_fns) # sort chronologically\n",
    "        \n",
    "        # aoi\n",
    "        aoi_fn = glob.glob(os.path.join(usgs_path, 'glacierBoundaries', site_name, 'shapefile', '*.shp'))[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        \n",
    "        # dem\n",
    "        dem_fn = glob.glob(os.path.join(usgs_path, 'dems', site_name, '*.tif'))[-1]\n",
    "        dem = xr.open_dataset(dem_fn)\n",
    "        dem = dem.squeeze().rename({'band_data': 'elevation'})\n",
    "        \n",
    "        # define output folders for classified images and snowline estimates\n",
    "        im_classified_path = os.path.join(snowlines_obs_path, site_name, 'classified')\n",
    "        snowlines_est_path = os.path.join(snowlines_obs_path, site_name, 'snowlines_est')\n",
    "    \n",
    "        # initialize observed snowline elevations\n",
    "        sl_obs_elevs = np.zeros(len(sl_obs_fns)) \n",
    "\n",
    "        # loop through observed snow lines\n",
    "        for sl_obs_fn in sl_obs_fns:\n",
    "    \n",
    "            # -----Load datasets\n",
    "            ### Observed\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            # drop None geometry columns\n",
    "            sl_obs = sl_obs.drop(columns=['id']).dropna().reset_index(drop=True)\n",
    "            # reproject observed snow line to UTM\n",
    "            sl_obs_UTM = sl_obs.to_crs(f'EPSG:{aoi.crs.to_epsg()}')\n",
    "            # extract date from filename\n",
    "            date = os.path.basename(sl_obs_fn).split(site_name+'_')[1][0:11]\n",
    "            datetime = np.datetime64(f'{date[0:4]}-{date[4:6]}-{date[6:8]}T{date[9:11]}:00:00')\n",
    "            print(f'\\n{datetime}')\n",
    "            \n",
    "            ### Estimated      \n",
    "            # open raw image of the same date\n",
    "            im_fn = glob.glob(os.path.join(im_path, date.replace('-','')[0:8] + '*_adj.tif'))[0] # define file name\n",
    "            im = rxr.open_rasterio(im_fn) # open image as xarray.DataArray\n",
    "            # create xarray.Dataset\n",
    "            im_adj = xr.Dataset(\n",
    "                data_vars=dict(\n",
    "                    Blue=(['y', 'x'], im.data[0]),\n",
    "                    Green=(['y', 'x'], im.data[1]),\n",
    "                    Red=(['y', 'x'], im.data[2]),\n",
    "                    NIR=(['y', 'x'], im.data[3])\n",
    "                ),\n",
    "                coords=im.coords,\n",
    "                attrs=dict(\n",
    "                    no_data_values=np.nan,\n",
    "                    image_scalar=1\n",
    "                )\n",
    "            )\n",
    "            im_adj = xr.where(im_adj != 0, im_adj/1e4, np.nan)\n",
    "            im_adj = im_adj.rio.write_crs('EPSG:' + str(im.rio.crs.to_epsg()))\n",
    "            # add NDSI band\n",
    "            im_adj['NDSI'] = ((im_adj[dataset_dict[dataset]['NDSI_bands'][0]] - im_adj[dataset_dict[dataset]['NDSI_bands'][1]])\n",
    "                              / (im_adj[dataset_dict[dataset]['NDSI_bands'][0]] + im_adj[dataset_dict[dataset]['NDSI_bands'][1]]))\n",
    "            # add time dimension\n",
    "            im_adj = im_adj.expand_dims({'time': [datetime]})\n",
    "            # classify image\n",
    "            im_classified_fn = f'{site_name}_{str(date)}_PlanetScope_classified.nc'\n",
    "            if os.path.exists(os.path.join(im_classified_path, im_classified_fn)):\n",
    "                print('Classified image already exists in file, loading...')\n",
    "                im_classified = xr.open_dataset(os.path.join(im_classified_path, im_classified_fn))\n",
    "                # remove no data values\n",
    "                im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "                im_classified = im_classified.rio.write_crs(f\"EPSG:{dem.rio.crs.to_epsg()}\")\n",
    "                # im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "                # im_classified = im_classified.rio.reproject(f'EPSG:{dem.rio.crs.to_epsg()}')\n",
    "            else:  \n",
    "                im_classified = f.classify_image(im_adj, clf, feature_cols, aoi, dataset_dict, dataset,\n",
    "                                                 im_classified_fn, im_classified_path, verbose=False)\n",
    "            # delineate snowline\n",
    "            sl_est_fn = os.path.join(snowlines_est_path, site_name + '_' + date + '_PlanetScope_snowline.csv')\n",
    "            if os.path.exists(sl_est_fn):\n",
    "                print('Snowline already exists in file, loading...')\n",
    "                sl_est = pd.read_csv(sl_est_fn)\n",
    "                sl_est['datetime'] = pd.to_datetime([f'{date[0:4]}-{date[4:6]}-{date[6:8]}T{date[9:11]}:00:00' \n",
    "                                                     for date in sl_est['datetime']])\n",
    "            else:\n",
    "                sl_est = f.delineate_snowline(im_classified, site_name, aoi, dem, dataset_dict, dataset,\n",
    "                                              str(datetime), sl_est_fn, im_classified_path, figures_out_path, \n",
    "                                              plot_results=False, im_xr=im_adj, verbose=False)\n",
    "                   \n",
    "            # check if snowlines were found\n",
    "            if type(sl_est['snowlines_coords_X'].values[0])==str:\n",
    "                print('No snowline coordinates detected, skipping...')\n",
    "            else:\n",
    "                # -----Sample elevations at observed snowline points\n",
    "                xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "                ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "                sl_obs_elev = [dem.sel(x=x, y=y, method='nearest')['elevation'].data for x,y in list(zip(xsamp, ysamp))]\n",
    "    \n",
    "                # -----Split line depending on distance between points\n",
    "                max_dist = 100 # m\n",
    "                line = sl_obs_UTM.geometry[0]\n",
    "                first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "                points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "                isplit = [0] # point indices where to split the line\n",
    "                for i, p in enumerate(points):\n",
    "                    if i!=0:\n",
    "                        dist = p.distance(points[i-1])\n",
    "                        if dist > max_dist:\n",
    "                            isplit.append(i)\n",
    "                isplit.append(len(points)) # add ending point to complete the last line\n",
    "                line_split = [] # initialize split lines\n",
    "                # loop through split indices\n",
    "                if isplit:\n",
    "                    for i, p in enumerate(isplit[:-1]):\n",
    "                        if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                            line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "                else:\n",
    "                    line_split = line\n",
    "        \n",
    "                # -----Regrid the observed snowlines to equal spacing\n",
    "                dx = 30 # point spacing\n",
    "                points_regrid = []\n",
    "                for line in line_split:\n",
    "                    distances = np.arange(0, line.length, dx)\n",
    "                    line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                    # filter points outside the aoi\n",
    "                    Iaoi = np.where(np.array([p.within(aoi.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                    points_aoi = [line_points[i] for i in Iaoi]\n",
    "                    points_regrid = points_regrid + [p for p in points_aoi]\n",
    "    \n",
    "                # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "                sl_est['geometry'] = LineString(list(zip(sl_est['snowlines_coords_X'].values[0], sl_est['snowlines_coords_Y'].values[0])))\n",
    "                distances = np.zeros(len(points_regrid))\n",
    "                for i, p in enumerate(points_regrid):\n",
    "                    # find nearest point\n",
    "                    nearest_point = nearest_points(sl_est['geometry'][0], p)[0]\n",
    "                    # calculate distance between points\n",
    "                    distances[i] = p.distance(nearest_point)\n",
    "                \n",
    "                # -----Display results\n",
    "                plt.figure(figsize=(8, 8))\n",
    "                plt.imshow(np.dstack([im_adj['Red'].data[0], im_adj['Green'].data[0], im_adj['Blue'].data[0]]), \n",
    "                           extent=(np.min(im_adj.x.data), np.max(im_adj.x.data), np.min(im_adj.y.data), np.max(im_adj.y.data)))\n",
    "                plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "                         [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "                plt.plot(sl_est['snowlines_coords_X'][0], sl_est['snowlines_coords_Y'][0], '.m', label='estimated')\n",
    "                plt.legend(loc='upper right')\n",
    "                plt.grid()\n",
    "                plt.title(datetime)\n",
    "                plt.show()\n",
    "    \n",
    "                # compile results in df\n",
    "                result_df = pd.DataFrame({'study_site': site_name, \n",
    "                                          'datetime': datetime, \n",
    "                                          'snowline_obs': [points_regrid], \n",
    "                                          'snowline_obs_elev_median': np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_est': [sl_est['geometry'][0]], \n",
    "                                          'snowline_est_elev_median': sl_est['snowline_elevs_median_m'],\n",
    "                                          'snowline_elev_median_differences': sl_est['snowline_elevs_median_m'] - np.nanmedian(sl_obs_elev),\n",
    "                                          'snowline_distances': [distances],\n",
    "                                          'snowline_distance_median': np.nanmedian(distances)})\n",
    "    \n",
    "                # concatenate to results_df\n",
    "                results_df = pd.concat([results_df, result_df])\n",
    "                \n",
    "        print(' ')\n",
    "            \n",
    "    # -----Save to file\n",
    "    results_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_PlanetScope.csv')\n",
    "    results_df.to_csv(results_fn, index=False)\n",
    "    print('Performance metrics saved to file: '+results_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2462620d-1e7d-44e0-9557-e851f872506a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df['snowline_elev_median_differences'])\n",
    "ax[1].set_title('Median snowline elevation differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['PlanetScope'],\n",
    "                                 'ground distance P0 [m]': np.nanpercentile(results_df['snowline_distance_median'], 0),\n",
    "                                 'ground distance P25 [m]': np.nanpercentile(results_df['snowline_distance_median'], 25),\n",
    "                                 'ground distance P50 [m]': np.nanpercentile(results_df['snowline_distance_median'], 50),\n",
    "                                 'ground distance P75 [m]': np.nanpercentile(results_df['snowline_distance_median'], 75),\n",
    "                                 'ground distance P100 [m]': np.nanpercentile(results_df['snowline_distance_median'], 100),\n",
    "                                 'elevation difference P0 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 0),\n",
    "                                 'elevation difference P25 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 25),\n",
    "                                 'elevation difference P50 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 50),\n",
    "                                 'elevation difference P75 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 75),\n",
    "                                 'elevation difference P100 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 100),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('PlanetScope snowline performance')\n",
    "print('----------')\n",
    "print(\"Ground distance: median = \" + str(np.round(results_stats_df['ground distance P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['ground distance P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['ground distance P75 [m]'][0],2)) + \" m\")\n",
    "print(\"Median elevation difference: median = \" + str(np.round(results_stats_df['elevation difference P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['elevation difference P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['elevation difference P75 [m]'][0],2)) + \" m\")\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_stats_PlanetScope.csv')\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: ', results_stats_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291b549f-0eb6-4d89-b9a5-c4100d340c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "-70.53 + 0.93"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe641225-4ca9-4c8d-a6c1-e1aa16a0793b",
   "metadata": {},
   "source": [
    "## Landsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b465145-6729-4af8-84ba-857c5f48a3b7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Check if output file already exists\n",
    "results_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_Landsat.csv')\n",
    "if os.path.exists(results_fn):\n",
    "    print('Performance stats already exist in file, loading...')\n",
    "    results_df = pd.read_csv(results_fn)\n",
    "else:\n",
    "\n",
    "    # -----Loop through sites\n",
    "    results_df = pd.DataFrame()\n",
    "    for i, site_name in enumerate(site_names):    \n",
    "    \n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # aoi\n",
    "        aoi_fn = glob.glob(os.path.join(usgs_path, 'glacierBoundaries', site_name, 'shapefile', '*.shp'))[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        \n",
    "        # load dem\n",
    "        dem_fn = glob.glob(os.path.join(usgs_path, 'DEMs', site_name, '*.tif'))[-1]\n",
    "        dem = xr.open_dataset(dem_fn)\n",
    "        dem = dem.squeeze().rename({'band_data': 'elevation'})\n",
    "        \n",
    "        # load observed snowlines file names\n",
    "        sl_obs_path = os.path.join(snowlines_obs_path, site_name, 'snowlines')\n",
    "        sl_obs_fns = sorted(glob.glob(os.path.join(sl_obs_path, '*.shp')))\n",
    "    \n",
    "        # load estimated snowlines \n",
    "        sl_est_fn = os.path.join(study_sites_path, site_name, f'{site_name}_snow_cover_stats.csv')\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        # subset to dataset\n",
    "        sl_est = sl_est.loc[sl_est['source']=='Landsat']\n",
    "        sl_est['datetime'] = pd.to_datetime(sl_est['datetime'])\n",
    "        sl_est['snowlines_coords_X'] = np.array(sl_est['snowlines_coords_X'].apply(literal_eval))\n",
    "        sl_est['snowlines_coords_Y'] = np.array(sl_est['snowlines_coords_Y'].apply(literal_eval))\n",
    "        sl_est.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        # Iterate over observed snowlines file names\n",
    "        for sl_obs_fn in sl_obs_fns:\n",
    "            # grab date from file name\n",
    "            date = os.path.basename(sl_obs_fn).split(site_name + '_')[1].split('_adj')[0] \n",
    "            date = np.datetime64(f'{date[0:4]}-{date[4:6]}-{date[6:8]}T{date[9:11]}:00:00') \n",
    "            print(f'\\n{date}')\n",
    "\n",
    "            # load observed snowline\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            sl_obs_UTM = sl_obs.to_crs(f'EPSG:{aoi.crs.to_epsg()}')\n",
    "    \n",
    "            # identify the closest estimated snowline in time\n",
    "            sl_est['dt'] = np.abs(sl_est['datetime'] - date)\n",
    "            sl_est_date = sl_est.loc[sl_est['dt']==sl_est['dt'].min()]\n",
    "            sl_est_date['geometry'] = LineString(list(zip(sl_est_date['snowlines_coords_X'].values[0], \n",
    "                                                          sl_est_date['snowlines_coords_Y'].values[0])))\n",
    "            if sl_est_date['dt'].values[0] > np.timedelta64(10, 'D'):\n",
    "                print('No observations within one week, skipping...')\n",
    "                continue\n",
    "    \n",
    "            if len(sl_est_date['snowlines_coords_X'].values[0]) < 1:\n",
    "                print('No snowline detected for date, skipping...')\n",
    "                continue\n",
    "               \n",
    "            # -----Sample elevations at observed snowline points\n",
    "            xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "            ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "            sl_obs_elev = [dem.sel(x=x, y=y, method='nearest')['elevation'].data for x,y in list(zip(xsamp, ysamp))]\n",
    "                    \n",
    "            # -----Split line depending on distance between points\n",
    "            max_dist = 100 # m\n",
    "            line = sl_obs_UTM.geometry[0]\n",
    "            first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "            points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "            isplit = [0] # point indices where to split the line\n",
    "            for i, p in enumerate(points):\n",
    "                if i!=0:\n",
    "                    dist = p.distance(points[i-1])\n",
    "                    if dist > max_dist:\n",
    "                        isplit.append(i)\n",
    "            isplit.append(len(points)) # add ending point to complete the last line\n",
    "            line_split = [] # initialize split lines\n",
    "            # loop through split indices\n",
    "            if isplit:\n",
    "                for i, p in enumerate(isplit[:-1]):\n",
    "                    if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                        line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "            else:\n",
    "                line_split = line\n",
    "                        \n",
    "            #-----Regrid the observed snowlines to equal spacing\n",
    "            dx = 30 # point spacing\n",
    "            points_regrid = []\n",
    "            for line in line_split:\n",
    "                distances = np.arange(0, line.length, dx)\n",
    "                line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                # filter points outside the aoi\n",
    "                Iaoi = np.where(np.array([p.within(aoi.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                points_aoi = [line_points[i] for i in Iaoi]\n",
    "                points_regrid = points_regrid + [p for p in points_aoi]\n",
    "                    \n",
    "            # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "            distances = np.zeros(len(points_regrid))\n",
    "            for i, p in enumerate(points_regrid):\n",
    "                # find nearest point\n",
    "                nearest_point = nearest_points(sl_est_date['geometry'].values[0], p)[0]\n",
    "                # calculate distance between points\n",
    "                distances[i] = p.distance(nearest_point)\n",
    "        \n",
    "            #-----Display results\n",
    "            # plt.figure(figsize=(8, 8))\n",
    "            # plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "            #          [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "            # plt.plot(*sl_est_date['geometry'].values[0].coords.xy, '.m', label='estimated')\n",
    "            # plt.legend(loc='upper right')\n",
    "            # plt.grid()\n",
    "            # plt.title(date)\n",
    "            # plt.show()\n",
    "    \n",
    "            # compile results in df\n",
    "            result_df = pd.DataFrame({'study_site': [site_name], \n",
    "                                      'snowline_obs_date': [str(date)], \n",
    "                                      'snowline_est_date': [sl_est_date['datetime'].values[0]],\n",
    "                                      'snowline_obs': [points_regrid], \n",
    "                                      'snowline_obs_elev_median': [np.nanmedian(sl_obs_elev)],\n",
    "                                      'snowline_est': [sl_est_date['geometry'].values[0]], \n",
    "                                      'snowline_est_elev_median': [sl_est_date['snowline_elevs_median_m'].values[0]],\n",
    "                                      'snowline_elev_median_differences': [sl_est_date['snowline_elevs_median_m'].values[0] - np.nanmedian(sl_obs_elev)],\n",
    "                                      'snowline_distances': [distances],\n",
    "                                      'snowline_distance_median': [np.nanmedian(distances)]})\n",
    "            # concatenate to results_df\n",
    "            results_df = pd.concat([results_df, result_df])\n",
    "\n",
    "    # Save to file\n",
    "    results_df.to_csv(results_fn, index=False)\n",
    "    print('Performance metrics saved to file:', results_fn) \n",
    "            \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a864f6e-0480-4ee8-883e-4ea21a0cc9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df['snowline_elev_median_differences'])\n",
    "ax[1].set_title('Median snowline elevation differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['Landsat'],\n",
    "                                 'ground distance P0 [m]': np.nanpercentile(results_df['snowline_distance_median'], 0),\n",
    "                                 'ground distance P25 [m]': np.nanpercentile(results_df['snowline_distance_median'], 25),\n",
    "                                 'ground distance P50 [m]': np.nanpercentile(results_df['snowline_distance_median'], 50),\n",
    "                                 'ground distance P75 [m]': np.nanpercentile(results_df['snowline_distance_median'], 75),\n",
    "                                 'ground distance P100 [m]': np.nanpercentile(results_df['snowline_distance_median'], 100),\n",
    "                                 'elevation difference P0 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 0),\n",
    "                                 'elevation difference P25 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 25),\n",
    "                                 'elevation difference P50 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 50),\n",
    "                                 'elevation difference P75 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 75),\n",
    "                                 'elevation difference P100 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 100),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('Landsat snowline performance')\n",
    "print('----------')\n",
    "print(\"Ground distance: median = \" + str(np.round(results_stats_df['ground distance P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['ground distance P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['ground distance P75 [m]'][0],2)) + \" m\")\n",
    "print(\"Median elevation difference: median = \" + str(np.round(results_stats_df['elevation difference P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['elevation difference P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['elevation difference P75 [m]'][0],2)) + \" m\")\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_stats_Landsat.csv')\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: ', results_stats_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746a297-3053-4595-89de-f6e60e6d8cf7",
   "metadata": {},
   "source": [
    "## Sentinel-2 SR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a227f17-27e8-4c0b-a6a1-d9c6b5a92d7b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Check if output file already exists\n",
    "results_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_Sentinel-2_SR.csv')\n",
    "if os.path.exists(results_fn):\n",
    "    print('Performance stats already exist in file, loading...')\n",
    "    results_df = pd.read_csv(results_fn)\n",
    "else:\n",
    "\n",
    "    # -----Loop through sites\n",
    "    results_df = pd.DataFrame()\n",
    "    for i, site_name in enumerate(site_names):    \n",
    "    \n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # aoi\n",
    "        aoi_fn = glob.glob(os.path.join(usgs_path, 'glacierBoundaries', site_name, 'shapefile', '*.shp'))[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        \n",
    "        # load dem\n",
    "        dem_fn = glob.glob(os.path.join(usgs_path, 'DEMs', site_name, '*.tif'))[-1]\n",
    "        dem = xr.open_dataset(dem_fn)\n",
    "        dem = dem.squeeze().rename({'band_data': 'elevation'})\n",
    "        \n",
    "        # load observed snowlines file names\n",
    "        sl_obs_path = os.path.join(snowlines_obs_path, site_name, 'snowlines')\n",
    "        sl_obs_fns = sorted(glob.glob(os.path.join(sl_obs_path, '*.shp')))\n",
    "    \n",
    "        # load estimated snowlines \n",
    "        sl_est_fn = os.path.join(study_sites_path, site_name, f'{site_name}_snow_cover_stats.csv')\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        # subset to dataset\n",
    "        sl_est = sl_est.loc[sl_est['source']=='Sentinel-2_SR']\n",
    "        sl_est['datetime'] = pd.to_datetime(sl_est['datetime'])\n",
    "        sl_est['snowlines_coords_X'] = np.array(sl_est['snowlines_coords_X'].apply(literal_eval))\n",
    "        sl_est['snowlines_coords_Y'] = np.array(sl_est['snowlines_coords_Y'].apply(literal_eval))\n",
    "        sl_est.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        # Iterate over observed snowlines file names\n",
    "        for sl_obs_fn in sl_obs_fns:\n",
    "            # grab date from file name\n",
    "            date = os.path.basename(sl_obs_fn).split(site_name + '_')[1].split('_adj')[0] \n",
    "            date = np.datetime64(f'{date[0:4]}-{date[4:6]}-{date[6:8]}T{date[9:11]}:00:00') \n",
    "            print(f'\\n{date}')\n",
    "\n",
    "            # load observed snowline\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            sl_obs_UTM = sl_obs.to_crs(f'EPSG:{aoi.crs.to_epsg()}')\n",
    "    \n",
    "            # identify the closest estimated snowline in time\n",
    "            sl_est['dt'] = np.abs(sl_est['datetime'] - date)\n",
    "            sl_est_date = sl_est.loc[sl_est['dt']==sl_est['dt'].min()]\n",
    "            sl_est_date['geometry'] = LineString(list(zip(sl_est_date['snowlines_coords_X'].values[0], \n",
    "                                                          sl_est_date['snowlines_coords_Y'].values[0])))\n",
    "            if sl_est_date['dt'].values[0] > np.timedelta64(7, 'D'):\n",
    "                print('No observations within one week, skipping...')\n",
    "                continue\n",
    "    \n",
    "            if len(sl_est_date['snowlines_coords_X'].values[0]) < 1:\n",
    "                print('No snowline detected for date, skipping...')\n",
    "                continue\n",
    "               \n",
    "            # -----Sample elevations at observed snowline points\n",
    "            xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "            ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "            sl_obs_elev = [dem.sel(x=x, y=y, method='nearest')['elevation'].data for x,y in list(zip(xsamp, ysamp))]\n",
    "                    \n",
    "            # -----Split line depending on distance between points\n",
    "            max_dist = 100 # m\n",
    "            line = sl_obs_UTM.geometry[0]\n",
    "            first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "            points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "            isplit = [0] # point indices where to split the line\n",
    "            for i, p in enumerate(points):\n",
    "                if i!=0:\n",
    "                    dist = p.distance(points[i-1])\n",
    "                    if dist > max_dist:\n",
    "                        isplit.append(i)\n",
    "            isplit.append(len(points)) # add ending point to complete the last line\n",
    "            line_split = [] # initialize split lines\n",
    "            # loop through split indices\n",
    "            if isplit:\n",
    "                for i, p in enumerate(isplit[:-1]):\n",
    "                    if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                        line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "            else:\n",
    "                line_split = line\n",
    "                        \n",
    "            #-----Regrid the observed snowlines to equal spacing\n",
    "            dx = 30 # point spacing\n",
    "            points_regrid = []\n",
    "            for line in line_split:\n",
    "                distances = np.arange(0, line.length, dx)\n",
    "                line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                # filter points outside the aoi\n",
    "                Iaoi = np.where(np.array([p.within(aoi.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                points_aoi = [line_points[i] for i in Iaoi]\n",
    "                points_regrid = points_regrid + [p for p in points_aoi]\n",
    "                    \n",
    "            # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "            distances = np.zeros(len(points_regrid))\n",
    "            for i, p in enumerate(points_regrid):\n",
    "                # find nearest point\n",
    "                nearest_point = nearest_points(sl_est_date['geometry'].values[0], p)[0]\n",
    "                # calculate distance between points\n",
    "                distances[i] = p.distance(nearest_point)\n",
    "        \n",
    "            #-----Display results\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "                     [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "            plt.plot(*sl_est_date['geometry'].values[0].coords.xy, '.m', label='estimated')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.grid()\n",
    "            plt.title(date)\n",
    "            plt.show()\n",
    "    \n",
    "            # compile results in df\n",
    "            result_df = pd.DataFrame({'study_site': [site_name], \n",
    "                                      'snowline_obs_date': [str(date)], \n",
    "                                      'snowline_est_date': [sl_est_date['datetime'].values[0]],\n",
    "                                      'snowline_obs': [points_regrid], \n",
    "                                      'snowline_obs_elev_median': [np.nanmedian(sl_obs_elev)],\n",
    "                                      'snowline_est': [sl_est_date['geometry'].values[0]], \n",
    "                                      'snowline_est_elev_median': [sl_est_date['snowline_elevs_median_m'].values[0]],\n",
    "                                      'snowline_elev_median_differences': [sl_est_date['snowline_elevs_median_m'].values[0] - np.nanmedian(sl_obs_elev)],\n",
    "                                      'snowline_distances': [distances],\n",
    "                                      'snowline_distance_median': [np.nanmedian(distances)]})\n",
    "            # concatenate to results_df\n",
    "            results_df = pd.concat([results_df, result_df])\n",
    "\n",
    "    # Save to file\n",
    "    results_df.to_csv(results_fn, index=False)\n",
    "    print('Performance metrics saved to file:', results_fn) \n",
    "            \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a598e62-39fb-47e5-a746-0bc1dcb88005",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df['snowline_elev_median_differences'])\n",
    "ax[1].set_title('Median snowline elevation differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['Sentinel-2_SR'],\n",
    "                                 'ground distance P0 [m]': np.nanpercentile(results_df['snowline_distance_median'], 0),\n",
    "                                 'ground distance P25 [m]': np.nanpercentile(results_df['snowline_distance_median'], 25),\n",
    "                                 'ground distance P50 [m]': np.nanpercentile(results_df['snowline_distance_median'], 50),\n",
    "                                 'ground distance P75 [m]': np.nanpercentile(results_df['snowline_distance_median'], 75),\n",
    "                                 'ground distance P100 [m]': np.nanpercentile(results_df['snowline_distance_median'], 100),\n",
    "                                 'elevation difference P0 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 0),\n",
    "                                 'elevation difference P25 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 25),\n",
    "                                 'elevation difference P50 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 50),\n",
    "                                 'elevation difference P75 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 75),\n",
    "                                 'elevation difference P100 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 100),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('Sentinel-2 SR snowline performance')\n",
    "print('----------')\n",
    "print(\"Ground distance: median = \" + str(np.round(results_stats_df['ground distance P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['ground distance P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['ground distance P75 [m]'][0],2)) + \" m\")\n",
    "print(\"Median elevation difference: median = \" + str(np.round(results_stats_df['elevation difference P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['elevation difference P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['elevation difference P75 [m]'][0],2)) + \" m\")\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_stats_Sentinel-2_SR.csv')\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: ', results_stats_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5b691-c0a5-4aa1-a1fc-9635b13fcfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "88/10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab980c1-34ef-45f7-a064-1345a1517da4",
   "metadata": {},
   "source": [
    "## Sentinel-2 TOA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcb1b80-c11b-4f62-86b0-06393f206cd2",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Check if output file already exists\n",
    "results_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_Sentinel-2_TOA.csv')\n",
    "if os.path.exists(results_fn):\n",
    "    print('Performance stats already exist in file, loading...')\n",
    "    results_df = pd.read_csv(results_fn)\n",
    "else:\n",
    "\n",
    "    # -----Loop through sites\n",
    "    results_df = pd.DataFrame()\n",
    "    for i, site_name in enumerate(site_names):    \n",
    "    \n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # aoi\n",
    "        aoi_fn = glob.glob(os.path.join(usgs_path, 'glacierBoundaries', site_name, 'shapefile', '*.shp'))[0]\n",
    "        aoi = gpd.read_file(aoi_fn)\n",
    "        \n",
    "        # load dem\n",
    "        dem_fn = glob.glob(os.path.join(usgs_path, 'DEMs', site_name, '*.tif'))[-1]\n",
    "        dem = xr.open_dataset(dem_fn)\n",
    "        dem = dem.squeeze().rename({'band_data': 'elevation'})\n",
    "        \n",
    "        # load observed snowlines file names\n",
    "        sl_obs_path = os.path.join(snowlines_obs_path, site_name, 'snowlines')\n",
    "        sl_obs_fns = sorted(glob.glob(os.path.join(sl_obs_path, '*.shp')))\n",
    "    \n",
    "        # load estimated snowlines \n",
    "        sl_est_fn = os.path.join(study_sites_path, site_name, f'{site_name}_snow_cover_stats.csv')\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        # subset to dataset\n",
    "        sl_est = sl_est.loc[sl_est['source']=='Sentinel-2_TOA']\n",
    "        sl_est['datetime'] = pd.to_datetime(sl_est['datetime'])\n",
    "        sl_est['snowlines_coords_X'] = np.array(sl_est['snowlines_coords_X'].apply(literal_eval))\n",
    "        sl_est['snowlines_coords_Y'] = np.array(sl_est['snowlines_coords_Y'].apply(literal_eval))\n",
    "        sl_est.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "        # Iterate over observed snowlines file names\n",
    "        for sl_obs_fn in sl_obs_fns:\n",
    "            # grab date from file name\n",
    "            date = os.path.basename(sl_obs_fn).split(site_name + '_')[1].split('_adj')[0] \n",
    "            date = np.datetime64(f'{date[0:4]}-{date[4:6]}-{date[6:8]}T{date[9:11]}:00:00') \n",
    "            print(f'\\n{date}')\n",
    "\n",
    "            # load observed snowline\n",
    "            sl_obs = gpd.read_file(sl_obs_fn)\n",
    "            sl_obs_UTM = sl_obs.to_crs(f'EPSG:{aoi.crs.to_epsg()}')\n",
    "    \n",
    "            # identify the closest estimated snowline in time\n",
    "            sl_est['dt'] = np.abs(sl_est['datetime'] - date)\n",
    "            sl_est_date = sl_est.loc[sl_est['dt']==sl_est['dt'].min()]\n",
    "            sl_est_date['geometry'] = LineString(list(zip(sl_est_date['snowlines_coords_X'].values[0], \n",
    "                                                          sl_est_date['snowlines_coords_Y'].values[0])))\n",
    "            if sl_est_date['dt'].values[0] > np.timedelta64(7, 'D'):\n",
    "                print('No observations within one week, skipping...')\n",
    "                continue\n",
    "    \n",
    "            if len(sl_est_date['snowlines_coords_X'].values[0]) < 1:\n",
    "                print('No snowline detected for date, skipping...')\n",
    "                continue\n",
    "               \n",
    "            # -----Sample elevations at observed snowline points\n",
    "            xsamp = sl_obs_UTM.geometry[0].coords.xy[0]\n",
    "            ysamp = sl_obs_UTM.geometry[0].coords.xy[1]\n",
    "            sl_obs_elev = [dem.sel(x=x, y=y, method='nearest')['elevation'].data for x,y in list(zip(xsamp, ysamp))]\n",
    "                    \n",
    "            # -----Split line depending on distance between points\n",
    "            max_dist = 100 # m\n",
    "            line = sl_obs_UTM.geometry[0]\n",
    "            first_point = Point(line.coords.xy[0][0], line.coords.xy[1][0])\n",
    "            points = [Point(line.coords.xy[0][i], line.coords.xy[1][i]) for i in np.arange(0,len(line.coords.xy[0]))]\n",
    "            isplit = [0] # point indices where to split the line\n",
    "            for i, p in enumerate(points):\n",
    "                if i!=0:\n",
    "                    dist = p.distance(points[i-1])\n",
    "                    if dist > max_dist:\n",
    "                        isplit.append(i)\n",
    "            isplit.append(len(points)) # add ending point to complete the last line\n",
    "            line_split = [] # initialize split lines\n",
    "            # loop through split indices\n",
    "            if isplit:\n",
    "                for i, p in enumerate(isplit[:-1]):\n",
    "                    if isplit[i+1]-isplit[i] > 1: # must have at least two points to make a line\n",
    "                        line_split = line_split + [LineString(points[isplit[i]:isplit[i+1]])]\n",
    "            else:\n",
    "                line_split = line\n",
    "                        \n",
    "            #-----Regrid the observed snowlines to equal spacing\n",
    "            dx = 30 # point spacing\n",
    "            points_regrid = []\n",
    "            for line in line_split:\n",
    "                distances = np.arange(0, line.length, dx)\n",
    "                line_points = [line.interpolate(distance) for distance in distances] + [first_point]\n",
    "                # filter points outside the aoi\n",
    "                Iaoi = np.where(np.array([p.within(aoi.geometry[0]) for p in line_points], dtype=int) ==1)[0]\n",
    "                points_aoi = [line_points[i] for i in Iaoi]\n",
    "                points_regrid = points_regrid + [p for p in points_aoi]\n",
    "                    \n",
    "            # -----Calculate distance between each observed snowline point and the closest estimated snowline point\n",
    "            distances = np.zeros(len(points_regrid))\n",
    "            for i, p in enumerate(points_regrid):\n",
    "                # find nearest point\n",
    "                nearest_point = nearest_points(sl_est_date['geometry'].values[0], p)[0]\n",
    "                # calculate distance between points\n",
    "                distances[i] = p.distance(nearest_point)\n",
    "        \n",
    "            #-----Display results\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.plot([p.coords.xy[0][0] for p in points_regrid], \n",
    "                     [p.coords.xy[1][0] for p in points_regrid], '.c', label='observed')\n",
    "            plt.plot(*sl_est_date['geometry'].values[0].coords.xy, '.m', label='estimated')\n",
    "            plt.legend(loc='upper right')\n",
    "            plt.grid()\n",
    "            plt.title(date)\n",
    "            plt.show()\n",
    "    \n",
    "            # compile results in df\n",
    "            result_df = pd.DataFrame({'study_site': [site_name], \n",
    "                                      'snowline_obs_date': [str(date)], \n",
    "                                      'snowline_est_date': [sl_est_date['datetime'].values[0]],\n",
    "                                      'snowline_obs': [points_regrid], \n",
    "                                      'snowline_obs_elev_median': [np.nanmedian(sl_obs_elev)],\n",
    "                                      'snowline_est': [sl_est_date['geometry'].values[0]], \n",
    "                                      'snowline_est_elev_median': [sl_est_date['snowline_elevs_median_m'].values[0]],\n",
    "                                      'snowline_elev_median_differences': [sl_est_date['snowline_elevs_median_m'].values[0] - np.nanmedian(sl_obs_elev)],\n",
    "                                      'snowline_distances': [distances],\n",
    "                                      'snowline_distance_median': [np.nanmedian(distances)]})\n",
    "            # concatenate to results_df\n",
    "            results_df = pd.concat([results_df, result_df])\n",
    "\n",
    "    # Save to file\n",
    "    results_df.to_csv(results_fn, index=False)\n",
    "    print('Performance metrics saved to file:', results_fn) \n",
    "            \n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d084ea9-8b68-4637-9920-e810fcd77555",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax[0].boxplot(results_df['snowline_distance_median'])\n",
    "ax[0].set_title('snowline_distance_median')\n",
    "ax[1].boxplot(results_df['snowline_elev_median_differences'])\n",
    "ax[1].set_title('Median snowline elevation differences')\n",
    "plt.show()\n",
    "\n",
    "# compile stats in dataframe\n",
    "results_stats_df = pd.DataFrame({'dataset':['Sentinel-2_TOA'],\n",
    "                                 'ground distance P0 [m]': np.nanpercentile(results_df['snowline_distance_median'], 0),\n",
    "                                 'ground distance P25 [m]': np.nanpercentile(results_df['snowline_distance_median'], 25),\n",
    "                                 'ground distance P50 [m]': np.nanpercentile(results_df['snowline_distance_median'], 50),\n",
    "                                 'ground distance P75 [m]': np.nanpercentile(results_df['snowline_distance_median'], 75),\n",
    "                                 'ground distance P100 [m]': np.nanpercentile(results_df['snowline_distance_median'], 100),\n",
    "                                 'elevation difference P0 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 0),\n",
    "                                 'elevation difference P25 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 25),\n",
    "                                 'elevation difference P50 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 50),\n",
    "                                 'elevation difference P75 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 75),\n",
    "                                 'elevation difference P100 [m]': np.nanpercentile(results_df['snowline_elev_median_differences'], 100),\n",
    "                                 'N': len(results_df)\n",
    "                                })\n",
    "\n",
    "# print results\n",
    "print('Sentinel-2 TOA snowline performance')\n",
    "print('----------')\n",
    "print(\"Ground distance: median = \" + str(np.round(results_stats_df['ground distance P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['ground distance P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['ground distance P75 [m]'][0],2)) + \" m\")\n",
    "print(\"Median elevation difference: median = \" + str(np.round(results_stats_df['elevation difference P50 [m]'][0],2)) + \", \"\n",
    "      + \"IQR = \" + str(np.round(results_stats_df['elevation difference P25 [m]'][0],2))\n",
    "      + \"–\" + str(np.round(results_stats_df['elevation difference P75 [m]'][0],2)) + \" m\")\n",
    "\n",
    "# save to file\n",
    "results_stats_fn = os.path.join(base_path, 'inputs-outputs', 'snowline_performance_stats_Sentinel-2_TOA.csv')\n",
    "results_stats_df.to_csv(results_stats_fn, index=False)\n",
    "print('Performance metrics saved to file: ', results_stats_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81575359-18cc-4320-8db2-23207ce5a431",
   "metadata": {},
   "source": [
    "## Compile all stats tables into one CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28022f71-313b-443f-83e5-1361ba8ad459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grab stats file names\n",
    "fns = sorted(glob.glob(os.path.join(base_path, 'inputs-outputs', 'snowline_performance_stats_*.csv')))\n",
    "\n",
    "# initialize dataframe for all files\n",
    "results_full = pd.DataFrame()\n",
    "\n",
    "# loop through files\n",
    "for fn in fns:\n",
    "    # open file\n",
    "    results = pd.read_csv(fn)\n",
    "    # concatenate to full dataframe\n",
    "    results_full = pd.concat([results_full, results])\n",
    "    \n",
    "# add column for average metrics\n",
    "results = pd.DataFrame({'dataset': 'All datasets AVERAGE',\n",
    "                        'ground distance P0 [m]': np.nanmean(results_full['ground distance P0 [m]']),\n",
    "                        'ground distance P25 [m]': np.nanmean(results_full['ground distance P25 [m]']),\n",
    "                        'ground distance P50 [m]': np.nanmean(results_full['ground distance P50 [m]']),\n",
    "                        'ground distance P75 [m]': np.nanmean(results_full['ground distance P75 [m]']),\n",
    "                        'ground distance P100 [m]': np.nanmean(results_full['ground distance P100 [m]']),\n",
    "                        'elevation difference P0 [m]': np.nanmean(results_full['elevation difference P0 [m]']),\n",
    "                        'elevation difference P25 [m]': np.nanmean(results_full['elevation difference P25 [m]']),\n",
    "                        'elevation difference P50 [m]': np.nanmean(results_full['elevation difference P50 [m]']),\n",
    "                        'elevation difference P75 [m]': np.nanmean(results_full['elevation difference P75 [m]']),\n",
    "                        'elevation difference P100 [m]': np.nanmean(results_full['elevation difference P100 [m]']),\n",
    "                        'N': np.sum(results_full['N'])\n",
    "                       }, index=[5])\n",
    "results_full = pd.concat([results_full, results])\n",
    "                        \n",
    "    \n",
    "# save full dataframe to file\n",
    "results_full_fn = 'snowline_performance_stats.csv'\n",
    "results_full.to_csv(os.path.join(base_path, 'inputs-outputs', results_full_fn), index=False)\n",
    "print('stats for all datasets compiled and saved: ', os.path.join(base_path, 'inputs-outputs', results_full_fn))\n",
    "\n",
    "# delete individual files\n",
    "for fn in fns:\n",
    "    os.remove(fn)\n",
    "    print('file deleted: '+fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f077c0e-fb8b-4170-a1eb-64ff9f9fad5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9bc00c-bb50-4cd9-95da-b585834bac73",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
