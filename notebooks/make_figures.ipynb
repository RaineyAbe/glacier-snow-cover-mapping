{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "473f63e8",
   "metadata": {},
   "source": [
    "# Notebook to make figures for presentations, manuscripts, etc.\n",
    "\n",
    "Rainey Aberle\n",
    "\n",
    "2022/2023/2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d28528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import contextily as cx\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from skimage.measure import find_contours\n",
    "import ee\n",
    "import sys\n",
    "from shapely.geometry import Point, LineString, Polygon, MultiPolygon\n",
    "import rasterio as rio\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap, LightSource\n",
    "import glob\n",
    "import wxee as wx\n",
    "import matplotlib\n",
    "import pickle\n",
    "from scipy.signal import medfilt\n",
    "from scipy.stats import iqr\n",
    "import os\n",
    "import glob\n",
    "import operator\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import seaborn as sns\n",
    "import wxee as wx\n",
    "import geedim as gd\n",
    "import requests\n",
    "from PIL import Image\n",
    "import io\n",
    "from shapely import wkt\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# path to glacier-snow-cover-mapping/\n",
    "base_path = '/Users/raineyaberle/Research/PhD/snow_cover_mapping/snow-cover-mapping/'\n",
    "\n",
    "# path to study sites\n",
    "study_sites_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/'\n",
    "# determine whether to save output figures\n",
    "save_figures = True\n",
    "\n",
    "# path for saving output figures\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "\n",
    "# add path to functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import pipeline_utils as f\n",
    "import PlanetScope_preprocessing as psp\n",
    "\n",
    "# load dataset dictionary\n",
    "dataset_dict = json.load(open(os.path.join(base_path,'inputs-outputs', 'datasets_characteristics.json')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7e45bd",
   "metadata": {},
   "source": [
    "### Define some colormaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a420aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Imagery Datasets\n",
    "color_Landsat = '#ff7f00'\n",
    "color_Sentinel2 = '#984ea3'\n",
    "color_PlanetScope = '#4daf4a'\n",
    "\n",
    "ListedColormap([color_Landsat, color_Sentinel2, color_PlanetScope])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Classified images\n",
    "# Indicies: 0 = snow, 1 = shadowed snow, 2 = ice, 3 = bare ground, 4 = water\n",
    "colors_classified = list(dataset_dict['classified_image']['class_colors'].values())\n",
    "ListedColormap(colors_classified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156a7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----No-snow to Snow\n",
    "cmap_snow = matplotlib.colors.LinearSegmentedColormap.from_list('custom_colormap', \n",
    "                                                                ['#543005', '#8c510a', '#bf812d', '#dfc27d', '#f6e8c3', \n",
    "                                                                 '#f5f5f5', '#c7eae5', '#80cdc1', '#35978f', '#01665e', \n",
    "                                                                 '#003c30',\n",
    "                                                                ],\n",
    "                                                                N=256, gamma=1.0)\n",
    "cmap_snow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62bc747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Elevation\n",
    "cmap_elev = plt.cm.terrain\n",
    "cmap_elev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed62cde2",
   "metadata": {},
   "source": [
    "## Figure 1. Snow, ice, and firn reflectance at Wolverine and satellite band ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44377bc6-844c-4754-9080-389891148420",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Select snow, ice, and firn points for image sampling\n",
    "snow_pt = [3.95e5, 6.6995e6]\n",
    "ice_pt = [3.9415e5, 6.6958e6]\n",
    "firn_pt = [3.9358e5, 6.6992e6]\n",
    "\n",
    "# -----Check if sampled points CSV and TIF already exist in file\n",
    "sample_pts_fn = os.path.join(figures_out_path, 'fig01_sample_points', 'fig01_sample_points.csv')\n",
    "im_xr_fn = os.path.join(figures_out_path, 'fig01_sample_points', 'Wolverine_S2SR_20200817.tif')\n",
    "if os.path.exists(sample_pts_fn):\n",
    "    sample_pts = pd.read_csv(sample_pts_fn)\n",
    "    sample_pts = sample_pts.loc[sample_pts['band']!='B1']\n",
    "    im_xr = rxr.open_rasterio(im_xr_fn)\n",
    "    print('Sample points and Sentinel-2 SR harmonized image loaded from file')\n",
    "\n",
    "else:\n",
    "    # -----Load AOI\n",
    "    AOI_fn = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS/glacierBoundaries/wolverine/shapefile/Wolverine_Glacier_Boundaries.shp'\n",
    "    AOI_UTM = gpd.read_file(AOI_fn)\n",
    "    AOI_UTM = pd.DataFrame(AOI_UTM.iloc[-1, :]).transpose().reset_index(drop=True)\n",
    "    AOI_UTM = gpd.GeoDataFrame(AOI_UTM, geometry='geometry', crs='EPSG:32606')\n",
    "    AOI_WGS_buffer = AOI_UTM.buffer(1e3).to_crs(\"EPSG:4326\")\n",
    "    xmin, ymin, xmax, ymax = AOI_WGS_buffer.geometry[0].bounds\n",
    "    bounding_poly = [[xmin, ymin], [xmax, ymin], [xmax, ymax], [xmin, ymax], [xmin, ymin]]\n",
    "    ee.Initialize()\n",
    "    aoi_ee = ee.Geometry.Polygon(bounding_poly)\n",
    "\n",
    "\n",
    "    sample_pts = pd.DataFrame()\n",
    "    # Plot\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(*snow_pt, '*', color=colors_classified[0], markersize=10, label='Snow')\n",
    "    ax.plot(*ice_pt, '*', color=colors_classified[2], markersize=10, label='Ice')\n",
    "    ax.plot(*firn_pt, '*m', markersize=10, label='Firn')\n",
    "    AOI_UTM.plot(ax=ax, facecolor='None', edgecolor='k')\n",
    "    ax.legend(loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    # -----Snow and ice points\n",
    "    print('Loading images for snow and ice points')\n",
    "    # Load Sentinel-2 SR harmonized images\n",
    "    date_ranges = [['2019-06-18', '2019-10-08'],\n",
    "                   ['2020-06-29', '2020-08-15'],\n",
    "                   ['2021-06-29', '2021-09-15'],\n",
    "                   ['2022-06-24', '2022-10-13']]\n",
    "    im_ids = []\n",
    "    for start_date, end_date in date_ranges:\n",
    "        im_col = gd.MaskedCollection.from_name(\"COPERNICUS/S2_SR_HARMONIZED\").search(start_date=start_date,\n",
    "                                                                                    end_date=end_date,\n",
    "                                                                                    region=aoi_ee,\n",
    "                                                                                    mask=True,\n",
    "                                                                                    fill_portion=70)\n",
    "        im_ids += list(im_col.properties.keys())\n",
    "    # Convert ee.ImageCollections to xarray.Dataset\n",
    "    for i, im_id in enumerate(im_ids):\n",
    "        im_xr = gd.MaskedImage.from_id(im_id, mask=False).ee_image.clip(aoi_ee).wx.to_xarray(scale=30, crs='EPSG:32606')\n",
    "        im_xr = im_xr /1e4 #* 0.0000275 + -0.2\n",
    "        if i==0:\n",
    "            im_col1_xr = im_xr\n",
    "        else:\n",
    "            im_col1_xr = xr.concat([im_col1_xr, im_xr], dim='time')\n",
    "        # plt.figure()\n",
    "        # plt.imshow(np.dstack([im_xr.B4.data[0], im_xr.B3.data[0], im_xr.B2.data[0]]),\n",
    "        #            extent=(np.min(im_xr.x.data), np.max(im_xr.x.data), \n",
    "        #                    np.min(im_xr.y.data), np.max(im_xr.y.data)))\n",
    "        # plt.plot(*snow_pt, '*', color=colors_classified[0], markersize=10)\n",
    "        # plt.plot(*ice_pt, '*b', color=colors_classified[2], markersize=10)\n",
    "        # plt.title(im_id)\n",
    "        # plt.show()\n",
    "    # Add NDSI band\n",
    "    im_col1_xr['NDSI'] = (im_col1_xr['B3'] - im_col1_xr['B11']) / (im_col1_xr['B3'] + im_col1_xr['B11'])\n",
    "    # Sample band values at points\n",
    "    snow_values = im_col1_xr.sel(x=snow_pt[0], y=snow_pt[1], method='nearest')\n",
    "    ice_values = im_col1_xr.sel(x=ice_pt[0], y=ice_pt[1], method='nearest')\n",
    "    bands = [band for band in snow_values.data_vars if (band.startswith('B')) or (band=='NDSI')]\n",
    "    for band in bands:\n",
    "        snow_df = pd.DataFrame({'datetime': im_col1_xr.time.data,\n",
    "                                'band': [band]*len(snow_values[band].data),\n",
    "                                'type': ['Snow']*len(snow_values[band].data),\n",
    "                                'value': snow_values[band].data})\n",
    "        ice_df = pd.DataFrame({'datetime': im_col1_xr.time.data,\n",
    "                               'band': [band]*len(ice_values[band].data),\n",
    "                               'type': ['Ice']*len(ice_values[band].data),\n",
    "                               'value': ice_values[band].data})\n",
    "        sample_pts = pd.concat([sample_pts, snow_df])\n",
    "        sample_pts = pd.concat([sample_pts, ice_df])\n",
    "    \n",
    "    # -----Firn points\n",
    "    print('Loading images for firn points')\n",
    "    # Load Sentinel-2 SR harmonized images\n",
    "    start_date, end_date = '2020-08-16', '2020-10-01'\n",
    "    im_col = gd.MaskedCollection.from_name(\"COPERNICUS/S2_SR_HARMONIZED\").search(start_date=start_date,\n",
    "                                                                                 end_date=end_date,\n",
    "                                                                                 region=aoi_ee,\n",
    "                                                                                 mask=True,\n",
    "                                                                                 fill_portion=70)\n",
    "    # Convert ee.ImageCollections to xarray.Dataset\n",
    "    im_ids = list(im_col.properties.keys())\n",
    "    for i, im_id in enumerate(im_ids):\n",
    "        im_xr = gd.MaskedImage.from_id(im_id, mask=False).ee_image.clip(aoi_ee).wx.to_xarray(scale=30, crs='EPSG:32606')\n",
    "        im_xr = im_xr /1e4 \n",
    "        if i==0:\n",
    "            im_col2_xr = im_xr\n",
    "        else:\n",
    "            im_col2_xr = xr.concat([im_col2_xr, im_xr], dim='time')\n",
    "        # plt.figure()\n",
    "        # plt.imshow(np.dstack([im_xr.B4.data[0], im_xr.B3.data[0], im_xr.B2.data[0]]),\n",
    "        #            extent=(np.min(im_xr.x.data), np.max(im_xr.x.data), \n",
    "        #                    np.min(im_xr.y.data), np.max(im_xr.y.data)))\n",
    "        # plt.plot(*firn_pt, '*m', markersize=10)\n",
    "        # plt.title(im_id)\n",
    "        # plt.show()\n",
    "    # Add NDSI band\n",
    "    im_col2_xr['NDSI'] = (im_col2_xr['B3'] - im_col2_xr['B11']) / (im_col2_xr['B3'] + im_col2_xr['B11'])\n",
    "    # Sample band values at points\n",
    "    firn_values = im_col2_xr.sel(x=firn_pt[0], y=snow_pt[1], method='nearest')\n",
    "    bands = [band for band in snow_values.data_vars if (band.startswith('B')) or (band=='NDSI')]\n",
    "    for band in bands:\n",
    "        firn_df = pd.DataFrame({'datetime': im_col2_xr.time.data,\n",
    "                                'band': [band]*len(firn_values[band].data),\n",
    "                                'type': ['Firn']*len(firn_values[band].data),\n",
    "                                'value': firn_values[band].data})\n",
    "        sample_pts = pd.concat([sample_pts, firn_pts])\n",
    "    sample_pts.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "    # -----Select an example image for plotting\n",
    "    im_xr = im_col2_xr.sel(time=np.datetime64('2020-08-17T21:28:35.000000000'))\n",
    "\n",
    "    # -----Save to file\n",
    "    sample_pts.to_csv(sample_pts_fn, index=False)\n",
    "    im_xr = im_xr.rio.write_crs('EPSG:32606')\n",
    "    im_xr.rio.to_raster(im_xr_fn)\n",
    "    print('Sample points and Sentinel-2 SR harmonized image saved to file in:', os.path.dirname(sample_pts_fn))\n",
    "    \n",
    "sample_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a712067-c4e4-4bbe-a169-97c952f3a4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Plot\n",
    "fontsize = 12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,8), gridspec_kw=dict(width_ratios=[1, 3], height_ratios=[2.5,1]))\n",
    "ax = ax.flatten()\n",
    "\n",
    "### Sampled Sentinel-2 image\n",
    "firn_color = '#ce1256'\n",
    "ax[0].imshow(np.flipud(np.dstack([im_xr.data[3], im_xr.data[2], im_xr.data[1]])),\n",
    "             extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, \n",
    "                     np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "ax[0].plot(snow_pt[0]/1e3, snow_pt[1]/1e3, 'o', markeredgecolor='w', markerfacecolor=colors_classified[0], \n",
    "           markersize=8, label='Snow')\n",
    "ax[0].plot(ice_pt[0]/1e3, ice_pt[1]/1e3, 'o', markeredgecolor='w', markerfacecolor=colors_classified[2],\n",
    "           markersize=8, label='Ice')\n",
    "ax[0].plot(firn_pt[0]/1e3, firn_pt[1]/1e3, 'o', markeredgecolor='w', markerfacecolor=firn_color,\n",
    "           markersize=8, label='Firn')\n",
    "ax[0].set_xticks([])\n",
    "ax[0].set_yticks([])\n",
    "ax[0].legend(ncols=3, loc='best', bbox_to_anchor=[0.88, -0.2, 0.2, 0.2], handletextpad=0.1, columnspacing=0.5)\n",
    "\n",
    "### Boxplots\n",
    "sns.boxplot(data=sample_pts, x='band', y='value', hue='type', whis=[0,100], ax=ax[1],\n",
    "            palette={'Snow': colors_classified[0], 'Ice': colors_classified[2], 'Firn': firn_color},\n",
    "            medianprops={'color':'w', 'linewidth':1})\n",
    "ax[1].get_legend().remove()\n",
    "ax[1].set_xlabel('Sentinel-2 band')\n",
    "ax[1].set_ylabel('Value')\n",
    "ax[1].set_xticks(ax[1].get_xticks() + 0.5, minor=True)\n",
    "ax[1].xaxis.grid(True, which='minor')\n",
    "\n",
    "### Satellite band ranges\n",
    "def draw_boxes(axis, band_ranges, NDSI_indices, y0=0.2, box_height=0.4, \n",
    "               facecolor='#bdbdbd', edgecolor='k', alpha=1.0, NDSI_label=False):\n",
    "    labeled = False\n",
    "    # loop over band ranges\n",
    "    for i, band_range in enumerate(band_ranges):\n",
    "        # convert from nanometers to micrometers\n",
    "        x0, x1 = band_range[0], band_range[1]\n",
    "        # calculate width\n",
    "        box_width = x1-x0\n",
    "        # create rectangle and add to axes\n",
    "        axis.add_patch(matplotlib.patches.Rectangle((x0, y0), width=box_width, height=box_height, \n",
    "                       facecolor=facecolor, edgecolor=edgecolor, alpha=alpha))\n",
    "        # plot star on NDSI bands\n",
    "        if i in NDSI_indices:\n",
    "            if (not labeled) and NDSI_label:\n",
    "                label = 'NDSI bands'\n",
    "                labeled = True\n",
    "            else:\n",
    "                label='_nolegend_'\n",
    "            axis.plot(x0 + box_width/2, y0 + box_height/2, '*k', markersize=10, label=label)\n",
    "    # add one rectangle to contain all bands\n",
    "    x0, x1 = band_ranges[0][0], band_ranges[-1][-1]\n",
    "    box_width = x1-x0\n",
    "    axis.add_patch(matplotlib.patches.Rectangle((x0, y0), width=box_width, height=box_height, \n",
    "                                                facecolor='none', edgecolor='k', alpha=1.0))\n",
    "    return\n",
    "# Landsat 8/9 OLI\n",
    "L_band_ranges = [[0.45, 0.51], [0.53, 0.59], [0.64, 0.67], [0.85, 0.88], # 2, 3, 4, 5\n",
    "                 [1.57, 1.65], [2.11, 2.29]] # 6, 7\n",
    "L_band_names = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']#, 'TIRS1', 'TIRS2']\n",
    "L_NDSI_band_indices = [1, 4]\n",
    "draw_boxes(ax[3], L_band_ranges, L_NDSI_band_indices, y0=1, NDSI_label=True, facecolor=color_Landsat)\n",
    "# Sentinel-2 MSI\n",
    "S2_20_band_ranges = [[0.69, 0.718], [0.727, 0.755], [0.764, 0.802], # B5, B6, B7\n",
    "                     [0.845, 0.85], [1.52, 1.70], [2.010, 2.37]] # B8A, B11 (SWIR1), B12 (SWIR2)\n",
    "S2_20_NDSI_band_indices = [4]\n",
    "draw_boxes(ax[3], S2_20_band_ranges, S2_20_NDSI_band_indices, y0=2, facecolor=color_Sentinel2)\n",
    "\n",
    "S2_10_band_ranges = [[0.425, 0.555], [0.525, 0.595], [0.635, 0.695], # B2 B3 B4 \n",
    "                     [0.728, 1.038]] # B8 (NIR)\n",
    "S2_10_NDSI_band_indices = [1]\n",
    "draw_boxes(ax[3], S2_10_band_ranges, S2_10_NDSI_band_indices, y0=3, facecolor=color_Sentinel2)\n",
    "# PlanetScope 4-band\n",
    "PS_band_ranges = [[0.455, 0.515], [0.51, 0.59], [0.590, 0.670], [0.780, 0.860]]\n",
    "PS_NDSI_indices = [1, 3]\n",
    "draw_boxes(ax[3], PS_band_ranges, PS_NDSI_indices, y0=4, facecolor=color_PlanetScope)\n",
    "ax[3].set_xlim(0.4, 2.5)\n",
    "ax[3].set_xlabel('Wavelength [$\\mu$m]')\n",
    "ax[3].set_yticks([1.2, 2.2, 3.2, 4.2])\n",
    "ax[3].set_yticklabels(['Landsat 8/9 (30 m)', 'Sentinel-2 (20 m)', 'Sentinel-2 (10 m)', 'PlanetScope 4-band (3$-$5 m)'])\n",
    "\n",
    "### text labels\n",
    "ax[0].text(ax[0].get_xlim()[0] + (ax[0].get_xlim()[1]-ax[0].get_xlim()[0]) * 0.85,\n",
    "           ax[0].get_ylim()[0] + (ax[0].get_ylim()[1]-ax[0].get_ylim()[0]) * 0.85, \n",
    "           '(a)', fontsize=fontsize+2, fontweight='bold', \n",
    "           bbox=dict(facecolor='white', edgecolor='none', pad=3))\n",
    "ax[1].text(ax[1].get_xlim()[0] + (ax[1].get_xlim()[1]-ax[1].get_xlim()[0]) * 0.95,\n",
    "           ax[1].get_ylim()[0] + (ax[1].get_ylim()[1]-ax[1].get_ylim()[0]) * 0.9, \n",
    "           '(b)', fontsize=fontsize+2, fontweight='bold', \n",
    "           bbox=dict(facecolor='white', edgecolor='none', pad=3))\n",
    "ax[3].text(ax[3].get_xlim()[0] + (ax[3].get_xlim()[1]-ax[3].get_xlim()[0]) * 0.95,\n",
    "           ax[3].get_ylim()[0] + (ax[3].get_ylim()[1]-ax[3].get_ylim()[0]) * 0.8, \n",
    "           '(c)', fontsize=fontsize+2, fontweight='bold', \n",
    "           bbox=dict(facecolor='white', edgecolor='none', pad=3))\n",
    "\n",
    "ax[2].remove()\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig01_spectral_signatures_satellite_bands.png')\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae8606",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from matplotlib.patches import Rectangle\n",
    "# import matplotlib.gridspec as gridspec\n",
    "\n",
    "# # -----Set up figure\n",
    "# # define colors for different materials\n",
    "# color_snow = colors_classified[0]\n",
    "# color_firn = '#12617a'\n",
    "# color_ice = colors_classified[2]\n",
    "# color_dirty_ice = '#1a2c40'\n",
    "# color_veg = '#006d2c'\n",
    "# color_rock = colors_classified[3]\n",
    "# color_water = colors_classified[4]\n",
    "# # plot\n",
    "# fontsize = 12\n",
    "# plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# fig = plt.figure(figsize=(12,14))\n",
    "# linewidth=2\n",
    "# # define axes layout using gridspec\n",
    "# gs = gridspec.GridSpec(2, 3, height_ratios=[1, 1])\n",
    "# ax = [fig.add_subplot(gs[0, :]), \n",
    "#       fig.add_subplot(gs[1, 0]), fig.add_subplot(gs[1, 1]), fig.add_subplot(gs[1, 2])]\n",
    "\n",
    "# # -----Plot satellite band ranges\n",
    "# def draw_boxes(axis, band_ranges, NDSI_indices, y0=0.2, box_height=0.04, \n",
    "#                facecolor='#bdbdbd', edgecolor='k', alpha=1.0, NDSI_label=False):\n",
    "#     labeled = False\n",
    "#     # loop over band ranges\n",
    "#     for i, band_range in enumerate(band_ranges):\n",
    "#         # convert from nanometers to micrometers\n",
    "#         x0, x1 = band_range[0], band_range[1]\n",
    "#         # calculate width\n",
    "#         box_width = x1-x0\n",
    "#         # create rectangle and add to axes\n",
    "#         axis.add_patch(Rectangle((x0, y0), width=box_width, height=box_height, \n",
    "#                        facecolor=facecolor, edgecolor=edgecolor, alpha=alpha))\n",
    "#         # plot star on NDSI bands\n",
    "#         if i in NDSI_indices:\n",
    "#             if (not labeled) and NDSI_label:\n",
    "#                 label = 'NDSI bands'\n",
    "#                 labeled = True\n",
    "#             else:\n",
    "#                 label='_nolegend_'\n",
    "#             axis.plot(x0 + box_width/2, y0 + box_height/2, '*k', markersize=10, label=label)\n",
    "            \n",
    "#     # add one rectangle to contain all bands\n",
    "#     x0, x1 = band_ranges[0][0], band_ranges[-1][-1]\n",
    "#     box_width = x1-x0\n",
    "#     axis.add_patch(Rectangle((x0, y0), width=box_width, height=box_height, facecolor='none', edgecolor='k', alpha=1.0))\n",
    "#     return\n",
    "\n",
    "# # Landsat 8/9 OLI\n",
    "# L_band_ranges = [[0.45, 0.51], [0.53, 0.59], [0.64, 0.67], [0.85, 0.88], # 2, 3, 4, 5\n",
    "#                  [1.57, 1.65], [2.11, 2.29]] # 6, 7\n",
    "# L_band_names = ['Blue', 'Green', 'Red', 'NIR', 'SWIR1', 'SWIR2']#, 'TIRS1', 'TIRS2']\n",
    "# L_NDSI_band_indices = [1, 4]\n",
    "# draw_boxes(ax[0], L_band_ranges, L_NDSI_band_indices, y0=1.001, NDSI_label=True, facecolor=color_Landsat)\n",
    "# ax[0].text(2.32, 1.005, 'Landsat 8/9 (30 m)')\n",
    "# # Sentinel-2 MSI\n",
    "# S2_20_band_ranges = [[0.69, 0.718], [0.727, 0.755], [0.764, 0.802], # B5, B6, B7\n",
    "#                      [0.845, 0.85], [1.52, 1.70], [2.010, 2.37]] # B8A, B11 (SWIR1), B12 (SWIR2)\n",
    "# S2_20_NDSI_band_indices = [4]\n",
    "# draw_boxes(ax[0], S2_20_band_ranges, S2_20_NDSI_band_indices, y0=1.101, facecolor=color_Sentinel2)\n",
    "# ax[0].text(2.4, 1.105, 'Sentinel-2 (20 m)')\n",
    "\n",
    "# S2_10_band_ranges = [[0.425, 0.555], [0.525, 0.595], [0.635, 0.695], # B2 B3 B4 \n",
    "#                      [0.728, 1.038]] # B8 (NIR)\n",
    "# S2_10_NDSI_band_indices = [1]\n",
    "# draw_boxes(ax[0], S2_10_band_ranges, S2_10_NDSI_band_indices, y0=1.201, facecolor=color_Sentinel2)\n",
    "# ax[0].text(1.068, 1.205, 'Sentinel-2 (10 m)')\n",
    "# # PlanetScope 4-band\n",
    "# PS_band_ranges = [[0.455, 0.515], [0.51, 0.59], [0.590, 0.670], [0.780, 0.860]]\n",
    "# PS_NDSI_indices = [1, 3]\n",
    "# draw_boxes(ax[0], PS_band_ranges, PS_NDSI_indices, y0=1.301, facecolor=color_PlanetScope)\n",
    "# ax[0].text(0.90, 1.305, 'PlanetScope 4-band (3-5 m)')\n",
    "\n",
    "# # -----Load spectral signatures data and plot\n",
    "# # Painter et al. (2009): snow, coarse- to fine-grained\n",
    "# spec_path_painter = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_Painter_et_al_2009'\n",
    "# os.chdir(spec_path_painter)\n",
    "# coarse_snow = pd.read_csv('coarse_snow_Painter_et_al_2009.csv', header=None)\n",
    "# fine_snow = pd.read_csv('fine_snow_Painter_et_al_2009.csv', header=None)\n",
    "# # interpolate to same x values\n",
    "# x_snow = np.linspace(0, 2.5, num=100)\n",
    "# y_coarse = np.interp(x_snow, coarse_snow[0].values, coarse_snow[1].values)\n",
    "# y_fine = np.interp(x_snow, fine_snow[0].values, fine_snow[1].values)\n",
    "# y_med = np.array([np.nanmean([y1, y2]) for y1, y2 in list(zip(y_coarse, y_fine))])\n",
    "# # plot\n",
    "# ax[0].fill_between(x_snow, y_fine, y_coarse, color=color_snow, alpha=0.4)\n",
    "# ax[0].plot(x_snow, y_med, '-', color=color_snow, linewidth=linewidth, label='snow')\n",
    "# # Salvatori et al. (2022): ice \n",
    "# spec_path_salv = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_Salvatori_et_al_2022'\n",
    "# os.chdir(spec_path_salv)\n",
    "# ice = pd.read_csv('ice.csv', header=None)\n",
    "# # interpolate min, max, and mean values\n",
    "# x_ice = np.linspace(0.4, 2.4, num=50)\n",
    "# dx = x_ice[1] - x_ice[0]\n",
    "# y_ice_min, y_ice_max, y_ice_mean = np.zeros(len(x_ice)), np.zeros(len(x_ice)), np.zeros(len(x_ice))\n",
    "# for i in range(0,len(x_ice)-1):\n",
    "#     x_window = [x_ice[i] - dx/2, x_ice[i] + dx/2]\n",
    "#     if np.any((ice[0].values > x_window[0]) & (ice[0].values < x_window[1])):\n",
    "#         y_ice_min[i] = np.nanmin(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "#         y_ice_max[i] = np.nanmax(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "#         y_ice_mean[i] = np.nanmean(ice[1].values[(ice[0].values > x_window[0]) & (ice[0].values < x_window[1])])\n",
    "#     else:\n",
    "#         y_ice_min[i], y_ice_max[i], y_ice_mean[i] = np.nan, np.nan, np.nan\n",
    "# # plot\n",
    "# ax[0].fill_between(x_ice, y_ice_min, y_ice_max, facecolor=color_ice, edgecolor=None, alpha=0.4)\n",
    "# ax[0].plot(x_ice, y_ice_mean, '-', color=color_ice, linewidth=linewidth, label='ice and firn')\n",
    "\n",
    "# # USGS: vegetation, soil, seawater\n",
    "# colors = [color_veg, color_rock, color_water]\n",
    "# spec_path_usgs = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/figures/spectral_signatures_USGS/'\n",
    "# os.chdir(spec_path_usgs)\n",
    "# # define prefixes used in file names for each material\n",
    "# prefixes = ['Aspen', 'Basalt', 'Seawater']\n",
    "# # define labels for plot\n",
    "# labels = ['vegetation', 'soil', 'seawater']\n",
    "# # loop through prefixes\n",
    "# for i, prefix in enumerate(prefixes):\n",
    "#     # grab folder name\n",
    "#     folder = glob.glob('*'+prefix+'*')[0]\n",
    "#     # load wavelengths\n",
    "#     wave_fn = glob.glob(folder + '/*Wavelengths*.txt')[0]\n",
    "#     wave = pd.read_csv(wave_fn)\n",
    "#     wave = wave[wave.keys()[0]].values\n",
    "#     if prefix=='Basalt':\n",
    "#         refl_fn = glob.glob(folder + '/*'+prefix+'*.txt')[1]\n",
    "#     else:\n",
    "#         refl_fn = glob.glob(folder + '/*'+prefix+'*.txt')[0]\n",
    "#     refl = pd.read_csv(refl_fn)\n",
    "#     refl = refl[refl.keys()[0]].values\n",
    "#     refl[refl<0] = np.nan\n",
    "#     # plot reflectances\n",
    "#     ax[0].plot(wave, refl, '-', color=colors[i], linewidth=linewidth, label=labels[i])\n",
    "    \n",
    "# ax[0].grid(True)\n",
    "# ax[0].set_xlim(0.4, 3)\n",
    "# ax[0].set_ylim(0, 1.4)\n",
    "# ax[0].set_yticks([0.0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "# ax[0].legend(loc='center right', bbox_to_anchor=[0.8, 0.3, 0.2, 0.2])\n",
    "# ax[0].set_xlabel('Wavelength [$\\mu$m]')\n",
    "# ax[0].set_ylabel('Reflectance')\n",
    "# ax[0].text((ax[0].get_xlim()[1] - ax[0].get_xlim()[0])*0.94 + ax[0].get_xlim()[0],\n",
    "#            (ax[0].get_ylim()[1] - ax[0].get_ylim()[0])*0.07 + ax[0].get_ylim()[0], '(a)', fontweight='bold', fontsize=fontsize+2, \n",
    "#            bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# # -----Band value boxplots\n",
    "# # xticks = np.arange(int(np.min(im_xr.x.data)/1e3)+1, np.max(im_xr.x.data)/1e3, step=2)\n",
    "# # yticks = np.arange(int(np.min(im_xr.y.data)/1e3)+1, np.max(im_xr.y.data)/1e3, step=2)\n",
    "# # # RGB image\n",
    "# # ax[1].imshow(np.dstack([im_xr['SR_B4'].data[0], im_xr['SR_B3'].data[0], im_xr['SR_B2'].data[0]]),\n",
    "# #              extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "# # ax[1].set_xticks(xticks)\n",
    "# # ax[1].set_yticks(yticks)\n",
    "# # ax[1].grid(False)\n",
    "# # ax[1].set_xlabel('Easting [km]')\n",
    "# # ax[1].set_ylabel('Northing [km]')\n",
    "# # text_x = (ax[1].get_xlim()[1] - ax[1].get_xlim()[0])*0.85 + ax[1].get_xlim()[0]\n",
    "# # text_y = (ax[1].get_ylim()[1] - ax[1].get_ylim()[0])*0.07 + ax[1].get_ylim()[0]\n",
    "# # ax[1].text(text_x, text_y, '(b)', fontweight='bold', fontsize=fontsize+2, \n",
    "# #            bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# # # NDSI image\n",
    "# # ndsi_im = ax[2].imshow(NDSI, cmap=cmap_snow, clim=(-1, 1),\n",
    "# #                        extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, \n",
    "# #                                np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "# # ax[2].set_xticks(xticks)\n",
    "# # ax[2].set_yticks([])\n",
    "# # ax[2].grid(False)\n",
    "# # ax[2].set_xlabel('Easting [km]')\n",
    "# # ax[2].text(text_x, text_y, '(c)', fontweight='bold', fontsize=fontsize+2, \n",
    "# #            bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# # cbar_ax = fig.add_axes([0.42, 0.135, 0.19, 0.01])\n",
    "# # fig.colorbar(ndsi_im, cax=cbar_ax, orientation='horizontal', label='NDSI')\n",
    "# # # NDSI thresholded image\n",
    "# # ax[3].imshow(NDSI_thresh, cmap=NDSI_cmap,\n",
    "# #              extent=(np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3, np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3))\n",
    "# # ax[3].set_xticks(xticks)\n",
    "# # ax[3].set_yticks([])\n",
    "# # ax[3].grid(False)\n",
    "# # ax[3].set_xlabel('Easting [km]')\n",
    "# # ax[3].text(text_x, text_y, '(d)', fontweight='bold', fontsize=fontsize+2, \n",
    "# #            bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# # # plot dummy points for legend\n",
    "# # xlim, ylim = ax[3].get_xlim(), ax[3].get_ylim()\n",
    "# # ax[3].plot(0, 0, 's', markersize=15, markerfacecolor=NDSI_cmap(1), markeredgecolor='k', linewidth=1, label='classified as snow')\n",
    "# # ax[3].plot(0, 0, 's', markersize=15, markerfacecolor='w', markeredgecolor='k', linewidth=1, label='classified as no snow')\n",
    "# # ax[3].legend(loc='lower left', bbox_to_anchor=[0.1, -0.35, 0.2, 0.2])\n",
    "# # # reset axis limits\n",
    "# # ax[3].set_xlim(xlim)\n",
    "# # ax[3].set_ylim(ylim)\n",
    "\n",
    "# # annotations\n",
    "# # snow\n",
    "# # ax[1].arrow(397, 6699, -2, 1, color=colors_classified[0], width=0.05)\n",
    "# # ax[1].text(397, 6698.9, 'snow', color=colors_classified[0], fontsize='large',\n",
    "# #            bbox=dict(facecolor='white', edgecolor=colors_classified[0], linewidth=3, pad=5))\n",
    "# # # firn\n",
    "# # ax[1].arrow(397, 6697.5, -1.8, 1.6, color=color_firn, width=0.05)\n",
    "# # ax[1].text(397, 6697.4, 'firn', color=color_firn, fontsize='large',\n",
    "# #            bbox=dict(facecolor='white', edgecolor=color_firn, linewidth=3, pad=5))\n",
    "# # # ice\n",
    "# # ax[1].arrow(397, 6696, -2, 1, color=colors_classified[2], width=0.05)\n",
    "# # ax[1].text(397, 6695.9, 'ice', color=colors_classified[2], fontsize='large',\n",
    "# #            bbox=dict(facecolor='white', edgecolor=colors_classified[2], linewidth=3, pad=5))\n",
    "# # ax[1].set_xlim(xlim)\n",
    "# # ax[1].set_ylim(ylim)\n",
    "\n",
    "# # plt.subplots_adjust(wspace=0.1, hspace=-0.05)\n",
    "# # fig.tight_layout()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# # -----Save figure to file\n",
    "# # if save_figures:\n",
    "# #     fig_fn = os.path.join(figures_out_path, 'fig01_spectral_signatures_satellite_bands.png')\n",
    "# #     fig.savefig(fig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "# #     print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917342ad",
   "metadata": {},
   "source": [
    "## Figure 2. Study sites - USGS Benchmark Glaciers & Emmons Glacier, WA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d57bddd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Load RGI data\n",
    "# path to RGI data\n",
    "rgi_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/RGI/'\n",
    "# RGI shapefile names\n",
    "rgi_fns = ['01_rgi60_Alaska/01_rgi60_Alaska.shp', \n",
    "           '02_rgi60_WesternCanadaUS/02_rgi60_WesternCanadaUS.shp']\n",
    "# load and combine rgis\n",
    "rgis = gpd.GeoDataFrame()\n",
    "for rgi_fn in rgi_fns:\n",
    "    rgi = gpd.read_file(rgi_path + rgi_fn)\n",
    "    rgis = pd.concat([rgis, rgi])\n",
    "rgis.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# -----Load country outlines\n",
    "countries_fn = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/countries_shp/countries.shp'\n",
    "countries = gpd.read_file(countries_fn)\n",
    "countries = countries.loc[countries['CONTINENT']=='North America']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffc37d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Count number of glaciers with areas < 1 km^2 (for manuscript)\n",
    "print('Total # of glaciers in RGI regions 1 and 2 = ', len(rgis))\n",
    "print('Number of glaciers with areas < 1 km^2 = ', len(rgis.loc[rgis['Area'] < 1]))\n",
    "print('Percentage of glaciers with areas < 1 km^2 = ', len(rgis.loc[rgis['Area'] < 1]) / len(rgis))\n",
    "print(' ')\n",
    "\n",
    "# -----Count total area of glaciers with areas < 1 km^2 with respect to total glacier area (for manuscript)\n",
    "lt1_area = np.sum(rgis.loc[rgis['Area'] < 1, 'Area'].values)\n",
    "gte1_area = np.sum(rgis.loc[rgis['Area'] >= 1, 'Area'].values)\n",
    "print('Total area of glaciers with area < 1 km^2 = ', lt1_area, ' km^2')\n",
    "print('Total area of glaciers with area >= 1 km^2 = ', gte1_area, ' km^2')\n",
    "print('Areal percentage of glaciers with area < 1 km^2 = ', lt1_area / gte1_area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54170b66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define site specifics\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade', 'Emmons']\n",
    "site_names_display = [x.replace('C', ' C') for x in site_names]\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)']\n",
    "\n",
    "# -----Set up figure\n",
    "fontsize = 14\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "ax = [fig.add_subplot(2, 3, 1), fig.add_subplot(2, 3, 2), fig.add_subplot(2, 3, 3),\n",
    "      fig.add_subplot(2, 4, 5), fig.add_subplot(2, 4, 6), fig.add_subplot(2, 4, 7), fig.add_subplot(2, 4, 8)]\n",
    "epsg_A = 4326    \n",
    "\n",
    "# -----Loop through sites\n",
    "for i, (site_name, site_name_display, text_label) in enumerate(zip(site_names, site_names_display, text_labels)):\n",
    "    ### AOI\n",
    "    # load file\n",
    "    if site_name=='Emmons':\n",
    "        AOI_fn = glob.glob(os.path.join(study_sites_path, 'RGI60-02.14297', 'AOIs', 'RGI60-02.14297_outline.shp'))[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "    else:\n",
    "        AOI_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS/glacierBoundaries/' + site_name.lower() + '/shapefile'\n",
    "        AOI_fn = glob.glob(os.path.join(AOI_path, site_name + '_Glacier_Boundaries.shp'))[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        AOI_crs = 'EPSG:' + str(AOI.crs.to_epsg())\n",
    "        AOI = pd.DataFrame(AOI.iloc[-1, :]).transpose().reset_index(drop=True)\n",
    "        AOI = gpd.GeoDataFrame(AOI, geometry='geometry', crs=AOI_crs)\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    # solve for optimal UTM zone\n",
    "    AOI_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_centroid[0], AOI_centroid[1])\n",
    "    # reproject\n",
    "    AOI_UTM = AOI_WGS.to_crs(epsg_UTM)\n",
    "    AOI_A = AOI.to_crs(epsg_A)\n",
    "    ### DEM\n",
    "    if site_name=='Emmons':\n",
    "        DEM_fn =  glob.glob(os.path.join(study_sites_path, 'RGI60-02.14297', 'DEMs', 'RGI60-02.14297*NASADEM*.tif'))[0]\n",
    "    else:\n",
    "        DEM_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS/DEMs/' + site_name\n",
    "        DEM_fn =  sorted(glob.glob(os.path.join(DEM_path, site_name + '*_DEM.tif')))[-1]\n",
    "    DEM = xr.open_dataset(DEM_fn)\n",
    "    DEM = DEM.rename({'band_data': 'elevation'})\n",
    "    # reproject \n",
    "    DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "    ### Plot\n",
    "    # A) Study sites map\n",
    "    text = ax[0].text(AOI_A.geometry[0].centroid.xy[0][0], AOI_A.geometry[0].centroid.xy[1][0],\n",
    "               text_labels[i].replace('(','').replace(')',''), fontweight='bold', fontsize=fontsize+4, \n",
    "               color='k', va='center', ha='center', bbox=dict(facecolor='None', edgecolor='None'))\n",
    "    text.set_path_effects([matplotlib.patheffects.Stroke(linewidth=4, foreground='white'), matplotlib.patheffects.Normal()])\n",
    "    # Shaded relief\n",
    "    z = DEM.elevation.data[0]\n",
    "    nan_mask = np.isnan(z)\n",
    "    z[nan_mask] = 0\n",
    "    ls = matplotlib.colors.LightSource(azdeg=315, altdeg=15)\n",
    "    shade = ls.shade(z, cmap=cmap_elev, vert_exag=3, vmin=0, vmax=4000, blend_mode='soft', fraction=0.7)\n",
    "    shade[nan_mask, :] = [1, 1, 1, 1]\n",
    "    DEM_im = ax[i+1].imshow(shade, extent=(np.min(DEM.x.data), np.max(DEM.x.data), \n",
    "                                           np.min(DEM.y.data), np.max(DEM.y.data)))\n",
    "    # AOI\n",
    "    AOI_UTM.plot(ax=ax[i+1], edgecolor='k', facecolor='none', linewidth=2)\n",
    "    # Set axis limits based on AOI extent\n",
    "    if AOI_UTM.geometry[0].geom_type=='MultiPolygon':\n",
    "        xmin_AOI = np.min([np.min(geom.exterior.coords.xy[0]) for geom in AOI_UTM.geometry[0].geoms])\n",
    "        xmax_AOI = np.max([np.max(geom.exterior.coords.xy[0]) for geom in AOI_UTM.geometry[0].geoms])\n",
    "        ymin_AOI = np.min([np.min(geom.exterior.coords.xy[1]) for geom in AOI_UTM.geometry[0].geoms])\n",
    "        ymax_AOI = np.max([np.max(geom.exterior.coords.xy[1]) for geom in AOI_UTM.geometry[0].geoms])      \n",
    "    else:\n",
    "        xmin_AOI = np.min(AOI_UTM.geometry[0].exterior.coords.xy[0])\n",
    "        xmax_AOI = np.max(AOI_UTM.geometry[0].exterior.coords.xy[0])\n",
    "        ymin_AOI = np.min(AOI_UTM.geometry[0].exterior.coords.xy[1])\n",
    "        ymax_AOI = np.max(AOI_UTM.geometry[0].exterior.coords.xy[1])  \n",
    "    xmin = xmin_AOI - 0.08*(xmax_AOI - xmin_AOI)\n",
    "    xmax = xmax_AOI + 0.08*(xmax_AOI - xmin_AOI)\n",
    "    ymin = ymin_AOI - 0.08*(ymax_AOI - ymin_AOI)\n",
    "    ymax = ymax_AOI + 0.08*(ymax_AOI - ymin_AOI)        \n",
    "    ax[i+1].set_xlim(xmin, xmax)\n",
    "    ax[i+1].set_ylim(ymin, ymax)\n",
    "    # Add scale bar\n",
    "    scale_color = 'k'\n",
    "    xrange = (ax[i+1].get_xlim()[1] - ax[i+1].get_xlim()[0])\n",
    "    if (i==0) :\n",
    "        xstart = xrange * 0.75 + ax[i+1].get_xlim()[0]\n",
    "        xend = xstart + 1e3\n",
    "        ypos = ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.2 \n",
    "        ly = ypos - (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.05\n",
    "        l = '1.0 km'\n",
    "    elif (i==1):\n",
    "        xstart = xrange * 0.75 + ax[i+1].get_xlim()[0]\n",
    "        xend = xstart + 1e3\n",
    "        ypos = ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.95\n",
    "        ly = ypos - (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.05\n",
    "        l = '1.0 km'\n",
    "    elif (i==2):\n",
    "        xstart = ax[i+1].get_xlim()[0] + xrange * 0.25\n",
    "        xend = xstart + 1e3\n",
    "        ypos = ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.95\n",
    "        ly = ypos - (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.05\n",
    "        l = '1.0 km'\n",
    "    elif (i==3):\n",
    "        xstart = ax[i+1].get_xlim()[0] + xrange * 0.55\n",
    "        xend = xstart + 500\n",
    "        ypos = ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.1\n",
    "        ly = ypos - (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.05\n",
    "        l = '0.5 km'\n",
    "    elif (i==4):\n",
    "        xstart = ax[i+1].get_xlim()[0] + xrange * 0.7\n",
    "        xend = xstart + 500\n",
    "        ypos = ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.85\n",
    "        ly = ypos - (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.05\n",
    "        l = '0.5 km'\n",
    "    elif (i==5):\n",
    "        xstart = ax[i+1].get_xlim()[0] + xrange * 0.25 \n",
    "        xend = xstart + 1e3\n",
    "        ypos = ax[i+1].get_ylim()[0] + (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.9 \n",
    "        ly = ypos - (ax[i+1].get_ylim()[1] - ax[i+1].get_ylim()[0]) * 0.08\n",
    "        l = '1.0 km'\n",
    "    lx = (xstart+xend)/2\n",
    "    ax[i+1].plot([xstart, xend], [ypos, ypos], '-', color=scale_color, linewidth=3) # line\n",
    "    ax[i+1].text(lx, ly, l, color=scale_color, fontweight='bold', fontsize=fontsize+2, ha='center', va='center') # text label\n",
    "    # Remove axis ticks\n",
    "    ax[i+1].set_xticks([])\n",
    "    ax[i+1].set_yticks([])\n",
    "    # Add title\n",
    "    ax[i+1].set_title(text_label + ' ' + site_name_display + ' Glacier')\n",
    "    ax[i+1].grid()\n",
    "\n",
    "# A: study sites map\n",
    "countries_reproj = countries.to_crs(f'EPSG:{epsg_A}')\n",
    "countries_reproj.plot(ax=ax[0], facecolor='#d9d9d9', edgecolor='#969696')\n",
    "rgis_reproj = rgis.to_crs(f'EPSG:{epsg_A}')\n",
    "rgis_reproj.plot(ax=ax[0], facecolor=colors_classified[2], edgecolor=colors_classified[2])\n",
    "ax[0].set_xlim(-165, -110)\n",
    "ax[0].set_ylim(45, 68)\n",
    "ax[0].set_xlabel('Longitude')\n",
    "ax[0].set_ylabel('Latitude')\n",
    "# Colorbar for elevations at bottom of figure\n",
    "fig.subplots_adjust(bottom=0.1)\n",
    "cbar_ax = fig.add_axes([0.35, 0.03, 0.3, 0.02])\n",
    "matplotlib.colorbar.ColorbarBase(cbar_ax, cmap=cmap_elev,\n",
    "                                 norm=matplotlib.colors.Normalize(vmin=0, vmax=4000),\n",
    "                                 orientation='horizontal',\n",
    "                                 label='Elevation [m]')\n",
    "# North arrow\n",
    "arrow_pos = (0.6, 0.05)\n",
    "ax[-1].annotate('N', xy=arrow_pos, xycoords='figure fraction',\n",
    "                 ha='center', va='center', fontsize=fontsize+8, color='black', fontweight='bold')\n",
    "ax[-1].annotate('', xy=(arrow_pos[0], arrow_pos[1]+0.04), xytext=(arrow_pos[0], arrow_pos[1]+0.03), xycoords='figure fraction',\n",
    "                arrowprops=dict(facecolor='black', edgecolor='black', arrowstyle='fancy', linewidth=2, mutation_scale=35))\n",
    "\n",
    "fig.subplots_adjust(hspace=0.05, wspace=0.05)\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig02_study_sites_elevations.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5ca01a",
   "metadata": {},
   "source": [
    "## Figure 3. Example training points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563e4ace",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load glacier outlines from file\n",
    "AOI_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS/glacierBoundaries/gulkana/shapefile/'\n",
    "AOI_fn = os.path.join(AOI_path, 'Gulkana_Glacier_Boundaries.shp')\n",
    "AOI = gpd.read_file(AOI_fn).loc[9:, :].reset_index(drop=True)\n",
    "AOI_color = 'w'\n",
    "\n",
    "# Query GEE for images\n",
    "ee.Initialize()\n",
    "im_dates = ['2021-06-15', '2021-08-02']\n",
    "date_starts, date_ends = ['2021-06-14', '2021-08-01'], ['2021-06-16', '2021-08-03']\n",
    "ims = []\n",
    "for date_start, date_end in zip(date_starts, date_ends):\n",
    "    im = f.query_gee_for_imagery(dataset_dict, 'Sentinel-2_SR', AOI.buffer(750), date_start, date_end,\n",
    "                                 month_start=5, month_end=11, cloud_cover_max=100, mask_clouds=False,\n",
    "                                 im_out_path=None, im_download=False)[0]\n",
    "    ims.append(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f290e33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from shapely.geometry import Polygon, MultiPolygon\n",
    "\n",
    "# define path to manually classified points\n",
    "site_name = 'Gulkana'\n",
    "im_dates = ['20210615', '20210806']\n",
    "data_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/classified-points/'\n",
    "\n",
    "# set up figure\n",
    "fontsize = 16\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "text_labels = ['(a)', '(b)']\n",
    "\n",
    "# iterate over images\n",
    "for i, (im_date, im) in enumerate(zip(im_dates, ims)):\n",
    "    \n",
    "    # grab CRS\n",
    "    crs = im.rio.crs.to_epsg()\n",
    "    im = im / 1e4\n",
    "    AOI = AOI.to_crs(crs)\n",
    "\n",
    "    # grab validation point names\n",
    "    data_pts_fns = sorted(glob.glob(os.path.join(data_path, site_name + '*' + im_date.replace('-', '') + '*.shp')))\n",
    "\n",
    "    # Plot\n",
    "    # RGB image\n",
    "    ax[i].imshow(np.dstack([im.B4.data[0] * 1e4, im.B3.data[0] * 1e4, im.B2.data[0] * 1e4]), #alpha=0.8,\n",
    "                           extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3,\n",
    "                                   np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "    # Glacier boundary\n",
    "    if type(AOI.geometry[0])==MultiPolygon:\n",
    "        for j, geom in enumerate(AOI.geometry[0].geoms):\n",
    "            if j==0:\n",
    "                ax[i].plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), \n",
    "                           '-', linewidth=2, color=AOI_color, label='Glacier boundaries')\n",
    "            else:\n",
    "                ax[i].plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), \n",
    "                           '-', linewidth=2, color=AOI_color, label='_nolegend')\n",
    "    else:\n",
    "        ax[i].plot(np.divide(AOI.geometry[0].exterior.coords.xy[0], 1e3), np.divide(AOI.geometry[0].exterior.coords.xy[1], 1e3),\n",
    "                   '-', color=AOI_color, label='Glacier boundaries')\n",
    "    # Classified points\n",
    "    for data_pts_fn in data_pts_fns:\n",
    "        data_pts = gpd.read_file(data_pts_fn)\n",
    "        data_pts = data_pts.to_crs(crs)\n",
    "        if 'snow.' in data_pts_fn:\n",
    "            color = colors_classified[0]\n",
    "            label = 'Snow'\n",
    "        elif 'snow-shadowed' in data_pts_fn:\n",
    "            if i==0:\n",
    "                color = colors_classified[0]\n",
    "                data_pts = data_pts.iloc[np.arange(0, len(data_pts), step=2), :]\n",
    "            else:\n",
    "                color = colors_classified[1]\n",
    "            label = 'Shadowed snow'\n",
    "        elif 'ice' in data_pts_fn:\n",
    "            color = colors_classified[2]\n",
    "            label = 'Ice'\n",
    "        elif 'rock' in data_pts_fn:\n",
    "            color = colors_classified[3]\n",
    "            label = 'Rock'\n",
    "        elif 'water' in data_pts_fn:\n",
    "            color = colors_classified[4]\n",
    "            label = 'Water'\n",
    "        for j, point in enumerate(data_pts['geometry'].values):\n",
    "            if j==0:\n",
    "                legend_label = label\n",
    "            else:\n",
    "                legend_label = '_nolegend'\n",
    "            ax[i].plot(np.divide(point.geoms[0].coords.xy[0], 1e3), np.divide(point.geoms[0].coords.xy[1], 1e3), \n",
    "                       '.', color=color, markersize=4, label=legend_label)\n",
    "\n",
    "        ax[i].set_xlim(575, 583)\n",
    "        ax[i].set_ylim(7014, 7020.5)\n",
    "    # text label\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0]) * 0.05 + ax[i].get_xlim()[0], \n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0]) * 0.9 + ax[i].get_ylim()[0],\n",
    "               text_labels[i], fontsize=fontsize+2, fontweight='bold', bbox=dict(facecolor='white', edgecolor='none', pad=5))\n",
    "    # axes ticks\n",
    "    ax[i].set_xticks(np.arange(576, 583, 2))\n",
    "    ax[i].set_yticks(np.arange(7014, 7021, 2))\n",
    "        \n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "legend = ax[1].legend(loc='lower right', bbox_to_anchor=[0.5, 1.05, 0.2, 0.2], markerscale=5, ncols=5, frameon=1)\n",
    "for line in legend.get_lines():\n",
    "    line.set_linewidth(3.0)\n",
    "frame = legend.get_frame()\n",
    "frame.set_color('#bdbdbd')\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig03_example_training_points.png')\n",
    "    fig.savefig(fig_fn, dpi=250, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e04057",
   "metadata": {},
   "source": [
    "## Figure 5. Methods workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefeb168",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "font_size = 32\n",
    "save_figures = 1\n",
    "\n",
    "# -----Load dataset dictionary\n",
    "with open(base_path + 'inputs-outputs/datasets_characteristics.json') as fn:\n",
    "    dataset_dict = json.load(fn)\n",
    "dataset = 'PlanetScope'\n",
    "\n",
    "# -----Image settings\n",
    "# site name\n",
    "site_name = 'SouthCascade'\n",
    "# create colormap for classified image\n",
    "cmp = ListedColormap(colors_classified)\n",
    "\n",
    "# -----Load AOI as gpd.GeoDataFrame\n",
    "AOI_fn = study_sites_path + site_name + '/AOIs/' + site_name + '_USGS_*.shp'\n",
    "AOI_fn = glob.glob(AOI_fn)[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "# reproject the AOI to WGS to solve for the optimal UTM zone\n",
    "AOI_WGS = AOI.to_crs(4326)\n",
    "AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "# reproject AOI to UTM\n",
    "AOI_UTM = AOI.to_crs(str(epsg_UTM))\n",
    "\n",
    "# -----Load DEM as Xarray DataSet\n",
    "DEM_fn = study_sites_path + site_name + '/DEMs/' + site_name + '*_DEM*.tif'\n",
    "# load DEM as xarray DataSet\n",
    "DEM_fn = glob.glob(DEM_fn)[0]\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rename({'band_data': 'elevation'})\n",
    "# reproject the DEM to the optimal UTM zone\n",
    "DEM = DEM.rio.reproject(str('EPSG:'+epsg_UTM))\n",
    "# remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "if len(np.shape(DEM.elevation.data))>2:\n",
    "    DEM['elevation'] = DEM.elevation[0]\n",
    "    \n",
    "# -----1. Raw image\n",
    "im_path = study_sites_path + site_name + '/imagery/PlanetScope/mosaics/'\n",
    "im_fn = '20210924_18.tif'\n",
    "im = xr.open_dataset(im_path + im_fn)\n",
    "# determine image date from image mosaic file name\n",
    "im_date = im_fn[0:4] + '-' + im_fn[4:6] + '-' + im_fn[6:8] + 'T' + im_fn[9:11] + ':00:00'\n",
    "im_dt = np.datetime64(im_date)\n",
    "xmin, xmax, ymin, ymax = np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)\n",
    "# plot\n",
    "fig1, ax1 = plt.subplots(figsize=(8,8))\n",
    "ax1.imshow(np.dstack([im.band_data.data[2]/1e4, im.band_data.data[1]/1e4, im.band_data.data[0]/1e4]), \n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI.plot(ax=ax1, facecolor='none', edgecolor='k', linewidth=3)\n",
    "ax1.set_xlim(xmin, xmax)\n",
    "ax1.set_ylim(ymin, ymax)\n",
    "ax1.axis('off')\n",
    "\n",
    "# -----2. Adjusted image\n",
    "polygon_top, polygon_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "im_adj, im_adj_method = f.planetscope_adjust_image_radiometry(im, im_dt, polygon_top, polygon_bottom, dataset_dict, skip_clipped=False)\n",
    "# plot\n",
    "fig2, ax2 = plt.subplots(figsize=(8,8))\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]), \n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI_UTM.plot(ax=ax2, facecolor='none', edgecolor='k', linewidth=3)\n",
    "ax2.set_xlim(xmin, xmax)\n",
    "ax2.set_ylim(ymin, ymax)\n",
    "ax2.axis('off')\n",
    "\n",
    "# -----3. Classified image\n",
    "im_classified_path = study_sites_path + site_name + '/imagery/classified/'\n",
    "im_classified_fn = '20210924T180000_SouthCascade_PlanetScope_classified.nc'\n",
    "im_classified = xr.open_dataset(im_classified_path + im_classified_fn)\n",
    "# remove no data values\n",
    "im_classified = xr.where(im_classified==-9999, np.nan, im_classified)\n",
    "# plot\n",
    "fig3, ax3 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "ax3.imshow(im_classified.classified.data[0], cmap=cmp, vmin=1, vmax=5,\n",
    "           extent=(xmin, xmax, ymin, ymax))\n",
    "AOI.plot(ax=ax3, facecolor='none', edgecolor='k', linewidth=3)\n",
    "# plot dummy points for legend\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[0], s=300, label='snow')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[1], s=300, label='shadowed snow')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[2], s=300, label='ice')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[3], s=300, label='rock')\n",
    "ax3.scatter(0, 0, marker='s', color=colors_classified[4], s=300, label='water')\n",
    "ax3.set_xlim(xmin, xmax+300)\n",
    "ax3.set_ylim(ymin, ymax)\n",
    "ax3.legend(loc='center right', bbox_to_anchor=[1.3, 0.7, 0.2, 0.2])\n",
    "ax3.axis('off')\n",
    "\n",
    "# -----4. Snow edges\n",
    "# create binary snow matrix\n",
    "im_binary = xr.where(im_classified.classified.data[0] <=2, 1, 0)\n",
    "# Find contours at a constant value of 0.5 (between 0 and 1)\n",
    "contours = find_contours(im_binary, 0.5)\n",
    "# convert contour points to image coordinates\n",
    "contours_coords = []\n",
    "for contour in contours: \n",
    "    ix = np.round(contour[:,1]).astype(int)\n",
    "    iy = np.round(contour[:,0]).astype(int)\n",
    "    coords = (im_adj.isel(x=ix, y=iy).x.data, # image x coordinates\n",
    "              im_adj.isel(x=ix, y=iy).y.data) # image y coordinates\n",
    "    # zip points together\n",
    "    xy = list(zip([x for x in coords[0]], \n",
    "                  [y for y in coords[1]]))\n",
    "    contours_coords = contours_coords + [xy]\n",
    "# plot\n",
    "fig4, ax4 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "binary_plt = ax4.imshow(im_binary, cmap='Greys')\n",
    "for i, contour in list(zip(np.arange(0,len(contours)), contours)):\n",
    "    if i==0:\n",
    "        plt.plot(contour[:,1], contour[:,0], '-c', label='edges', linewidth=2)\n",
    "    else:\n",
    "        plt.plot(contour[:,1], contour[:,0], '-c', label='_nolegend', linewidth=2)\n",
    "# plot dummy points for legend\n",
    "ax4.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='k', s=100, label='snow')\n",
    "ax4.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='w', s=100, label='no snow')\n",
    "ax4.set_xlim(0,len(im.x.data)+300)\n",
    "ax4.set_ylim(len(im.y.data), 0)\n",
    "ax4.legend(loc='upper right', bbox_to_anchor=[0.9, 0.8, 0.2, 0.2])\n",
    "ax4.axis('off')\n",
    "\n",
    "# -----5. Snow line\n",
    "snowlines_fn = study_sites_path + site_name + '/imagery/snowlines/20210924T180000_SouthCascade_PlanetScope_snowline.csv'\n",
    "snowlines = pd.read_csv(snowlines_fn)\n",
    "snowlines_X = snowlines.snowlines_coords_X.apply(literal_eval)[0]\n",
    "snowlines_Y = snowlines.snowlines_coords_Y.apply(literal_eval)[0]\n",
    "\n",
    "# plot\n",
    "fig5, ax5 = plt.subplots(figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':font_size, 'font.sans-serif':'Arial'})\n",
    "binary_plt = ax5.imshow(im_binary, \n",
    "                        extent=(xmin, xmax, ymin, ymax),\n",
    "                        cmap='Greys')\n",
    "ax5.plot(snowlines_X, snowlines_Y, '.m', label='_nolegend', markersize=10)\n",
    "ax5.plot(-20, -20, 'm', label='snowline', linewidth=5)\n",
    "# plot dummy points for legend\n",
    "ax5.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='k', s=100, label='snow')\n",
    "ax5.scatter(np.array([-10, -9]),np.array([-10, -9]), edgecolor='k', facecolor='w', s=100, label='no snow')\n",
    "ax5.set_xlim(xmin, xmax+300)\n",
    "ax5.set_ylim(ymin, ymax)\n",
    "ax5.legend(loc='center right', bbox_to_anchor=[1.0, 0.6, 0.2, 0.2])\n",
    "ax5.axis('off')\n",
    "plt.show()\n",
    "\n",
    "if save_figures:\n",
    "    fig_fns = ['methods_workflow_1.png', 'methods_workflow_2.png', 'methods_workflow_3.png', \n",
    "               'methods_workflow_4.png', 'methods_workflow_5.png']\n",
    "    for fig_fn, fig in list(zip(fig_fns, [fig1, fig2, fig3, fig4, fig5])):\n",
    "        fig_fn = os.path.join(figures_out_path, fig_fn)\n",
    "        fig.savefig(fig_fn, dpi=300, facecolor='white', edgecolor='none', bbox_inches='tight')\n",
    "        print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd61711-80fd-4b41-a983-0c46346c1cb4",
   "metadata": {},
   "source": [
    "## Figure 6. Manually vs. automatically detected snowlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257056c2-1da9-4f27-94c6-d9705aeb8d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_diff_fns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cebfa96a-cda5-4515-bcbe-847cddfd02a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab file names for snowline comparison\n",
    "sl_diff_fns = sorted(glob.glob(os.path.join(base_path, 'inputs-outputs', 'snowline_performance*.csv')))\n",
    "sl_diff_fns = [x for x in sl_diff_fns if '_stats' not in x]\n",
    "\n",
    "# Set up figure\n",
    "fontsize = 12\n",
    "plt.rcParams.update({'font.size': fontsize, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,6))\n",
    "ax.plot([0, 800], [0, 800], '-k')\n",
    "\n",
    "# Iterate over datasets\n",
    "datasets = ['PlanetScope', 'Sentinel-2_TOA', 'Sentinel-2_SR', 'Landsat']\n",
    "for dataset in datasets:\n",
    "    fn = [x for x in sl_diff_fns if dataset in x][0]\n",
    "    sl_diff = pd.read_csv(fn)\n",
    "\n",
    "    if dataset=='PlanetScope':\n",
    "        marker = '.'\n",
    "        markerfacecolor = color_PlanetScope\n",
    "        markeredgecolor = 'None'\n",
    "        linewidth = 1\n",
    "        markersize = 8\n",
    "    elif dataset=='Landsat':\n",
    "        marker = '^'\n",
    "        markerfacecolor = color_Landsat\n",
    "        markeredgecolor = 'w'\n",
    "        linewidth = 1\n",
    "        markersize = 6\n",
    "    elif dataset=='Sentinel-2_TOA':\n",
    "        marker = '+'\n",
    "        markerfacecolor = color_Sentinel2\n",
    "        markeredgecolor = color_Sentinel2\n",
    "        linewidth = 1\n",
    "        markersize = 6\n",
    "    elif dataset=='Sentinel-2_SR':\n",
    "        marker = 'o'\n",
    "        markerfacecolor = 'None'\n",
    "        markeredgecolor = color_Sentinel2\n",
    "        linewidth = 1\n",
    "        markersize = 6\n",
    "\n",
    "    # Remove minimum snowline altitude\n",
    "    sl_diff['snowline_est_elev_median_adj'] = sl_diff['snowline_est_elev_median'] - sl_diff.groupby('study_site')['snowline_est_elev_median'].transform('min')    \n",
    "    sl_diff['snowline_obs_elev_median_adj'] = sl_diff['snowline_obs_elev_median'] - sl_diff.groupby('study_site')['snowline_est_elev_median'].transform('min')\n",
    "\n",
    "        \n",
    "    ax.plot(sl_diff['snowline_obs_elev_median_adj'], sl_diff['snowline_est_elev_median_adj'], \n",
    "            marker=marker, linestyle='None', markerfacecolor=markerfacecolor, markeredgecolor=markeredgecolor, \n",
    "            linewidth=linewidth, markersize=markersize, label=dataset.replace('_',' '))\n",
    "\n",
    "ax.legend(ncols=len(datasets), loc='upper center', markerscale=2, bbox_to_anchor=[0.4, 0.9, 0.2, 0.2],\n",
    "         frameon=False, labelspacing=0.3, handlelength=0.15)\n",
    "ax.set_xlabel('Manual median snowline altitude [m]')\n",
    "ax.set_ylabel('Automatic median snowline altitude [m]')\n",
    "ax.set_xlim(-10, 825)\n",
    "ax.set_ylim(-10, 825)\n",
    "ax.grid()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig_fn = os.path.join(figures_out_path, 'fig06_manual_vs_automatic_snowlines.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('Figure saved to file:', fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918f98af",
   "metadata": {},
   "source": [
    "## Figures 7 and 8. Timseries of SCA, weekly median trends for AAR and median snowline altitudes for the USGS Benchmark Glaciers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441605f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# -----Define site names\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade']\n",
    "\n",
    "# -----Load and compile snowlines\n",
    "sl_ests_full = pd.DataFrame()\n",
    "for site_name in site_names:\n",
    "\n",
    "    print(site_name)\n",
    "    \n",
    "    # load estimated snowlines  \n",
    "    sl_ests_path = f'/Volumes/LaCie/raineyaberle/Research/PhD/write-ups/CH1_snow_cover_mapping_methods_manuscript/Aberle_et_al_dataset_submission/{site_name}'\n",
    "    sl_ests_fn = os.path.join(sl_ests_path, f'{site_name}_snow_cover_stats.csv')\n",
    "    sl_ests = pd.read_csv(sl_ests_fn)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "\n",
    "    # concatenate to full df\n",
    "    sl_ests_full = pd.concat([sl_ests_full, sl_ests])\n",
    "\n",
    "# reset index, add year and week columns\n",
    "sl_ests_full.reset_index(drop=True, inplace=True)\n",
    "sl_ests_full['Year'] = sl_ests_full['datetime'].dt.year\n",
    "sl_ests_full['Week'] = sl_ests_full['datetime'].dt.isocalendar().week\n",
    "sl_ests_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab1468a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_names = ['Gulkana', 'Wolverine', 'LemonCreek', 'Sperry', 'SouthCascade']\n",
    "site_names_display = [x.replace('C', ' C') for x in site_names]\n",
    "text_labels1 = ['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "text_labels2 = [['(a)', '(b)', '(c)'], ['(d)', '(e)', '(f)'], ['(g)', '(h)', '(i)'], \n",
    "                ['(j)', '(k)', '(l)'], ['(m)', '(n)', '(o)']]\n",
    "\n",
    "# -----Set up figures\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# time series: SCA\n",
    "fig1, ax1 = plt.subplots(5, 1, figsize=(10, 12))\n",
    "# weekly median trends: SCA, AAR, median snowline elevation\n",
    "alpha = 0.9\n",
    "color_aar, color_snowline = colors_classified[0], '#FB65FB'\n",
    "fig2, ax2 = plt.subplots(5, 3, figsize=(8, 12), width_ratios=[2, 2, 1])\n",
    "\n",
    "# Path to USGS data for hypsometry\n",
    "usgs_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS'\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name, site_name_display, text_label, i in list(zip(site_names, site_names_display, text_labels1, np.arange(0,len(site_names)))):\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # subset snowlines\n",
    "    sl_ests = sl_ests_full.loc[sl_ests_full['site_name']==site_name].reset_index(drop=True)\n",
    "    sl_ests = sl_ests.sort_values(by='datetime')\n",
    "        \n",
    "    # -----Define axis limits\n",
    "    xmin, xmax = np.datetime64('2013-05-01T00:00:00'), np.datetime64('2023-12-01T00:00:00')\n",
    "    sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "    sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    yrange1 = [0, np.nanmax(sl_ests['SCA_m2']) * 1e-6 * 1.1]\n",
    "    yrange2_sca = yrange1\n",
    "    yrange2_aar = [-0.1, 1.1]\n",
    "    min_snowline_elev, max_snowline_elev = np.nanmin(sl_ests['snowline_elevs_median_m']), np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    max_area = np.nanmax(sl_ests['SCA_m2']) \n",
    "    yrange2_snowline = [min_snowline_elev * 0.97, max_snowline_elev * 1.02]\n",
    "\n",
    "    # -----Plot light grey boxes where no observations exist on SCA plots\n",
    "    years = np.arange(2012, 2023, step=1)\n",
    "    for year in years:\n",
    "        min_date, max_date = np.datetime64(str(year) + '-11-01'), np.datetime64(str(year+1) + '-05-01')\n",
    "        rect = matplotlib.patches.Rectangle((min_date, yrange1[0]), width=max_date-min_date, height=yrange1[1]-yrange1[0], color='#d9d9d9')\n",
    "        ax1[i].add_patch(rect)\n",
    "        \n",
    "    # -----Plot SCA time series\n",
    "    # PlanetScope\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['source']=='PlanetScope'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['source']=='PlanetScope'].values, 1e6), \n",
    "                '.', markeredgecolor='w', markerfacecolor=color_PlanetScope, \n",
    "                alpha=alpha, markersize=8, markeredgewidth=1, label='PlanetScope')\n",
    "    # Sentinel-2 SR\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['source']=='Sentinel-2_SR'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['source']=='Sentinel-2_SR'].values, 1e6), \n",
    "                'o', markeredgecolor=color_Sentinel2, markerfacecolor='None', linewidth=1,\n",
    "                alpha=alpha, markersize=4, markeredgewidth=1, label='Sentinel-2 SR')\n",
    "    # Sentinel-2 TOA\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['source']=='Sentinel-2_TOA'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['source']=='Sentinel-2_TOA'].values, 1e6), \n",
    "                '+', markeredgecolor=color_Sentinel2, markerfacecolor=color_Sentinel2, linewidth=2, \n",
    "                alpha=alpha, markersize=6, markeredgewidth=1.2, label='Sentinel-2 TOA')  \n",
    "    # Landsat\n",
    "    ax1[i].plot(sl_ests['datetime'].loc[sl_ests['source']=='Landsat'], \n",
    "                np.divide(sl_ests['SCA_m2'].loc[sl_ests['source']=='Landsat'].values, 1e6), \n",
    "                '^', markeredgecolor='w', markerfacecolor=color_Landsat, \n",
    "                alpha=alpha, markersize=6, markeredgewidth=1, label='Landsat')    \n",
    "\n",
    "    # -----Plot max glacier area on SCA plots\n",
    "    ax1[i].plot([np.datetime64('2013-01-01'), np.datetime64('2024-01-01')], \n",
    "                [max_area / 1e6, max_area / 1e6], '--', color='grey')\n",
    "\n",
    "    # -----Plot min and max elevations on snowline plots\n",
    "    ax2[i,1].plot([15, 45], [min_snowline_elev, min_snowline_elev], '--', color='grey')\n",
    "    ax2[i,1].plot([15, 45], [max_snowline_elev, max_snowline_elev], '--', color='grey')\n",
    "    \n",
    "    # -----Adjust axes display settings\n",
    "    ax1[i].set_xlim(xmin, xmax)\n",
    "    ax1[i].set_ylim(yrange1[0], yrange1[1])\n",
    "    ax1[i].set_xticks([np.datetime64(str(year)+'-01-01') for year in np.arange(2013, 2024)])\n",
    "    if site_name == site_names[-1]:\n",
    "        ax1[i].set_xticklabels([str(year) for year in np.arange(2013, 2024)])\n",
    "    else:\n",
    "        ax1[i].set_xticklabels([])\n",
    "    ax1[i].grid(True)\n",
    "    ax1[i].text(np.datetime64('2013-02-01'),\n",
    "                (ax1[i].get_ylim()[1] - ax1[i].get_ylim()[0]) * 0.06 + ax1[i].get_ylim()[0],\n",
    "                text_label + ' ' + site_name_display + ' (N=' + str(len(sl_ests)) + ')',\n",
    "                bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    if site_name=='SouthCascade':\n",
    "        ax1[i].set_yticks(np.arange(0, 2.4, step=0.4))\n",
    "    if i==2:\n",
    "        ax1[i].set_ylabel('Snow-covered area [km$^2$]', fontsize=fontsize+2)\n",
    "    ax2[i,0].set_ylabel(site_name_display)\n",
    "        \n",
    "    # -----Calculate median and interquartile ranges for weekly trends\n",
    "    q1, q3 = 0.25, 0.75 # define quartiles\n",
    "    # calculate weekly trends using only Sentinel-2 snowlines\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['source']!='PlanetScope']   \n",
    "    for ax, column, color, yrange, ylabel in list(zip([ax2[i,0], ax2[i,1]], \n",
    "                                                      ['AAR', 'snowline_elevs_median_m'],\n",
    "                                                      [color_aar, color_snowline],\n",
    "                                                      [yrange2_aar, yrange2_snowline],\n",
    "                                                      ['Transient AAR', 'Median snowline altitude [m]'])):\n",
    "        weekly = sl_ests_noPS.groupby(by='Week')[column].agg(['median', lambda x: x.quantile(q1), lambda x: x.quantile(q3)])\n",
    "        weekly.columns = ['Median', 'Q1', 'Q3'] # Rename the columns for clarity\n",
    "        weekly.index = weekly.index.astype(float)\n",
    "        # plot\n",
    "        ax.fill_between(weekly.index, weekly['Q1'], weekly['Q3'].values, color=color, alpha=0.5)\n",
    "        ax.plot(weekly.index, weekly['Median'], color=color, linewidth=2)\n",
    "        ax.grid(True)\n",
    "        # adjust x axis\n",
    "        ax.set_xlim(15, 45)\n",
    "        ax.set_xticks([18, 31, 44])\n",
    "        ax.set_xticklabels([])\n",
    "        ax.set_xticklabels(['May', 'Aug', 'Nov'])\n",
    "        # adjust y axis\n",
    "        ax.set_ylim(yrange[0], yrange[1])\n",
    "        if column=='AAR':\n",
    "            ax.set_yticks([0, 0.25, 0.5, 0.75, 1])\n",
    "            ax.set_yticklabels(['0.0', '', '0.5', '', '1.0'])\n",
    "        if i==0:\n",
    "            ax.set_title(ylabel)\n",
    "\n",
    "    # ----Glacier hypsometry histogram\n",
    "    dem_fn = sorted(glob.glob(os.path.join(usgs_path, 'DEMs', site_name, '*.tif')))[-1]\n",
    "    dem = rxr.open_rasterio(dem_fn)\n",
    "    aoi_fn = glob.glob(os.path.join(usgs_path, 'glacierBoundaries', site_name, 'shapefile', '*.shp'))[0]\n",
    "    aois = gpd.read_file(aoi_fn)\n",
    "    aoi = gpd.GeoDataFrame(pd.DataFrame(aois.iloc[-1]).transpose(), geometry='geometry', crs=aois.crs)\n",
    "    dem_clip = dem.rio.clip(aoi.geometry)\n",
    "    dem_clip = xr.where(dem_clip < 0, np.nan, dem_clip)\n",
    "    ax2[i,2].hist(np.ravel(dem_clip.data[0]), orientation='horizontal', bins=50, color='grey')\n",
    "    ax2[i,2].spines[['right', 'top']].set_visible(False)\n",
    "    ax2[i,2].set_ylim(ax2[i,1].get_ylim())\n",
    "    ax2[i,2].set_ylabel('Elevation [m]')\n",
    "\n",
    "ax2[-1,2].set_xlabel('Counts')\n",
    "\n",
    "# -----Add text labels to figure 2\n",
    "for i in range(len(site_names)):\n",
    "    for j in [0,1,2]:\n",
    "        if j==2:\n",
    "            xscale = 0.92\n",
    "        else:\n",
    "            xscale = 0.08\n",
    "        ax2[i,j].text((ax2[i,j].get_xlim()[1] - ax2[i,j].get_xlim()[0]) * xscale + ax2[i,j].get_xlim()[0],\n",
    "                      (ax2[i,j].get_ylim()[1] - ax2[i,j].get_ylim()[0]) * 0.2 + ax2[i,j].get_ylim()[0],\n",
    "                      text_labels2[i][j], fontweight='bold', fontsize=fontsize+2,\n",
    "                      bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# -----Add legend to figure 1\n",
    "ax1[0].legend(loc='center', bbox_to_anchor=(0.5, 1.1), handletextpad=0.1, labelspacing=0.5, markerscale=2, ncol=4)\n",
    "\n",
    "fig1.tight_layout()\n",
    "fig2.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig1_fn = os.path.join(figures_out_path, 'fig07_timeseries_SCA.png')\n",
    "    fig1.savefig(fig1_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 1 saved to file: ' + fig1_fn)\n",
    "    fig2_fn = os.path.join(figures_out_path, 'fig08_weekly_median_trends_no_PlanetScope.png')\n",
    "    fig2.savefig(fig2_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 2 saved to file: ' + fig2_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c818645",
   "metadata": {},
   "source": [
    "### Testing other plots... weekly medians excluding PlanetScope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cde754",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(site_names), 1, figsize=(10, 18))\n",
    "cmap = matplotlib.cm.viridis\n",
    "\n",
    "for i, site_name in enumerate(site_names):\n",
    "    sl_ests = sl_ests_full.loc[sl_ests_full['site_name']==site_name].reset_index(drop=True)\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']\n",
    "    years = [2017, 2018, 2019, 2020, 2021, 2022]\n",
    "    for j, year in enumerate(years):\n",
    "        sl_ests_year = sl_ests_noPS.loc[sl_ests['Year']==year]\n",
    "        sl_ests_year_weekly_median = sl_ests_year.groupby('Week')['SCA_m2'].mean()\n",
    "        # ax[i].plot(sl_ests_year_weekly_median.index.values, np.divide(sl_ests_year_weekly_median.values, 1e6), \n",
    "        #         '.-', color=cmap(j/len(unique_years)), label=year)\n",
    "        # convert year and week to date for plotting\n",
    "        weekly_median = pd.DataFrame({'Week': sl_ests_year_weekly_median.index.values,\n",
    "                                      'SCA_m2': sl_ests_year_weekly_median.values,\n",
    "                                      'Formatted Date': year * 1000 + sl_ests_year_weekly_median.index * 10})\n",
    "        weekly_median['date'] = pd.to_datetime(weekly_median['Formatted Date'], format='%Y%W%w')\n",
    "        ax[i].plot(weekly_median['date'], np.divide(weekly_median['SCA_m2'], 1e6), '.-', color=cmap(j/len(years)))\n",
    "\n",
    "    ax[i].set_ylabel('Snow-covered area [km$^2$]')\n",
    "    ax[i].grid()\n",
    "    \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240cabf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Plot only USGS ELAs over time\n",
    "usgs_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/benchmarkGlacier_massBalance/'\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "col = plt.cm.viridis\n",
    "for i, site_name in enumerate(site_names):\n",
    "    usgs_fn = usgs_path + site_name+'/Output_'+site_name+'_Glacier_Wide_solutions_calibrated.csv'\n",
    "    usgs_file = pd.read_csv(usgs_fn)\n",
    "    ELA = usgs_file['ELA']\n",
    "    ELA_date = usgs_file['Ba_Date'].astype('datetime64[ns]')\n",
    "    plt.plot(ELA_date, ELA, '.-', color=col((i+1)/len(site_names)), label=site_name)\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadfde49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print stats for SCA\n",
    "\n",
    "# -----Loop through sites\n",
    "for site_name in site_names:\n",
    "    \n",
    "    print(site_name)\n",
    "    \n",
    "    # load estimated snow lines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = gpd.GeoDataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "    \n",
    "    # identify min and max SCAs\n",
    "    imin = np.argwhere(sl_ests['SCA_m2'].values==np.nanmin(sl_ests['SCA_m2'].values))[0][0]\n",
    "    SCA_min, SCA_min_date = sl_ests.iloc[imin]['SCA_m2'], sl_ests.iloc[imin]['datetime']\n",
    "    print('Minimum SCA: ' + str(SCA_min) + ' m^2 on ' + str(SCA_min_date))\n",
    "    imax = np.argwhere(sl_ests['SCA_m2'].values==np.nanmax(sl_ests['SCA_m2'].values))[0][0]\n",
    "    SCA_max, SCA_max_date = sl_ests.iloc[imax]['SCA_m2'], sl_ests.iloc[imax]['datetime']\n",
    "    print('Maximum SCA: ' + str(SCA_max) + ' m^2 on ' + str(SCA_max_date)       )  \n",
    "    print(' ')\n",
    "          "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf5bd7a",
   "metadata": {},
   "source": [
    "## Figure 9. Example shortcomings and successes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4ec3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Initialize GEE\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# -----Query GEE for images\n",
    "im_dates = ['20190820T125044', '20190808T212744', '20190731T125046', '20190706T211748']\n",
    "im_site_names = ['Sperry', 'Gulkana', 'Sperry', 'Gulkana']\n",
    "im_datasets = ['Sentinel-2_TOA', 'Sentinel-2_SR', 'Sentinel-2_SR', 'Sentinel-2_SR']\n",
    "data_path = '/Users/raineyaberle/Research/glacier_snow_cover_mapping/'\n",
    "AOIs_UTM = []\n",
    "epsg_UTMs = []\n",
    "ims_ds = []\n",
    "for im_date, site_name, dataset in zip(im_dates, im_site_names, im_datasets):\n",
    "    # load AOI\n",
    "    AOI_path =  os.path.join(data_path, site_name, 'AOIs')\n",
    "    AOI_fn = sorted(glob.glob(os.path.join(AOI_path, '*Glacier_Boundaries_*.shp')))[-1]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "    # solve for optimal UTM zone\n",
    "    AOI_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                    AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_centroid[0], AOI_centroid[1])\n",
    "    epsg_UTMs.append(epsg_UTM)\n",
    "    AOI_UTM = AOI_WGS.to_crs('EPSG:' + epsg_UTM)\n",
    "    AOIs_UTM.append(AOI_UTM)  # append to list of AOIs\n",
    "    \n",
    "    # query GEE for imagery\n",
    "    im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "    date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "    month_start, month_end = 1, 12\n",
    "    cloud_cover_max = 100\n",
    "    mask_clouds=True\n",
    "    im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, \n",
    "                                    cloud_cover_max, mask_clouds)[0]\n",
    "    # reproject to UTM\n",
    "    im_ds = im_ds.rio.reproject('EPSG:' + epsg_UTM)\n",
    "    ims_ds.append(im_ds)  # append to list of images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc87fe13",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fontsize = 16\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(3, 4, figsize=(14,8), gridspec_kw={'width_ratios':[1, 1, 2, 2], 'height_ratios':[1, 1, 0.5]})\n",
    "ax = ax.flatten()\n",
    "text_labels = [['(a)', '(b)'], ['(c)', '(d)'], ['(e)', '(f)'], ['(g)', '(h)']]\n",
    "\n",
    "data_path = '/Users/raineyaberle/glacier_snow_cover_mapping/study-sites/'\n",
    "\n",
    "# -----Iterate over images\n",
    "for i, (im_date, site_name, dataset, text_label, AOI, epsg_UTM, im_ds) in enumerate(zip(im_dates, im_site_names, \n",
    "                                                                              im_datasets, text_labels, AOIs_UTM, epsg_UTMs, ims_ds)):\n",
    "        \n",
    "    # classified image\n",
    "    im_classified_path = os.path.join(data_path, site_name, 'imagery', 'classified')\n",
    "    im_classified_fn = glob.glob(os.path.join(im_classified_path, im_date + '_' + site_name + '_' + dataset + '*.nc'))[0]\n",
    "    im_classified = xr.open_dataset(im_classified_fn)\n",
    "    im_classified = xr.where((im_classified < 1e3) & (im_classified != -9999) & (im_classified!=0), im_classified, np.nan)\n",
    "    im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "    # snowline\n",
    "    sl_path = os.path.join(data_path, site_name, 'imagery', 'snowlines')\n",
    "    sl_fn = glob.glob(os.path.join(sl_path, im_date + '_' + site_name + '_' + dataset + '*.csv'))[0]\n",
    "    sl_df = pd.read_csv(sl_fn)\n",
    "    # plot RGB image\n",
    "    ax[2*i].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                              im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                              im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "                   extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                           np.min(im_ds.y.data), np.max(im_ds.y.data)))    \n",
    "    # plot classified image\n",
    "    im_classified = im_classified.rio.reproject('EPSG:' + epsg_UTM)\n",
    "    im_classified = xr.where(im_classified < 1e3, im_classified, np.nan)\n",
    "    ax[1+2*i].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "                     extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                             np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "    # plot snowline\n",
    "    if sl_df['geometry'][0] != '[]':\n",
    "        sl_df['geometry'] = sl_df['geometry'].apply(wkt.loads)\n",
    "        ax[2*i].plot(*sl_df['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "        ax[1+2*i].plot(*sl_df['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "    # set same limits on axes\n",
    "    ax[2*i].set_xlim(ax[1+2*i].get_xlim())\n",
    "    ax[2*i].set_ylim(ax[1+2*i].get_ylim())\n",
    "    # add text labels\n",
    "    ax[2*i].text((ax[2*i].get_xlim()[1] - ax[2*i].get_xlim()[0]) * 0.8 + ax[2*i].get_xlim()[0],\n",
    "                 (ax[2*i].get_ylim()[1] - ax[2*i].get_ylim()[0]) * 0.1 + ax[2*i].get_ylim()[0],\n",
    "                 text_label[0], fontweight='bold', bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "    ax[1+2*i].text((ax[1+2*i].get_xlim()[1] - ax[1+2*i].get_xlim()[0]) * 0.8 + ax[1+2*i].get_xlim()[0],\n",
    "                   (ax[1+2*i].get_ylim()[1] - ax[1+2*i].get_ylim()[0]) * 0.1 + ax[1+2*i].get_ylim()[0],\n",
    "                   text_label[1], fontweight='bold', fontsize=fontsize+2,\n",
    "                   bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "\n",
    "# add dummy points for legend\n",
    "xmin, xmax = ax[2].get_xlim()\n",
    "ymin, ymax = ax[2].get_ylim()\n",
    "ax[2].plot([-10, -10], [-10, -20], '-m', linewidth=3, label='snowline')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[0], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='snow')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[1], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='shadowed snow')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[2], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='ice')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[3], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='rock')\n",
    "ax[2].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[4], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='water')\n",
    "ax[2].set_xlim(xmin, xmax)\n",
    "ax[2].set_ylim(ymin, ymax)\n",
    "handles, labels = ax[2].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc = (0.015, 0.0))\n",
    "\n",
    "# add arrows to areas of interest\n",
    "for axis in [ax[0], ax[1], ax[4], ax[5]]:\n",
    "    axis.arrow(297.3e3, 5388.8e3, -0.15e3, 0.15e3, width=30, fill=True, edgecolor='k', facecolor='w')\n",
    "for axis in [ax[2], ax[3], ax[6], ax[7]]:\n",
    "    axis.arrow(581.8e3, 7020e3, -0.8e3, -0.8e3, width=110, fill=True, edgecolor='k', facecolor='w')\n",
    "\n",
    "# remove empty axes\n",
    "for axis in ax[8:]:\n",
    "    axis.remove()\n",
    "\n",
    "# remove axis ticks\n",
    "for axis in ax:\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, \n",
    "                          'fig06_example_successes+shortcomings.png')\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c12c8",
   "metadata": {},
   "source": [
    "## Figure 10. Firn detection at Wolverine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fde87e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Initialize GEE\n",
    "try:\n",
    "    ee.Initialize()\n",
    "except:\n",
    "    ee.Authenticate()\n",
    "    ee.Initialize()\n",
    "\n",
    "# -----Load AOI\n",
    "site_name = 'Wolverine'\n",
    "AOI_path = '/Users/raineyaberle/glacier_snow_cover_mapping/study-sites/Wolverine/AOIs/'\n",
    "AOI_fn = glob.glob(os.path.join(AOI_path, 'Wolverine_Glacier_Boundaries_20201019.shp'))[0]\n",
    "AOI_UTM = gpd.read_file(AOI_fn)\n",
    "\n",
    "# -----Query GEE for images\n",
    "im_date = '20190828T212833'\n",
    "datasets = ['Sentinel-2_TOA', 'Sentinel-2_SR']\n",
    "im_ds_list = []\n",
    "for dataset in datasets:\n",
    "    im_dt = np.datetime64(im_date[0:4] + '-' + im_date[4:6] + '-' + im_date[6:8])\n",
    "    date_start, date_end = str(im_dt), str(im_dt + np.timedelta64(1, 'D'))\n",
    "    month_start, month_end = 1, 12\n",
    "    cloud_cover_max = 100\n",
    "    mask_clouds=True\n",
    "    im_ds = f.query_gee_for_imagery(dataset_dict, dataset, AOI_UTM, date_start, date_end, month_start, month_end, cloud_cover_max, mask_clouds)[0]\n",
    "    im_ds_list.append(im_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acb6bde",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Set up figure\n",
    "fontsize = 12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "ax = [fig.add_axes([0.05, 0.4, 0.35, 0.3]), \n",
    "      fig.add_axes([0.43, 0.4, 0.35, 0.3]),\n",
    "      fig.add_axes([0.05, 0.05, 0.35, 0.3]),\n",
    "      fig.add_axes([0.43, 0.05, 0.35, 0.3])\n",
    "     ]\n",
    "\n",
    "# -----Difficult conditions (unsuccessful)\n",
    "im_ds = im_ds_list[0]\n",
    "dataset = 'Sentinel-2_TOA'\n",
    "# classified image\n",
    "im_classified_path = os.path.join(AOI_path, '..', 'imagery', 'classified')\n",
    "im_classified_fn = glob.glob(os.path.join(im_classified_path, im_date + '_' + site_name + '_' + dataset + '*.nc'))[0]\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = xr.where((im_classified!=-9999) & (im_classified!=0), im_classified, np.nan)\n",
    "im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "im_classified = im_classified.rio.reproject(im_ds.rio.crs)\n",
    "# snowline\n",
    "sl_path = os.path.join(AOI_path, '..', 'imagery', 'snowlines')\n",
    "sl_fn = glob.glob(os.path.join(sl_path, im_date + '_' + site_name + '_' + dataset + '*.csv'))[0]\n",
    "sl_df = pd.read_csv(sl_fn)\n",
    "sl_df['geometry'] = sl_df['geometry'].apply(wkt.loads)\n",
    "sl_gdf = gpd.GeoDataFrame(sl_df, crs=sl_df['HorizontalCRS'][0])\n",
    "sl_gdf = sl_gdf.to_crs('EPSG:' + str(im_ds.rio.crs.to_epsg()))\n",
    "# plot\n",
    "ax[0].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "              extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                      np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "im_classified = xr.where(im_classified < 1e3, im_classified, np.nan)\n",
    "ax[1].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "               extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                       np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "# zoom in on firn area\n",
    "ax[0].set_ylim(6698*1e3, 6701.5*1e3)\n",
    "ax[1].set_ylim(6698*1e3, 6701.5*1e3) \n",
    "ax[0].plot(*sl_gdf['geometry'][0].coords.xy, '.', color='orange', markersize=1)\n",
    "ax[1].plot(*sl_gdf['geometry'][0].coords.xy, '.', color='orange', markersize=1)\n",
    "                                        \n",
    "# -----Ideal conditions (successes)\n",
    "im_ds = im_ds_list[1]\n",
    "dataset = 'Sentinel-2_SR'\n",
    "# classified image\n",
    "im_classified_fn = glob.glob(os.path.join(im_classified_path, im_date + '_' + site_name + '_' + dataset + '*.nc'))[0]\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = xr.where((im_classified!=-9999) & (im_classified!=0), im_classified, np.nan)\n",
    "im_classified = im_classified.rio.write_crs('EPSG:4326')\n",
    "im_classified = im_classified.rio.reproject(im_ds.rio.crs)\n",
    "# snowline\n",
    "sl_fn = glob.glob(os.path.join(sl_path, im_date + '_' + site_name + '_' + dataset + '*.csv'))[0]\n",
    "sl_df = pd.read_csv(sl_fn)\n",
    "sl_df['geometry'] = sl_df['geometry'].apply(wkt.loads)\n",
    "sl_gdf = gpd.GeoDataFrame(sl_df, crs=sl_df['HorizontalCRS'][0])\n",
    "im_ds = im_ds_list[1]\n",
    "sl_gdf = sl_gdf.to_crs('EPSG:' + str(im_ds.rio.crs.to_epsg()))\n",
    "# plot\n",
    "ax[2].imshow(np.dstack([im_ds[dataset_dict[dataset]['RGB_bands'][0]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][1]].data[0], \n",
    "                          im_ds[dataset_dict[dataset]['RGB_bands'][2]].data[0]]),\n",
    "              extent=(np.min(im_ds.x.data), np.max(im_ds.x.data), \n",
    "                      np.min(im_ds.y.data), np.max(im_ds.y.data)))\n",
    "im_classified = xr.where(im_classified < 1e3, im_classified, np.nan)\n",
    "ax[3].imshow(im_classified.classified.data[0], cmap=ListedColormap(colors_classified), clim=(1,5),\n",
    "               extent=(np.min(im_classified.x.data), np.max(im_classified.x.data), \n",
    "                       np.min(im_classified.y.data), np.max(im_classified.y.data)))\n",
    "ax[2].plot(*sl_gdf['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "ax[3].plot(*sl_gdf['geometry'][0].coords.xy, '.m', markersize=1)\n",
    "# zoom in on firn area\n",
    "ax[2].set_ylim(6698*1e3, 6701.5*1e3)\n",
    "ax[3].set_ylim(6698*1e3, 6701.5*1e3)\n",
    " \n",
    "# remove axis ticks and labels\n",
    "for axis in ax:\n",
    "    axis.set_xticks([])\n",
    "    axis.set_yticks([])\n",
    "    \n",
    "# add dummy points for legend\n",
    "xmin, xmax = ax[0].get_xlim()\n",
    "ymin, ymax = ax[0].get_ylim()\n",
    "ax[3].plot([-10, -10], [-10, -20], '-', color='orange', linewidth=3, label='Incorrect snowline')\n",
    "ax[3].plot([-10, -10], [-10, -20], '-m', linewidth=3, label='Correct snowline')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[0], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='snow')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[1], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='shadowed snow')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[2], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='ice')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[3], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='rock')\n",
    "ax[3].plot(-10, -10, 's', markersize=20, markerfacecolor=colors_classified[4], \n",
    "             markeredgecolor='k', markeredgewidth=1, label='water')\n",
    "ax[3].set_xlim(xmin, xmax)\n",
    "ax[3].set_ylim(ymin, ymax)\n",
    "\n",
    "# add text labels and arrows indicating firn\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "for i, axis in enumerate(ax):\n",
    "    axis.text(ax[0].get_xlim()[0] + 0.89*(ax[0].get_xlim()[1] - ax[0].get_xlim()[0]),\n",
    "              ax[0].get_ylim()[0] + 0.1*(ax[0].get_ylim()[1] - ax[0].get_ylim()[0]),\n",
    "              text_labels[i], fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='w', pad=5))\n",
    "    axis.arrow(393.8*1e3, 6698.55*1e3, 0, 0.5e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)    \n",
    "    axis.arrow(395*1e3, 6698.63*1e3, 0, 0.5e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "    axis.arrow(395.52*1e3, 6698.4*1e3, 0.4e3, 0.4e3, color='white', width=0.5, head_width=150, head_length=150, length_includes_head=True, zorder=10)\n",
    "\n",
    "# add legend\n",
    "handles, labels = ax[3].get_legend_handles_labels()\n",
    "leg = fig.legend(handles, labels, loc = (0.79, 0.35))\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'fig07_example_firn_detection.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: '+ fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787a99ba",
   "metadata": {},
   "source": [
    "## Figure 11. South Cascade snowline cover elevations distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fdce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()\n",
    "date_start, date_end = '2022-10-02', '2022-10-03'\n",
    "dataset = 'Sentinel-2_SR'\n",
    "site_name = 'SouthCascade'\n",
    "\n",
    "# load AOI\n",
    "AOI_path = '/Volumes/LaCie/raineyaberle/Research/PhD/snow_cover_mapping/study-sites/RGI60-02.18778/AOIs'\n",
    "AOI_fn = glob.glob(os.path.join(AOI_path, 'SouthCascade_USGS_glacier_outline_2021.shp'))[0]\n",
    "AOI = gpd.read_file(AOI_fn)\n",
    "\n",
    "# query GEE for image\n",
    "im_xr = f.query_gee_for_imagery(dataset_dict, dataset, AOI, date_start, date_end, \n",
    "                                month_start=1, month_end=12, cloud_cover_max=70, mask_clouds=True)[0]\n",
    "im_xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58005c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# load classified image\n",
    "im_classified_fn = os.path.join(AOI_path, '..', 'imagery', 'classified',\n",
    "                                '20221002T132103_SouthCascade_Sentinel-2_SR_classified.nc')\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = im_classified.rio.write_crs(\"EPSG:4326\")\n",
    "im_classified = im_classified.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# load snowline\n",
    "snowline_fn = os.path.join(AOI_path, '..', 'imagery', 'snowlines',\n",
    "                           '20221002T132103_RGI60-02.18778_Sentinel-2_SR_snowline.csv')\n",
    "snowline = pd.read_csv(snowline_fn)\n",
    "snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs='EPSG:4326')\n",
    "snowline_gdf['snowline_elevs_m'] = [pd.eval(snowline_gdf['snowline_elevs_m'][0].replace('\\n','').replace(' ',', '))]\n",
    "# reference elevations to the ellipsoid instead for direct comparison\n",
    "geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                 list(zip(snowline_gdf.geometry[0].coords.xy[0], snowline_gdf.geometry[0].coords.xy[1]))]\n",
    "snowline_elevs_m_ellipsoid = [(x+y) for x,y in list(zip(snowline_gdf['snowline_elevs_m'][0], geoid_heights))]\n",
    "\n",
    "# load DEM\n",
    "DEM_path = '/Volumes/LaCie/raineyaberle/Research/PhD/GIS_data/USGS/DEMs/SouthCascade'\n",
    "DEM_fn = os.path.join(DEM_path, 'SouthCascade_2021.08.13_DEM.tif')\n",
    "DEM = xr.open_dataset(DEM_fn)\n",
    "DEM = DEM.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "DEM = xr.where(DEM >= 1e38, np.nan, DEM)\n",
    "DEM = DEM.rio.write_crs('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# clip DEM to AOI and interpolate to classified image coordinates\n",
    "dem_aoi = DEM.rio.clip(AOI.geometry, AOI.crs)\n",
    "dem_aoi_interp = dem_aoi.interp(x=im_classified.x.data, y=im_classified.y.data, method='linear')\n",
    "# add elevation as a band to classified image for convenience\n",
    "im_classified['elevation'] = (('time', 'y', 'x'), dem_aoi_interp.band_data.data)\n",
    "\n",
    "# determine snow covered elevations\n",
    "all_elev = np.ravel(dem_aoi_interp.band_data.data)\n",
    "all_elev = all_elev[~np.isnan(all_elev)]  # remove NaNs\n",
    "snow_est_elev = np.ravel(im_classified.where((im_classified.classified <= 2))\n",
    "                         .where(im_classified.classified != -9999).elevation.data)\n",
    "snow_est_elev = snow_est_elev[~np.isnan(snow_est_elev)]  # remove NaNs\n",
    "\n",
    "# -----Create elevation histograms\n",
    "# determine bins to use in histograms\n",
    "elev_min = np.fix(np.nanmin(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "elev_max = np.round(np.nanmax(np.ravel(im_classified.elevation.data)) / 10) * 10\n",
    "bin_edges = np.linspace(elev_min, elev_max, num=int((elev_max - elev_min) / 10 + 1))\n",
    "bin_centers = (bin_edges[1:] + bin_edges[0:-1]) / 2\n",
    "# calculate elevation histograms\n",
    "hist_elev = np.histogram(all_elev, bins=bin_edges)[0]\n",
    "hist_snow_est_elev = np.histogram(snow_est_elev, bins=bin_edges)[0]\n",
    "hist_snow_est_elev_norm = hist_snow_est_elev / hist_elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9def209",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# PLOT\n",
    "fontsize = 18\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16,8))\n",
    "# RGB image\n",
    "xmin, xmax = np.min(im_xr.x.data)/1e3, np.max(im_xr.x.data)/1e3\n",
    "ymin, ymax = np.min(im_xr.y.data)/1e3, np.max(im_xr.y.data)/1e3\n",
    "ax[0].imshow(np.dstack([im_xr['B4'].data[0], im_xr['B3'].data[0], im_xr['B2'].data[0]]),\n",
    "             extent=(xmin, xmax, ymin, ymax))\n",
    "snowline_gdf = snowline_gdf.to_crs(snowline_gdf['HorizontalCRS'][0])\n",
    "ax[0].plot(np.divide(snowline_gdf.geometry[0].coords.xy[0], 1e3), \n",
    "           np.divide(snowline_gdf.geometry[0].coords.xy[1], 1e3), '.m', markersize=2)\n",
    "x_mesh, y_mesh = np.meshgrid(im_classified.x.data, im_classified.y.data)\n",
    "ax[0].contour(np.divide(x_mesh, 1e3), np.divide(y_mesh, 1e3), im_classified.elevation.data[0], \n",
    "              levels=[np.nanmedian(snowline_elevs_m_ellipsoid)], colors='#ff7f00', linewidths=3)\n",
    "# dummy lines for legend\n",
    "ax[0].plot([0,1], [0,1], '-m', linewidth=2, label='Snowline')\n",
    "ax[0].plot([0,1], [0,1], '-', color='#ff7f00', linewidth=2, label='Median snowline altitude')\n",
    "ax[0].set_xlim(xmin, xmax)\n",
    "ax[0].set_ylim(ymin, ymax)\n",
    "ax[0].legend(loc='lower center', bbox_to_anchor=[0.4, 1.02, 0.2, 0.2], framealpha=1)\n",
    "ax[0].set_xlabel('Easting [km]')\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[0].set_yticks([5356.5, 5357, 5357.5, 5358, 5358.5])\n",
    "ax[0].text((xmax-xmin)*0.08 + xmin, (ymax - ymin)*0.08 + ymin, \n",
    "           '(a)', fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# histograms\n",
    "sl_elev_min = np.nanmin(snowline_elevs_m_ellipsoid)\n",
    "sl_elev_max = np.nanmax(snowline_elevs_m_ellipsoid)\n",
    "rect = Rectangle((sl_elev_min, 0), width=sl_elev_max-sl_elev_min, height=950, facecolor='m', alpha=0.2)\n",
    "ax[1].add_patch(rect)\n",
    "ax[1].bar(bin_centers, hist_elev, width=(bin_centers[1] - bin_centers[0]), color='#238443', \n",
    "          align='center', label='All elevations')\n",
    "ax[1].bar(bin_centers, hist_snow_est_elev, width=(bin_centers[1] - bin_centers[0]), \n",
    "          color=colors_classified[0], align='center', label='Snow-covered elevations')\n",
    "ax[1].plot([np.nanmedian(snowline_elevs_m_ellipsoid), np.nanmedian(snowline_elevs_m_ellipsoid)], [0, 1000], '-',\n",
    "           color='#ff7f00', linewidth=3, label='Median snowline altitude')\n",
    "ax[1].grid()\n",
    "ax[1].set_xlabel('Elevation [m]')\n",
    "ax[1].set_ylabel('Count')\n",
    "ax[1].set_ylim(0, 950)\n",
    "xmin, xmax = ax[1].get_xlim()\n",
    "ymin, ymax = ax[1].get_ylim()\n",
    "ax[1].text((xmax-xmin)*0.05 + xmin, (ymax - ymin)*0.08 + ymin, \n",
    "           '(b)', fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax[1].text(1812, 900, 'Snowline elevation range', color='m', fontweight='bold', \n",
    "           bbox=dict(facecolor='w', edgecolor='None', pad=3))\n",
    "ax[1].arrow(1825, 910, -50, 0, color='m', width=5, length_includes_head=True, head_length=7, head_width=25)\n",
    "ax[1].arrow(2035, 910, 50, 0, color='m', width=5, length_includes_head=True, head_length=7, head_width=25)\n",
    "\n",
    "ax[1].legend(loc='lower center', bbox_to_anchor=[0.4, 1.02, 0.2, 0.2], framealpha=1)\n",
    "ax[1].set_position([0.55, 0.17, 0.5, 0.65])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'fig11_SouthCascade_snow_covered_elevations.png')\n",
    "fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176fafe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load EGM96 geoid heights\n",
    "egm96_fn = os.path.join(base_path, 'inputs-outputs', 'us_nga_egm96_15.tif')\n",
    "egm96 = xr.open_dataset(egm96_fn)\n",
    "egm96 = egm96.rename({'band_data': 'geoid_height'})\n",
    "\n",
    "# load classified image\n",
    "im_classified_fn = os.path.join(study_sites_path, site_name, 'imagery', 'classified',\n",
    "                                '20221002T132103_SouthCascade_Sentinel-2_SR_classified.nc')\n",
    "im_classified = xr.open_dataset(im_classified_fn)\n",
    "im_classified = im_classified.rio.reproject('EPSG:'+str(AOI.crs.to_epsg()))\n",
    "\n",
    "# load snowline\n",
    "snowline_fn = os.path.join(study_sites_path, site_name, 'imagery', 'snowlines',\n",
    "                           '20221002T132103_SouthCascade_Sentinel-2_SR_snowline.csv')\n",
    "snowline = pd.read_csv(snowline_fn)\n",
    "snowline['geometry'] = snowline['geometry'].apply(wkt.loads)\n",
    "snowline_gdf = gpd.GeoDataFrame(snowline, geometry=snowline['geometry'], crs='EPSG:4326')\n",
    "snowline_gdf['snowline_elevs_m'] = [pd.eval(snowline_gdf['snowline_elevs_m'][0].replace('\\n','').replace(' ',', '))]\n",
    "# reference elevations to the ellipsoid instead for direct comparison\n",
    "geoid_heights = [egm96.sel(x=x, y=y, method='nearest').geoid_height.data[0] for x,y in \n",
    "                 list(zip(snowline_gdf.geometry[0].coords.xy[0], snowline_gdf.geometry[0].coords.xy[1]))]\n",
    "snowline_elevs_m_ellipsoid = [(x+y) for x,y in list(zip(snowline_gdf['snowline_elevs_m'][0], geoid_heights))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7e3aa0",
   "metadata": {},
   "source": [
    "## Figure S1. PlanetScope image adjustment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af47eb3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "site_name = 'LemonCreek'\n",
    "\n",
    "# load AOI and DEM\n",
    "aoi_path = '/Users/raineyaberle/Research/glacier_snow_cover_mapping/study-sites/LemonCreek/AOIs'\n",
    "aoi_fn = glob.glob(os.path.join(aoi_path, site_name + '_Glacier_Boundaries_20211005.shp'))[0]\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "dem_path = os.path.join(study_sites_path, site_name, 'DEMs')\n",
    "dem_fn = glob.glob(os.path.join(dem_path, site_name + '*USGS*.tif'))[0]\n",
    "dem = xr.open_dataset(dem_fn)\n",
    "dem = dem.rio.reproject('EPSG:'+str(aoi.crs.to_epsg()))\n",
    "dem = dem.rename({'band_data': 'elevation'})\n",
    "dem = xr.where(dem > 1e38, np.nan, dem)\n",
    "dem = dem.isel(band=0)\n",
    "dem = dem.rio.write_crs('EPSG:'+str(aoi.crs.to_epsg()))\n",
    "\n",
    "# load dynamics ranges of all raw image mosaics\n",
    "ps_im_path = os.path.join(aoi_path, '..', 'imagery', 'PlanetScope', 'mosaics')\n",
    "im_fns = sorted(os.listdir(ps_im_path))\n",
    "im_fns = [x for x in im_fns if not x.startswith('.')]\n",
    "dynamic_ranges = pd.DataFrame()\n",
    "for im_fn in tqdm(im_fns):\n",
    "    im_date = im_fn[0:4] + '-' + im_fn[4:6] + '-' + im_fn[6:8]\n",
    "    im = xr.open_dataset(os.path.join(ps_im_path, im_fn))\n",
    "    im = xr.where(im == -9999, np.nan, im / 1e4)\n",
    "    b_min, b_max = np.nanmin(im.band_data.data[0]), np.nanmax(im.band_data.data[0])\n",
    "    dynamic_range = pd.DataFrame({'date': [im_date],\n",
    "                                  'b_min': [b_min],\n",
    "                                  'b_max': [b_max]})\n",
    "    dynamic_ranges = pd.concat([dynamic_ranges, dynamic_range])\n",
    "dynamic_ranges.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# load raw image\n",
    "im_date = '2020-07-29'\n",
    "im_fn = glob.glob(os.path.join(ps_im_path, im_date.replace('-','') + '*.tif'))[0]\n",
    "im = xr.open_dataset(im_fn)\n",
    "im = im / 1e4\n",
    "\n",
    "# construct elevation polygons\n",
    "polygon_top, polygon_bottom = psp.create_aoi_elev_polys(aoi, dem)\n",
    "\n",
    "# adjust image radiometry\n",
    "im_dt = np.datetime64(im_date)\n",
    "im_adj = psp.planetscope_adjust_image_radiometry(im, im_dt, polygon_top, polygon_bottom, dataset_dict, skip_clipped=False)[0]\n",
    "\n",
    "# PLOT\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "gs = matplotlib.gridspec.GridSpec(5, 2, figure=fig)\n",
    "# dynamic ranges\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "ax0.boxplot(dynamic_ranges[['b_min', 'b_max']], vert=False, bootstrap=100)\n",
    "ax0.set_xlim(0,1.5)\n",
    "ax0.set_yticks([1,2])\n",
    "ax0.set_yticklabels(['B$_{min}$', 'B$_{max}$'])\n",
    "ax0.set_xlabel('Reflectance')\n",
    "aoi_color = '#084594'\n",
    "poly_color = '#41ab5d'\n",
    "# original RGB image\n",
    "ax1 = fig.add_subplot(gs[1:4, 0])\n",
    "ax1.imshow(np.dstack([im.band_data.data[2], im.band_data.data[1], im.band_data.data[0]]),\n",
    "             extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                     np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax1.set_ylabel('Northing [km]')\n",
    "ax1.set_xlabel('Easting [km]')\n",
    "# original band histograms\n",
    "ax3 = fig.add_subplot(gs[4,0])\n",
    "ax3.hist(np.ravel(im.band_data.data[3]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax3.hist(np.ravel(im.band_data.data[2]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax3.hist(np.ravel(im.band_data.data[1]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax3.hist(np.ravel(im.band_data.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax3.grid()\n",
    "ax3.set_xlim(0,1.5)\n",
    "ax3.set_ylim(0,1e5)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_ylabel('Counts')\n",
    "ax3.set_xlabel('Reflectance')\n",
    "# adjusted RGB image\n",
    "ax2 = fig.add_subplot(gs[1:4,1])\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]),\n",
    "             extent=(np.min(im_adj.x.data)/1e3, np.max(im_adj.x.data)/1e3, \n",
    "                     np.min(im_adj.y.data)/1e3, np.max(im_adj.y.data)/1e3))\n",
    "ax2.set_xlabel('Easting [km]')\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax2.legend([aoi_line[0], poly_line[0]], ['Glacier boundary', 'Z$_{P80}$ Polygon'], \n",
    "           loc='center right', bbox_to_anchor=[1.7, 0.5, 0.2, 0.2])\n",
    "# adjusted band histograms\n",
    "ax4 = fig.add_subplot(gs[4,1])\n",
    "ax4.hist(np.ravel(im_adj.NIR.data[0]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax4.hist(np.ravel(im_adj.Blue.data[0]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax4.hist(np.ravel(im_adj.Green.data[0]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax4.hist(np.ravel(im_adj.Red.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax4.legend(loc='center right', bbox_to_anchor=[1.2, 0.4, 0.2, 0.2])\n",
    "ax4.grid()\n",
    "ax4.set_xlim(0,1.5)\n",
    "# ax4.set_ylim(0,1e5)\n",
    "ax4.set_yticklabels([])\n",
    "ax4.set_xlabel('Reflectance')\n",
    "# add text labels\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "for ax, text_label, i in list(zip([ax0, ax1, ax2, ax3, ax4], text_labels, np.arange(0,len(text_labels)))):\n",
    "    if i==0:\n",
    "        xscale, yscale = 0.03, 0.8\n",
    "    elif (i==1) or (i==2):\n",
    "        xscale, yscale = 0.1, 0.93\n",
    "    else:\n",
    "        xscale, yscale = 0.05, 0.8\n",
    "    ax.text((ax.get_xlim()[1] - ax.get_xlim()[0])*xscale + ax.get_xlim()[0],\n",
    "            (ax.get_ylim()[1] - ax.get_ylim()[0])*yscale + ax.get_ylim()[0],\n",
    "            text_label, fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_PlanetScope_image_adjustment.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('saved figure to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c808c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8,12))\n",
    "gs = matplotlib.gridspec.GridSpec(5, 2, figure=fig)\n",
    "# dynamic ranges\n",
    "ax0 = fig.add_subplot(gs[0,:])\n",
    "ax0.boxplot(dynamic_ranges[['b_min', 'b_max']], vert=False, bootstrap=100)\n",
    "ax0.set_xlim(0,1.5)\n",
    "ax0.set_yticks([1,2])\n",
    "ax0.set_yticklabels(['B$_{min}$', 'B$_{max}$'])\n",
    "ax0.set_xlabel('Reflectance')\n",
    "aoi_color = '#084594'\n",
    "poly_color = '#41ab5d'\n",
    "# original RGB image\n",
    "ax1 = fig.add_subplot(gs[1:4, 0])\n",
    "ax1.imshow(np.dstack([im.band_data.data[2], im.band_data.data[1], im.band_data.data[0]]),\n",
    "             extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                     np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax1.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax1.set_yticks(np.arange(6468, 6476, step=2))\n",
    "ax1.set_xticks(np.arange(536, 541, step=2))\n",
    "ax1.set_xlim((536, 540))\n",
    "ax1.set_ylim((6468.3, 6475))\n",
    "ax1.set_ylabel('Northing [km]')\n",
    "ax1.set_xlabel('Easting [km]')\n",
    "# original band histograms\n",
    "ax3 = fig.add_subplot(gs[4,0])\n",
    "ax3.hist(np.ravel(im.band_data.data[3]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax3.hist(np.ravel(im.band_data.data[2]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax3.hist(np.ravel(im.band_data.data[1]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax3.hist(np.ravel(im.band_data.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax3.grid()\n",
    "ax3.set_xlim(0,1.5)\n",
    "# ax3.set_ylim(0,1e5)\n",
    "ax3.set_yticklabels([])\n",
    "ax3.set_ylabel('Counts')\n",
    "ax3.set_xlabel('Reflectance')\n",
    "# adjusted RGB image\n",
    "ax2 = fig.add_subplot(gs[1:4,1])\n",
    "ax2.imshow(np.dstack([im_adj.Red.data[0], im_adj.Green.data[0], im_adj.Blue.data[0]]),\n",
    "             extent=(np.min(im_adj.x.data)/1e3, np.max(im_adj.x.data)/1e3, \n",
    "                     np.min(im_adj.y.data)/1e3, np.max(im_adj.y.data)/1e3))\n",
    "ax2.set_xticks(ax1.get_xticks())\n",
    "ax2.set_yticks(ax1.get_yticks())\n",
    "ax2.set_xlim((536, 540))\n",
    "ax2.set_ylim((6468.3, 6475))\n",
    "ax2.set_xlabel('Easting [km]')\n",
    "for geom in aoi.geometry[0].geoms:\n",
    "    aoi_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '-', color=aoi_color)\n",
    "for geom in polygon_top.geoms:\n",
    "    poly_line = ax2.plot(np.divide(geom.exterior.coords.xy[0], 1e3), np.divide(geom.exterior.coords.xy[1], 1e3), '--', color=poly_color)\n",
    "ax2.legend([aoi_line[0], poly_line[0]], ['Glacier boundary', 'Z$_{P80}$ Polygon'], \n",
    "           loc='center right', bbox_to_anchor=[1.55, 0.5, 0.2, 0.2])\n",
    "# adjusted band histograms\n",
    "ax4 = fig.add_subplot(gs[4,1])\n",
    "ax4.hist(np.ravel(im_adj.NIR.data[0]), color='purple', bins=100, histtype='step', linewidth=2, label='NIR')\n",
    "ax4.hist(np.ravel(im_adj.Blue.data[0]), color='blue', bins=100, histtype='step', linewidth=2, label='B')\n",
    "ax4.hist(np.ravel(im_adj.Green.data[0]), color='green', bins=100, histtype='step', linewidth=2, label='G')\n",
    "ax4.hist(np.ravel(im_adj.Red.data[0]), color='red', bins=100, histtype='step', linewidth=2, label='R')\n",
    "ax4.legend(loc='center right', bbox_to_anchor=[1.2, 0.4, 0.2, 0.2])\n",
    "ax4.grid()\n",
    "ax4.set_xlim(0,1.5)\n",
    "ax4.set_ylim(ax3.get_ylim())\n",
    "ax4.set_yticklabels([])\n",
    "ax4.set_xlabel('Reflectance')\n",
    "# add text labels\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)']\n",
    "for ax, text_label, i in list(zip([ax0, ax1, ax2, ax3, ax4], text_labels, np.arange(0,len(text_labels)))):\n",
    "    if i==0:\n",
    "        xscale, yscale = 0.92, 0.2\n",
    "    elif (i==1) or (i==2):\n",
    "        xscale, yscale = 0.85, 0.1\n",
    "    else:\n",
    "        xscale, yscale = 0.87, 0.22\n",
    "    ax.text((ax.get_xlim()[1] - ax.get_xlim()[0])*xscale + ax.get_xlim()[0],\n",
    "            (ax.get_ylim()[1] - ax.get_ylim()[0])*yscale + ax.get_ylim()[0],\n",
    "            text_label, fontweight='bold', fontsize=fontsize+2, bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(figures_out_path, 'figS1_PlanetScope_image_adjustment.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('saved figure to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589aff3f",
   "metadata": {},
   "source": [
    "## Figure S2. Model learning curves\n",
    "\n",
    "Located in `develop_classifiers.ipynb`step 3. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e3eba",
   "metadata": {},
   "source": [
    "## Figure S3. Impact of slope on ELA sensitivity to air temperature change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4116220d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc339918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define image query settings\n",
    "site_name = 'LemonCreek'\n",
    "dataset = 'Sentinel-2_SR'\n",
    "im_date = '2021-08-31'\n",
    "date_start, date_end = '2021-08-31', '2021-09-01'\n",
    "month_start, month_end = 7, 10\n",
    "cloud_cover_max = 70\n",
    "mask_clouds = False\n",
    "im_out_path = None\n",
    "im_download = False\n",
    "\n",
    "# Load AOI\n",
    "aoi_path = '/Users/raineyaberle/glacier_snow_cover_mapping/study-sites/' + site_name + '/AOIs/'\n",
    "aoi_fn = glob.glob(os.path.join(aoi_path, '*20211005.shp'))[0]\n",
    "aoi = gpd.read_file(aoi_fn)\n",
    "\n",
    "im_list = f.query_gee_for_imagery(dataset_dict, dataset, aoi, date_start, date_end,\n",
    "                                  month_start, month_end, cloud_cover_max, mask_clouds,\n",
    "                                  im_out_path, im_download)\n",
    "im = im_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3331c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dry adiabatic lapse rate ~= 9.8 deg C / km\n",
    "# 0.5 deg C / (9.8 deg C / 1000 m) = 51 m / deg C increase in temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c03bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine whether to plot only contours\n",
    "plot_contours = True\n",
    "\n",
    "# clip image to AOI\n",
    "aoi_sel = aoi.geometry[0].geoms[4]\n",
    "im_clip = im\n",
    "\n",
    "# load DEM\n",
    "dem_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/GIS_data/USGS/DEMs/lemonCreek'\n",
    "dem_fn = glob.glob(os.path.join(dem_path, '*2021*.tif'))[0]\n",
    "dem = xr.open_dataset(dem_fn)\n",
    "# reproject to image CRS\n",
    "dem = dem.rio.reproject('EPSG:'+str(im.rio.crs.to_epsg()))\n",
    "dem = xr.where(dem < 1e38, dem, np.nan)\n",
    "dem = dem.rio.write_crs('EPSG:'+str(im.rio.crs.to_epsg()))\n",
    "# interpolate dem to image coordinates\n",
    "dem_interp = dem.sel(x=im_clip.x.data, y=im_clip.y.data, method='nearest')\n",
    "# mask it using the AOI\n",
    "dem_interp_masked = dem_interp.rio.clip([aoi_sel.buffer(-80)], im.rio.crs, drop=False)\n",
    "z1 = dem_interp.band_data.data[0]\n",
    "z1_masked = dem_interp_masked.band_data.data[0]\n",
    "# remove minimum elevation so it has a minimum of zero\n",
    "z1_min = np.nanmin(z1)\n",
    "z1 = z1 - z1_min\n",
    "z1_masked = z1_masked - z1_min\n",
    "# flip it around to match the RGB image\n",
    "z1 = np.flipud(np.fliplr(z1))\n",
    "z1_masked = np.flipud(np.fliplr(z1_masked))\n",
    "# low slope\n",
    "z2 = z1 / 2\n",
    "z2_masked = z1_masked / 2\n",
    "\n",
    "# set up RGB data\n",
    "red = im_clip['B4'].data[0]\n",
    "green = im_clip['B3'].data[0]\n",
    "blue = im_clip['B2'].data[0]\n",
    "# Ensure color values are between 0 and 1\n",
    "red = np.clip(red, 0, 1)\n",
    "green = np.clip(green, 0, 1)\n",
    "blue = np.clip(blue, 0, 1)\n",
    "# Create the RGB image \n",
    "rgb_image = np.dstack((red, green, blue))\n",
    "# flip around so the glacier flows downhill\n",
    "rgb_image = np.fliplr(np.flipud(rgb_image))\n",
    "# Identify NaN values and set them to the specified color (white)\n",
    "nan_mask = np.isnan(rgb_image).any(axis=2)\n",
    "nan_color = [1, 1, 1]\n",
    "rgb_image[nan_mask] = nan_color\n",
    "cmap = plt.get_cmap('viridis')  # Adjust the colormap as needed\n",
    "cmap.set_bad(color='white')  # Set the color for NaN values to transparent\n",
    "\n",
    "# plot figures\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "ax1 = fig.add_axes((0.1, 0.1, 0.4, 0.8), projection='3d')\n",
    "ax2 = fig.add_axes((0.46, 0.1, 0.4, 0.8), projection='3d')\n",
    "text_labels = ['(a) High slope', '(b) Low slope']\n",
    "ela_inits = 370, 180\n",
    "for ax, z, z_masked, text_label, ela_init in list(zip([ax1, ax2], [z1, z2], [z1_masked, z2_masked], \n",
    "                                                      text_labels, ela_inits)):\n",
    "    X, Y = np.meshgrid(dem_interp.x.data, dem_interp.y.data)\n",
    "    if not plot_contours:\n",
    "        ax.plot_surface(X, Y, z, rstride=1, cstride=1, alpha=1, cmap=cmap, norm=False, facecolors=rgb_image)\n",
    "    # plot elevation contours\n",
    "    X, Y = np.meshgrid(dem_interp_masked.x.data, dem_interp_masked.y.data)\n",
    "    CS = ax.contour(X, Y, z_masked, levels=[ela_init, ela_init + 51], colors='m', alpha=1, \n",
    "                    linestyles=['solid', 'dashed'], linewidths=2) \n",
    "    # remove the axes\n",
    "    ax.set_axis_off()\n",
    "    ax.view_init(elev=20, azim=310)\n",
    "    ax.set_zlim(0, np.nanmax(z1))\n",
    "    # add text label\n",
    "    ax.set_title(text_label, fontweight='bold', x=0.3, y=0.85)\n",
    "\n",
    "# add legend for contours\n",
    "if plot_contours:\n",
    "    h,_ = CS.legend_elements()\n",
    "    ax1.legend([h[0], h[1]], ['ELA$_1$', 'ELA$_2$'], loc='lower left', bbox_to_anchor=[0.15, 0.15, 0.2, 0.2])\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "if save_figures:\n",
    "    if plot_contours:\n",
    "        fig_fn = os.path.join(figures_out_path, 'LemonCreek_3D_RGB_plot_contours.png')\n",
    "        fig.savefig(fig_fn, dpi=300, bbox_inches='tight', transparent=True)\n",
    "    else:\n",
    "        fig_fn = os.path.join(figures_out_path, 'LemonCreek_3D_RGB_plot.png')\n",
    "        fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c77da6a",
   "metadata": {},
   "source": [
    "## Figure X (removed). Testing SCA sensitivity to presence of firn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1157ee5f",
   "metadata": {},
   "source": [
    "### Slope vs. area of misclassified firn %"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ee0ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define ranges in slopes, percents_firn, and calculate changes in SCA\n",
    "slopes = np.arange(0, 51, step=0.5)\n",
    "percents_firn = np.arange(0, 20.1, step=0.5)\n",
    "differences = np.zeros((len(percents_firn), len(slopes)))\n",
    "for i, percent_firn in enumerate(percents_firn):\n",
    "    for j, slope in enumerate(slopes):\n",
    "        differences[i,j] = percent_firn * np.cos(np.radians(slope))\n",
    "# flip array for plotting\n",
    "differences = np.array(differences)\n",
    "differences = np.fliplr(np.flipud(differences))\n",
    "\n",
    "# Plot\n",
    "fontsize=14\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "# distribution of slopes\n",
    "ax1 = fig.add_axes((0.1, 0.7, 0.72, 0.2))\n",
    "hist = ax1.hist(rgis['Slope'].values, bins=50, color='grey', range=(0,50))\n",
    "counts, bins = hist[0], hist[1]\n",
    "ax1.set_xlim(0,50)\n",
    "ax1.axis('off')\n",
    "# heatmap of uncertainties\n",
    "ax2 = fig.add_axes((0.1, 0.1, 0.9, 0.775))\n",
    "diff_im = ax2.imshow(differences, extent=(slopes[0], slopes[-1], percents_firn[0], percents_firn[-1]), \n",
    "                    cmap='inferno', clim=(0,20))\n",
    "ax2.set_ylabel('Misclassified firn [% of SCA]')\n",
    "ax2.set_xlabel('Slope [degrees]')\n",
    "ax2.set_xlim(0,50)\n",
    "ax2.grid()\n",
    "ax2.set_aspect(1.5)\n",
    "ax2.set_yticks(np.arange(0,21, step=5))\n",
    "fig.colorbar(diff_im, ax=ax2, shrink=0.5, label='$\\Delta$SCA [%]', ticks=np.arange(0,21, step=5))\n",
    "# add text labels\n",
    "ax1.text((ax1.get_xlim()[1] - ax1.get_xlim()[0])*0.05,\n",
    "         (ax1.get_ylim()[1] - ax1.get_ylim()[0])*0.1, '(a)', fontweight='bold', fontsize=fontsize+2,\n",
    "         bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "ax2.text((ax2.get_xlim()[1] - ax2.get_xlim()[0])*0.05,\n",
    "         (ax2.get_ylim()[1] - ax2.get_ylim()[0])*0.1, '(b)', fontweight='bold', fontsize=fontsize+2,\n",
    "         bbox=dict(facecolor='white', edgecolor='None', pad=5))\n",
    "# draw box around most common slope\n",
    "box_color = 'black'\n",
    "I = np.argwhere(counts==np.nanmax(counts))[0][0]\n",
    "rect1 = matplotlib.patches.Rectangle((bins[I], 0), width=1, height=np.nanmax(counts), edgecolor=box_color, facecolor='None')\n",
    "ax1.add_patch(rect1)\n",
    "rect2 = matplotlib.patches.Rectangle((bins[I], 0), width=1, height=np.nanmax(percents_firn), edgecolor=box_color, facecolor='None')\n",
    "ax2.add_patch(rect2)\n",
    "# add descriptor text\n",
    "ax1.text(30, np.nanmax(counts)-100, \n",
    "         'Most frequent slopes = (' + str(int(bins[I])) + '\\N{degree sign}, ' + str(int(bins[I+1])) + '\\N{degree sign})',\n",
    "         color=box_color)\n",
    "ax1.text(41.1, np.nanmax(counts)-550,\n",
    "         '$\\Delta$SCA = (' \n",
    "         + str(int(differences[-1, np.argwhere(slopes==bins[I])[0][0]])) + '%, ' \n",
    "         + str(int(differences[0, np.argwhere(slopes==bins[I])[0][0]])) + '%)',\n",
    "         color=box_color)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'figS2_sensitivity_test_misclassified_firn.png')\n",
    "    fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9745e2cc",
   "metadata": {},
   "source": [
    "## Figure X (removed). Images used for classification performance assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d05a70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from shapely.geometry import MultiPolygon, Polygon\n",
    "\n",
    "data_path = '/Users/raineyaberle/Google Drive/My Drive/Research/PhD/snow_cover_mapping/classified-points/assessment/'\n",
    "\n",
    "# grab Sentinel-2_SR image file names\n",
    "os.chdir(data_path)\n",
    "im_fns = sorted(glob.glob('*Sentinel-2_SR*.tif'))\n",
    "# plot Lemon Creek images on top\n",
    "im_fns = im_fns[2:] + im_fns[0:2]\n",
    "\n",
    "# load glacier outlines from file\n",
    "study_sites_path = '/Users/raineyaberle/Google Drive/My Drive/Research/CryoGARS-Glaciology/Advising/student-research/Alexandra-Friel/snow_cover_mapping_application/study-sites/'\n",
    "AOI_fns = [study_sites_path + 'Emmons/AOIs/Emmons_RGI_outline.shp',\n",
    "           study_sites_path + 'LemonCreek/AOIs/LemonCreek_USGS_glacier_outline_2021.shp']\n",
    "AOIs = [gpd.read_file(AOI_fn) for AOI_fn in AOI_fns]\n",
    "\n",
    "# grab validation point names\n",
    "data_pts_fns = sorted(glob.glob('*.shp'))\n",
    "\n",
    "# set up figure\n",
    "fig, ax = plt.subplots(2, 2, figsize=(8,8))\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "ax = ax.flatten()\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)']\n",
    "# plot dummy points for legend\n",
    "ax[0].plot(0,0, '.', markersize=8, color=colors_classified[0], label='snow')\n",
    "ax[0].plot(0,0, '.', markersize=8, color=colors_classified[3], label='no snow')\n",
    "\n",
    "# loop through image files\n",
    "for i, im_fn in enumerate(im_fns):\n",
    "    \n",
    "    print(im_fn)\n",
    "    \n",
    "    # open image and plot\n",
    "    im = rxr.open_rasterio(im_fn)\n",
    "    # grab CRS\n",
    "    crs = im.rio.crs.to_epsg()\n",
    "    im = im / 1e4\n",
    "    ax[i].imshow(np.dstack([im.data[3], im.data[2], im.data[1]]),\n",
    "                extent=(np.min(im.x.data), np.max(im.x.data), np.min(im.y.data), np.max(im.y.data)))\n",
    "    \n",
    "    # load data points and plot\n",
    "    site_name = im_fn.split('_')[0]\n",
    "    im_date = im_fn[-12:-4]\n",
    "    data_pts_snow_fn = [x for x in data_pts_fns if (site_name in x) and (im_date[0:6] in x) and ('_snow' in x)]\n",
    "    if len(data_pts_snow_fn) > 0:\n",
    "        data_pts_snow = gpd.read_file(data_pts_snow_fn[0])\n",
    "        data_pts_snow = data_pts_snow.to_crs(im.rio.crs)\n",
    "        data_pts_snow.plot(ax=ax[i], color=colors_classified[0], markersize=1)\n",
    "    data_pts_no_snow_fn = [x for x in data_pts_fns if (site_name in x) and (im_date[0:6] in x) and ('no-snow' in x)]\n",
    "    if len(data_pts_no_snow_fn) > 0:\n",
    "        data_pts_no_snow = gpd.read_file(data_pts_no_snow_fn[0])\n",
    "        data_pts_no_snow = data_pts_no_snow.to_crs(im.rio.crs)\n",
    "        data_pts_no_snow.plot(ax=ax[i], color=colors_classified[3], markersize=1)\n",
    "        \n",
    "    # select AOI, reproject, and plot\n",
    "    if 'Emmons' in im_fn:\n",
    "        AOI = AOIs[0]\n",
    "    elif 'LemonCreek' in im_fn:\n",
    "        AOI = AOIs[1]\n",
    "    AOI = AOI.to_crs('EPSG:'+str(crs))\n",
    "    AOI_color = '#9e9ac8'\n",
    "    if type(AOI.geometry[0])==MultiPolygon:\n",
    "        for j, geom in enumerate(AOI.geometry[0].geoms):\n",
    "            if j==0:\n",
    "                ax[i].plot(*geom.exterior.coords.xy, '-', color=AOI_color, label='glacier boundary')\n",
    "            else:\n",
    "                ax[i].plot(*geom.exterior.coords.xy, '-', color=AOI_color, label='_nolegend')\n",
    "    else:\n",
    "        ax[i].plot(*AOI.geometry[0].exterior.coords.xy, '-', color=AOI_color, label='glacier outline')\n",
    "        \n",
    "    # set axis limits and ticks\n",
    "    if i>=2:\n",
    "        ax[i].set_xlim(593e3, 603e3)\n",
    "        ax[i].set_ylim(5188e3, 5196e3)\n",
    "        ax[i].set_xticks(np.arange(594e3, 603e3, step=2e3))\n",
    "        ax[i].set_yticks(np.arange(5188e3, 5197e3, step=2e3))\n",
    "    else:\n",
    "        ax[i].set_xlim(535e3, 541e3)\n",
    "        ax[i].set_ylim(6468e3, 6475e3)\n",
    "        ax[i].set_xticks(np.arange(536e3, 541e3, step=2e3))\n",
    "        ax[i].set_yticks(np.arange(6468e3, 6475e3, step=2e3))\n",
    "    # change labels from m to km\n",
    "    ax[i].set_xticklabels([str(int(x/1e3)) for x in ax[i].get_xticks()])\n",
    "    ax[i].set_yticklabels([str(int(x/1e3)) for x in ax[i].get_yticks()])\n",
    "        \n",
    "    # add text labels\n",
    "    ax[i].text((ax[i].get_xlim()[1] - ax[i].get_xlim()[0])*0.05 + ax[i].get_xlim()[0],\n",
    "               (ax[i].get_ylim()[1] - ax[i].get_ylim()[0])*0.9 + ax[i].get_ylim()[0],\n",
    "               text_labels[i], backgroundcolor='w')\n",
    "    \n",
    "\n",
    "# add axis labels\n",
    "ax[0].set_ylabel('Northing [km]')\n",
    "ax[2].set_ylabel('Northing [km]')\n",
    "ax[2].set_xlabel('Easting [km]')\n",
    "ax[3].set_xlabel('Easting [km]')\n",
    "\n",
    "# add legendwin\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=(0.27, 0.92), ncols=3)\n",
    "\n",
    "# fig.tight_layout()\n",
    "plt.show()\n",
    "    \n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'f04_classification_performance_assessment_images.png')\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300, bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c7d196",
   "metadata": {},
   "source": [
    "## Figure X (removed). SCA, snowlines, and AAR time series variability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4d18fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Settings and display parameters\n",
    "site_names = ['Wolverine', 'Gulkana', 'LemonCreek', 'SouthCascade', 'Sperry']\n",
    "\n",
    "fmt_month = matplotlib.dates.MonthLocator(bymonth=(5, 11)) # minor ticks every month\n",
    "fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "\n",
    "# -----Iterate over site names\n",
    "stats_df = pd.DataFrame()\n",
    "values_df = pd.DataFrame()\n",
    "for i, site_name in enumerate(site_names):\n",
    "    \n",
    "    print(site_name)\n",
    "\n",
    "    # Load estimated snow lines  \n",
    "    sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "    sl_ests = gpd.GeoDataFrame()\n",
    "    for sl_est_fn in sl_est_fns:\n",
    "        sl_est = pd.read_csv(sl_est_fn)\n",
    "        sl_ests = pd.concat([sl_ests, sl_est])\n",
    "    sl_ests.reset_index(drop=True, inplace=True)\n",
    "    sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "        \n",
    "    # Define axis limits\n",
    "    # xmin, xmax = np.datetime64('2016-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "    # sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "    # sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "    # ymin1, ymax1 = np.nanmin(sl_ests['SCA_m2']) * 1e-6 * -0.1, np.nanmax(sl_ests['SCA_m2']) * 1e-6 * 1.3\n",
    "    # ymin2 = sl_elev_median_min - 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "    # ymax2 = sl_elev_median_max + 0.1*(sl_elev_median_max - sl_elev_median_min)\n",
    "    # ymin3, ymax3 = -1, 125\n",
    "    # yrange1, yrange2, yrange3 = [ymin1, ymax1], [ymin2, ymax2], [ymin3, ymax3]\n",
    "\n",
    "    # Calculate monthly mean and std for Sentinel-2 time series\n",
    "    def custom_rolling_stats(data, data_var, window_size, months_to_include):\n",
    "        filtered_data = data[data['datetime'].dt.month.isin(months_to_include)]\n",
    "        rolling_std = filtered_data[data_var].rolling(window=window_size).apply(lambda x: np.std(x))\n",
    "        return rolling_std\n",
    "    \n",
    "    sl_ests.index = sl_ests.datetime\n",
    "    sl_ests.sort_index(inplace=True) # sort chronologically\n",
    "    \n",
    "    # define settings for rolling stats\n",
    "    time_range = pd.Timedelta(days=7) # window for rolling stats\n",
    "    months_to_include = [5,6,7,8,9,10] # which months to include\n",
    "    SCA_std = custom_rolling_stats(sl_ests, 'SCA_m2', int(time_range.days), months_to_include)\n",
    "    sl_std = custom_rolling_stats(sl_ests, 'snowline_elevs_median_m', int(time_range.days), months_to_include)\n",
    "    AAR_std = custom_rolling_stats(sl_ests, 'AAR', int(time_range.days), months_to_include)\n",
    "    \n",
    "    # append to dataframe\n",
    "    df = pd.DataFrame({'site_name': site_name,\n",
    "                       'SCA_std': SCA_std.values,\n",
    "                       'SCA_std_normalized': np.divide(SCA_std, np.nanmax(sl_ests['SCA_m2'])).values,\n",
    "                       'sl_std': sl_std.values,\n",
    "                       'sl_std_normalized': np.divide(sl_std, (np.nanmax(sl_ests['snowline_elevs_median_m']) - np.nanmin(sl_ests['snowline_elevs_median_m']))).values,\n",
    "                       'AAR_std': AAR_std.values,\n",
    "                       'AAR_std_normalized': np.divide(AAR_std, np.nanmax(sl_ests['AAR'])).values,\n",
    "                      })\n",
    "    stats_df = pd.concat([stats_df, df])\n",
    "    \n",
    "# -----Adjust dataframe\n",
    "# reset index, drop NaN values\n",
    "stats_df.reset_index(drop=True).dropna(inplace=True)\n",
    "# add display name to dataframe for plotting\n",
    "stats_df['display_name'] = [x.replace('C', ' C') for x in stats_df['site_name'].values]\n",
    "# adjust units for SCA (km^2) and AAR (%)\n",
    "stats_df[['SCA_std']] = np.divide(stats_df[['SCA_std']], 1e6)\n",
    "stats_df[['AAR_std']] = np.multiply(stats_df[['AAR_std']], 100)\n",
    "\n",
    "# -----Plot\n",
    "plt.rcParams.update({'font.size':14, 'font.sans-serif': 'Arial'})\n",
    "fig, ax = plt.subplots(3, 2, figsize=(16, 16))\n",
    "ax = ax.flatten()  \n",
    "# define axes settings\n",
    "text_labels = ['(a)', '(b)', '(c)', '(d)', '(e)', '(f)']\n",
    "ylabels = ['SCA [km$^2$]', 'SCA [unitless]',\n",
    "           'AAR [%]', 'AAR [unitless]',\n",
    "           'Median snowline altitude [m]', 'Median snowline altitude [unitless]'\n",
    "           ]\n",
    "data_vars = ['SCA_std', 'SCA_std_normalized', 'AAR_std', 'AAR_std_normalized', 'sl_std', 'sl_std_normalized']\n",
    "colors = [colors_classified[0], colors_classified[0], \n",
    "          '#FFC107', '#FFC107',\n",
    "          '#FB65FB', '#FB65FB']\n",
    "ax[0].set_title('Weekly standard deviation')\n",
    "ax[1].set_title('Normalized weekly standard deviation')\n",
    "ax[1].set_ylim(-0.01, 0.5)\n",
    "ax[2].set_ylim(-1, 50)\n",
    "ax[3].set_ylim(-0.01, 0.5)\n",
    "ax[5].set_ylim(-0.01, 0.5)\n",
    "# iterate over axes\n",
    "for axis, data_var, color, text_label, ylabel in list(zip(ax, data_vars, colors, text_labels, ylabels)):\n",
    "    sns.boxplot(data=stats_df, x='display_name', y=data_var, ax=axis, color=color) \n",
    "    axis.set(xlabel=None)\n",
    "    axis.set_ylabel(ylabel)\n",
    "    axis.text((axis.get_xlim()[1] - axis.get_xlim()[0])*0.935 + axis.get_xlim()[0], \n",
    "               (axis.get_ylim()[1] - axis.get_ylim()[0])*0.903 + axis.get_ylim()[0], \n",
    "               text_label, bbox=dict(facecolor='white', edgecolor='black', pad=5))\n",
    "    axis.set_xticks(axis.get_xticks(), axis.get_xticklabels(), rotation=20)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig_fn = os.path.join(figures_out_path, 'weekly_metric_ranges.png')\n",
    "    fig.savefig(fig_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1ae89a",
   "metadata": {},
   "source": [
    "## Example SCA time series at South Cascade Glacier for GitHub repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e43c8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_name = 'SouthCascade'\n",
    "\n",
    "# -----Load estimated snowlines  \n",
    "sl_ests_full = pd.DataFrame()    \n",
    "sl_est_fns = glob.glob(study_sites_path + site_name + '/imagery/snowlines/*snowline.csv')\n",
    "sl_ests = pd.DataFrame()\n",
    "for sl_est_fn in sl_est_fns:\n",
    "    sl_est = pd.read_csv(sl_est_fn)\n",
    "    sl_ests = pd.concat([sl_ests, sl_est])\n",
    "sl_ests.reset_index(drop=True, inplace=True)\n",
    "sl_ests['datetime'] = pd.to_datetime(sl_ests['datetime'], format='mixed')\n",
    "# reset index and add week-of-year column\n",
    "sl_ests.reset_index(drop=True, inplace=True)\n",
    "sl_ests['Week'] = sl_ests['datetime'].dt.isocalendar().week\n",
    "# convert SCA from m2 to km2\n",
    "sl_ests['SCA_km2'] = np.divide(sl_ests['SCA_m2'].values, 1e6)\n",
    "\n",
    "# -----Set up figure\n",
    "fontsize=12\n",
    "plt.rcParams.update({'font.size':fontsize, 'font.sans-serif':'Arial'})\n",
    "# time series: SCA\n",
    "fig1, ax1 = plt.subplots(3, 2, figsize=(10, 10), gridspec_kw=dict(width_ratios=[4,1]))\n",
    "# fmt_year = matplotlib.dates.YearLocator() # minor ticks every year\n",
    "fmt_year = matplotlib.dates.DateFormatter(\"%Y\")\n",
    "alpha = 0.9\n",
    "\n",
    "# -----Define axis limits\n",
    "xmin, xmax = np.datetime64('2013-05-01T00:00:00'), np.datetime64('2022-12-01T00:00:00')\n",
    "sl_elev_median_min = np.nanmin(sl_ests['snowline_elevs_median_m'])\n",
    "sl_elev_median_max = np.nanmax(sl_ests['snowline_elevs_median_m'])\n",
    "yrange1 = [np.nanmax(sl_ests['SCA_km2']) * -0.1, np.nanmax(sl_ests['SCA_km2']) * 1.1]\n",
    "yrange2 = [-0.1, 1.1]\n",
    "yrange3 = [np.nanmin(sl_ests['snowline_elevs_median_m']) * 0.98, np.nanmax(sl_ests['snowline_elevs_median_m']) * 1.02]\n",
    "        \n",
    "# -----Plot time series of SCA, AAR, and median snowline elevations\n",
    "for column, ylabel, yrange, i in list(zip(['SCA_km2', 'AAR', 'snowline_elevs_median_m'],\n",
    "                                          ['SCA [km$^2$]', 'AAR', 'Median snowline elevation [m]'],\n",
    "                                          [yrange1, yrange2, yrange3],\n",
    "                                          np.arange(0,3))):\n",
    "    # PlanetScope\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='PlanetScope'], \n",
    "                   sl_ests[column].loc[sl_ests['dataset']=='PlanetScope'].values, \n",
    "                   '.', markeredgecolor='w', markerfacecolor=color_PlanetScope, \n",
    "                alpha=alpha, markersize=10, markeredgewidth=1, label='PlanetScope')\n",
    "    # Sentinel-2 SR\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_SR'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Sentinel-2_SR'].values, \n",
    "                  'D', markeredgecolor='w', markerfacecolor=color_Sentinel2, \n",
    "                  alpha=alpha, markersize=4, markeredgewidth=1, label='Sentinel-2 SR')\n",
    "    # Sentinel-2 TOA\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Sentinel-2_TOA'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Sentinel-2_TOA'].values, \n",
    "                  'D', markeredgecolor=color_Sentinel2, markerfacecolor='None', \n",
    "                  alpha=alpha, markersize=3, markeredgewidth=1.2, label='Sentinel-2 TOA')  \n",
    "    # Landsat\n",
    "    ax1[i,0].plot(sl_ests['datetime'].loc[sl_ests['dataset']=='Landsat'], \n",
    "                  sl_ests[column].loc[sl_ests['dataset']=='Landsat'].values, \n",
    "                  '^', markeredgecolor='w', markerfacecolor=color_Landsat, \n",
    "                  alpha=alpha, markersize=6, markeredgewidth=1, label='Landsat')   \n",
    "    # adjust axis\n",
    "    ax1[i,0].set_ylabel(ylabel)\n",
    "    ax1[i,0].set_xlim(xmin, xmax)\n",
    "    ax1[i,0].set_ylim(yrange[0], yrange[1])\n",
    "    ax1[i,0].grid(True)\n",
    "\n",
    "    # -----Plot light grey boxes where no observations exist \n",
    "    years = np.arange(2013, 2022, step=1)\n",
    "    for year in years:\n",
    "        min_date, max_date = np.datetime64(str(year) + '-11-01'), np.datetime64(str(year+1) + '-05-01')\n",
    "        rect = matplotlib.patches.Rectangle((min_date, yrange[0]), width=max_date-min_date, height=yrange[1]-yrange[0], color='#d9d9d9')\n",
    "        ax1[i,0].add_patch(rect)\n",
    "    \n",
    "    # -----Calculate median and interquartile ranges for weekly trends\n",
    "    q1, q3 = 0.25, 0.75 # define quartiles\n",
    "    # calculate weekly trends using only Sentinel-2 snowlines\n",
    "    sl_ests_noPS = sl_ests.loc[sl_ests['dataset']!='PlanetScope']   \n",
    "    weekly = sl_ests_noPS.groupby(by='Week')[column].agg(['median', lambda x: x.quantile(q1), lambda x: x.quantile(q3)])\n",
    "    weekly.columns = ['Median', 'Q1', 'Q3'] # Rename the columns for clarity\n",
    "    weekly.index = weekly.index.astype(float)\n",
    "    # plot\n",
    "    ax1[i,1].fill_between(weekly.index, weekly['Q1'], weekly['Q3'].values, color='k', alpha=0.5)\n",
    "    ax1[i,1].plot(weekly.index, weekly['Median'], color='k', linewidth=2)\n",
    "    ax1[i,1].grid(True)\n",
    "    # adjust axis\n",
    "    ax1[i,1].set_xlim(15, 45)\n",
    "    ax1[i,1].set_xticks([18, 31, 44])\n",
    "    ax1[i,1].set_xticklabels([])\n",
    "    ax1[i,1].set_xticklabels(['May', 'Aug', 'Nov'])\n",
    "    ax1[i,1].set_ylim(yrange[0], yrange[1])\n",
    "\n",
    "ax1[0,1].set_title('Weekly median trend')\n",
    "\n",
    "# -----Plot glacier area on SCA plots\n",
    "# AOI_fn = glob.glob(os.path.join(study_sites_path, site_name, 'AOIs', site_name + '*USGS*.shp'))[0]\n",
    "# AOI = gpd.read_file(AOI_fn)\n",
    "# ax1[0,0].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "# ax1[0,1].plot([xmin, xmax], [AOI.geometry[0].area / 1e6, AOI.geometry[0].area / 1e6], '--', color='grey')\n",
    "        \n",
    "# -----Add legend to axis 1\n",
    "ax1[0,0].legend(loc='center', bbox_to_anchor=(0.5, 1.1), handletextpad=0.1, labelspacing=0.5, markerscale=2, ncol=4)\n",
    "fig1.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# -----Save figures\n",
    "if save_figures:\n",
    "    fig1_fn = os.path.join(figures_out_path, 'timeseries_SouthCascade_Glacier.png')\n",
    "    fig1.savefig(fig1_fn, dpi=300, facecolor='w', edgecolor='none', bbox_inches='tight')\n",
    "    print('figure 1 saved to file: ' + fig1_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5822293b",
   "metadata": {},
   "source": [
    "## Snow cover products comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3a5d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----Load Landsat fSCA\n",
    "# LS_fn = base_path+'../study-sites/Wolverine/imagery/Landsat/fSCA/LC08_AK_016008_20210829_20210913_02_SNOW/LC08_AK_016008_20210829_20210913_02_VIEWABLE_SNOW_UTM.TIF'\n",
    "# LS = rxr.open_rasterio(LS_fn)\n",
    "# # remove no-data values\n",
    "# LS = LS.where(LS != -9999)\n",
    "# # account for image multiplier\n",
    "# LS_scalar = 0.001\n",
    "# LS = LS * LS_scalar\n",
    "# crs = LS.rio.crs.to_string()\n",
    "\n",
    "# # -----Load MODIS fSCA\n",
    "# M_fn = base_path+'../study-sites/Wolverine/imagery/MODIS/Terra_fSCA/2021_08_15.tif'\n",
    "# M = rxr.open_rasterio(M_fn)\n",
    "# # grab snow cover band\n",
    "# M_fSCA = M.isel(band=0)\n",
    "# # remove no data values\n",
    "# M_fSCA = M_fSCA.where(M_fSCA != -3.2768e04)\n",
    "# # reproject \n",
    "# M_fSCA= M_fSCA.rio.reproject(crs)\n",
    "\n",
    "# # -----Load PlanetScope image and snow\n",
    "# # RGB image\n",
    "# PS_path = base_path+'../study-sites/Wolverine/imagery/PlanetScope/adjusted-filtered/'\n",
    "# PS_fn = '20210815_20_adj.tif'\n",
    "# PS = rxr.open_rasterio(PS_path + PS_fn)\n",
    "# PS = PS / 1e4\n",
    "# # classify image\n",
    "# clf_fn = base_path+'/inputs-outputs/PS_classifier_all_sites.sav'\n",
    "# clf = pickle.load(open(clf_fn, 'rb'))\n",
    "# feature_cols_fn = base_path+'inputs-outputs/PS_feature_cols.pkl'\n",
    "# feature_cols = pickle.load(open(feature_cols_fn,'rb'))\n",
    "# sys.path.insert(1, base_path+'functions/')\n",
    "# from ps_pipeline_utils import classify_image\n",
    "# im_classified_fn, im = classify_image(PS_fn, PS_path, clf, feature_cols, False, None, out_path)\n",
    "# # load classified image\n",
    "# im_classified = rxr.open_rasterio(out_path + im_classified_fn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afd06fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # -----Create snow colormap\n",
    "# color_snow = '#4eb3d3'\n",
    "# color_no_snow = 'w'\n",
    "# # create colormap\n",
    "# colors = [color_no_snow, color_snow]\n",
    "# cmp = cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n",
    "\n",
    "# # -----Plot\n",
    "# fig, ax = plt.subplots(2, 2, figsize=(10,10))\n",
    "# ax = ax.flatten()\n",
    "# plt.rcParams.update({'font.size':16, 'font.sans-serif':'Arial'})\n",
    "# xmin, xmax, ymin, ymax = 391, 399, 6694, 6702\n",
    "# # MODIS\n",
    "# M_im = ax[0].imshow(M_fSCA.data, cmap=cmp, clim=(0,100),\n",
    "#                     extent=(np.min(M_fSCA.x.data)/1000, np.max(M_fSCA.x.data)/1000, \n",
    "#                             np.min(M_fSCA.y.data)/1000, np.max(M_fSCA.y.data)/1000))\n",
    "# ax[0].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[0].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[0].set_xticklabels([])\n",
    "# ax[0].set_xlim(xmin, xmax)\n",
    "# ax[0].set_ylim(ymin, ymax)\n",
    "# ax[0].set_ylabel('Northing [km]')\n",
    "# ax[0].set_title('a) MODIS f$_{SCA}$')\n",
    "# # LS\n",
    "# LS_im = ax[1].imshow(LS_fSCA, cmap=cmp, clim=(0,1),\n",
    "#                    extent=(np.min(LS_x)/1000, np.max(LS_x)/1000, np.min(LS_y)/1000, np.max(LS_y)/1000))\n",
    "# ax[1].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[1].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[1].set_xticklabels([])\n",
    "# ax[1].set_yticklabels([])\n",
    "# ax[1].set_xlim(xmin, xmax)\n",
    "# ax[1].set_ylim(ymin, ymax)\n",
    "# ax[1].set_title('b) Landsat 8 f$_{SCA}$')\n",
    "# # PS RGB\n",
    "# ax[2].imshow(np.dstack([PS.data[2], PS.data[1], PS.data[0]]),\n",
    "#            extent=(np.min(PS.x.data)/1000, np.max(PS.x.data)/1000, np.min(PS.y.data)/1000, np.max(PS.y.data)/1000))\n",
    "# ax[2].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[2].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[2].set_xlim(xmin, xmax)\n",
    "# ax[2].set_ylim(ymin, ymax)\n",
    "# ax[2].set_ylabel('Northing [km]')\n",
    "# ax[2].set_xlabel('Easting [km]')\n",
    "# ax[2].set_title('c) PlanetScope RGB')\n",
    "# # PS snow\n",
    "# im_classified = im_classified.where(im_classified!=-9999)\n",
    "# im_binary = xr.where(im_classified<=2, 1, 0)\n",
    "# PS_snow_im = ax[3].imshow(im_binary.data[0], cmap=cmp, clim=(0,1),\n",
    "#                    extent=(np.min(PS.x.data)/1000, np.max(PS.x.data)/1000, np.min(PS.y.data)/1000, np.max(PS.y.data)/1000))\n",
    "# ax[3].set_xticks(np.linspace(392, 398, num=4))\n",
    "# ax[3].set_yticks(np.linspace(6694, 6702, num=5))\n",
    "# ax[3].set_yticklabels([])\n",
    "# ax[3].set_xlim(xmin, xmax)\n",
    "# ax[3].set_ylim(ymin, ymax)\n",
    "# ax[3].set_xlabel('Easting [km]')\n",
    "# ax[3].set_title('d) PlanetScope SCA')\n",
    "# # colorbar\n",
    "# cbar_ax = fig.add_axes([0.92, 0.35, 0.02, 0.3])\n",
    "# fig.colorbar(M_im, cax=cbar_ax)\n",
    "# plt.show()\n",
    "\n",
    "# if save_figures:\n",
    "#     fig.savefig(out_path+'comparing_SCA_products.png', dpi=300, facecolor='white', edgecolor='none')\n",
    "#     print('figure saved to file')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
