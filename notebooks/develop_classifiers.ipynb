{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4661c93",
   "metadata": {},
   "source": [
    "# Notebook to develop supervised classification algorithm for identifying snow in PlanetScope 4-band, Landsat 8, Sentinel-2, and MODIS imagery\n",
    "Rainey Aberle\n",
    "\n",
    "Adapted from the [SciKit Learn Classifier comparison tutorial](https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "## Outline:\n",
    "1. Set up training data\n",
    "- PlanetScope\n",
    "- Landsat 8/9\n",
    "- Sentinel-2 SR\n",
    "- Sentinel-2 TOA\n",
    "    \n",
    "2. Develop supervised classifiers for EACH site and ALL sites\n",
    "\n",
    "3. *Optional*\n",
    "- Test how the number of training points and samples impact model accuracies (calculate learning curves)\n",
    "- MODIS (not recommended)\n",
    "- Sentinel-1 (not recommended)\n",
    "\n",
    "## 0. Initial Setup\n",
    "\n",
    "Import packages, define paths in directory, authenticate Google Earth Engine (GEE), define classification settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088eeecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Import packages\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import rasterio as rio\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import ee\n",
    "import scipy\n",
    "import wxee as wx\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "from shapely.geometry import Polygon\n",
    "from sklearn.tree import DecisionTreeClassifier \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics \n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import sys\n",
    "import pickle\n",
    "import seaborn as sns\n",
    "from shapely.geometry import Point, MultiPoint\n",
    "from joblib import dump, load\n",
    "import json\n",
    "import datetime\n",
    "from shapely import wkt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e7a49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----Determine whether to save outputs to file\n",
    "save_outputs = False # = True to save output figures and best classifier \n",
    "\n",
    "# -----Define paths in directory\n",
    "# base directory (path to glacier-snow-cover-mapping/)\n",
    "base_path = '/Users/raineyaberle/Research/glacier_snow_cover_mapping/glacier-snow-cover-mapping/'\n",
    "# output folder for best classifier\n",
    "out_path = os.path.join(base_path, 'inputs-outputs')\n",
    "# output folder for figures\n",
    "figures_out_path = os.path.join(base_path, 'figures')\n",
    "# path to classified points used to train and test classifiers\n",
    "data_pts_path = '/Users/raineyaberle/Research/glacier_snow_cover_mapping/classified-points/'\n",
    "\n",
    "# -----Determine settings\n",
    "terrain_parameters = False # whether to use terrain parameters (elevation, slope, aspect) in classification\n",
    "save_figures = True # whether to save output figures\n",
    "\n",
    "# -----Add path to functions\n",
    "sys.path.insert(1, os.path.join(base_path, 'functions'))\n",
    "import pipeline_utils as f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1816e3d5",
   "metadata": {},
   "source": [
    "### Create dictionary of dataset-specific parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f9b92d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset_dict = {\n",
    "    'PlanetScope':{\n",
    "        'image_scalar': 1e4,\n",
    "        'refl_bands': {\n",
    "            'Blue': '0',\n",
    "            'Green': '1',\n",
    "            'Red': '2',\n",
    "            'NIR': '3'\n",
    "        },\n",
    "        'no_data_value': -9999,\n",
    "        'resolution_m': 3,\n",
    "        'NDSI_bands': ['Green', 'NIR'],\n",
    "        'RGB_bands': ['Red', 'Green', 'Blue']\n",
    "    },\n",
    "    \n",
    "    'Landsat': {\n",
    "        'image_scalar': 1/2.75e-05,\n",
    "        'refl_bands': {\n",
    "            'SR_B1': 'Ultra blue',\n",
    "            'SR_B2': 'Blue',\n",
    "            'SR_B3': 'Green',\n",
    "            'SR_B4': 'Red',\n",
    "            'SR_B5': 'NIR',\n",
    "            'SR_B6': 'SWIR1',\n",
    "            'SR_B7': 'SWIR2',\n",
    "            'ST_B10': 'Bit mask'\n",
    "        },\n",
    "        'no_data_value': 0,\n",
    "        'resolution_m': 30,\n",
    "        'NDSI_bands': ['SR_B3', 'SR_B6'],\n",
    "        'RGB_bands': ['SR_B4', 'SR_B3', 'SR_B2']\n",
    "    },\n",
    "    \n",
    "    'Sentinel-2_SR':{\n",
    "        'image_scalar': 1e4,\n",
    "        'refl_bands': {\n",
    "             'B1': 'Aerosols',\n",
    "             'B2': 'Blue',\n",
    "             'B3': 'Green',\n",
    "             'B4': 'Red',\n",
    "             'B5': 'Red Edge 1',\n",
    "             'B6': 'Red Edge 2',\n",
    "             'B7': 'Red Edge 3',\n",
    "             'B8': 'NIR',\n",
    "             'B8A': 'Red Edge 4',\n",
    "             'B9': 'Water vapor',\n",
    "             'B11': 'SWIR1',\n",
    "             'B12': 'SWIR2'\n",
    "        },\n",
    "        'no_data_value': 0,\n",
    "        'resolution_m': 10,\n",
    "        'NDSI_bands': ['B3', 'B11'],\n",
    "        'RGB_bands': ['B4', 'B3', 'B2']\n",
    "    },\n",
    "    \n",
    "    'Sentinel-2_TOA':{\n",
    "        'image_scalar': 1e4,\n",
    "        'refl_bands': {\n",
    "             'B1': 'Aerosols',\n",
    "             'B2': 'Blue',\n",
    "             'B3': 'Green',\n",
    "             'B4': 'Red',\n",
    "             'B5': 'Red Edge 1',\n",
    "             'B6': 'Red Edge 2',\n",
    "             'B7': 'Red Edge 3',\n",
    "             'B8': 'NIR',\n",
    "             'B8A': 'Red Edge 4',\n",
    "             'B9': 'Water vapor',\n",
    "             'B11': 'SWIR1',\n",
    "             'B12': 'SWIR2'\n",
    "        },\n",
    "        'no_data_value': 0,\n",
    "        'resolution_m': 10,\n",
    "        'NDSI_bands': ['B3', 'B11'],\n",
    "        'RGB_bands': ['B4', 'B3', 'B2']\n",
    "    },\n",
    "    \n",
    "    'classified_image': {\n",
    "        'image_scalar': 0,\n",
    "        'bands':{\n",
    "            'classified': 'classified image',\n",
    "            'elevation': 'interpolated elevation'\n",
    "        },\n",
    "        'no_data_value': -9999,\n",
    "        'classification_values':{\n",
    "            '1': 'Snow',\n",
    "            '2': 'Shadowed snow',\n",
    "            '3': 'Ice',\n",
    "            '4': 'Rock',\n",
    "            '5': 'Water'\n",
    "        },\n",
    "        'class_colors':{\n",
    "            'Snow': '#4eb3d3',\n",
    "            'Shadowed_snow': '#636363',\n",
    "            'Ice': '#084081',\n",
    "            'Rock': '#fe9929',\n",
    "            'Water': '#252525'\n",
    "        }\n",
    "    }\n",
    "        \n",
    "}\n",
    "\n",
    "# save dictionary as json file\n",
    "# dataset_dict_fn = 'datasets_characteristics.json'\n",
    "# json.dump(dataset_dict, open(out_path + dataset_dict_fn, 'w'))\n",
    "# print('dictionary saved to file: ' + out_path + dataset_dict_fn)\n",
    "# # open dictionary and display\n",
    "# dataset_dict = json.load(open(out_path + dataset_dict_fn))\n",
    "# dataset_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece21f2a",
   "metadata": {},
   "source": [
    "## 1. Construct or load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9202e64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define datasets\n",
    "datasets = ['PlanetScope', 'Landsat', 'Sentinel-2_SR', 'Sentinel-2_TOA']\n",
    "\n",
    "# -----Define site names\n",
    "site_names = ['Gulkana', 'SouthCascade', 'Sperry', 'Wolverine']\n",
    "\n",
    "# -----Grab colormap for classified image\n",
    "colors = list(dataset_dict['classified_image']['class_colors'].values())\n",
    "    \n",
    "# -----Loop through datasets\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print('----------')\n",
    "    print(dataset)\n",
    "    print('----------')\n",
    "\n",
    "    # Check if training data exist in file\n",
    "    training_data_fn = dataset + '_training_data.csv'\n",
    "    feature_cols_fn = dataset + '_feature_columns.json'\n",
    "    if (os.path.exists(os.path.join(out_path, training_data_fn))) and (os.path.exists(os.path.join(out_path, feature_cols_fn))):\n",
    "\n",
    "        data_pts_full = pd.read_csv(os.path.join(out_path, training_data_fn))\n",
    "        feature_cols = json.load(open(os.path.join(out_path, feature_cols_fn)))\n",
    "        print('Training data and feature columns already exist... loaded from file.')\n",
    "\n",
    "    else: \n",
    "        \n",
    "        # Define band names feature columns used to classify\n",
    "        band_names = list(dataset_dict[dataset]['refl_bands'].keys())\n",
    "        if dataset=='PlanetScope':\n",
    "            feature_cols = band_names + ['NDSI']\n",
    "        else:\n",
    "            feature_cols = band_names[0:-1] + ['NDSI']\n",
    "\n",
    "        # Load data points file names\n",
    "        if 'Sentinel-2' in dataset:\n",
    "            data_pts_path_dataset = os.path.join(data_pts_path, 'Sentinel-2')\n",
    "        else:\n",
    "            data_pts_path_dataset = data_pts_path + dataset + '/'\n",
    "        os.chdir(data_pts_path_dataset)\n",
    "        data_pts_fns = sorted(glob.glob('*.shp'))\n",
    "\n",
    "        # Load image file names\n",
    "        im_fns = sorted(glob.glob('*.tif'))\n",
    "        if dataset=='Sentinel-2_SR':\n",
    "            im_fns = [im_fn for im_fn in im_fns if 'SR' in im_fn]\n",
    "        elif dataset=='Sentinel-2_TOA':\n",
    "            im_fns = [im_fn for im_fn in im_fns if 'TOA' in im_fn]\n",
    "        \n",
    "        # Initialize full data points dataframe (for use in next step)\n",
    "        data_pts_full = gpd.GeoDataFrame()\n",
    "\n",
    "        # Loop through sites\n",
    "        for i, site_name in enumerate(site_names):\n",
    "            \n",
    "            # Grab image file names, dates, and data point file names\n",
    "            im_fns_site = [im_fn for im_fn in im_fns if site_name in im_fn]\n",
    "            print('image file names: ', im_fns_site)\n",
    "            if dataset=='Landsat':\n",
    "                im_dates = im_dates = [x[-12:-4] for x in im_fns_site]\n",
    "            else:\n",
    "                im_dates = [x.split(site_name+'_')[1][0:8] for x in im_fns_site]\n",
    "            data_pts_site_fns = [data_pts_fn for data_pts_fn in data_pts_fns if site_name in data_pts_fn]\n",
    "\n",
    "            # loop through image dates\n",
    "            for j, im_date in enumerate(im_dates):\n",
    "\n",
    "                # compile classified points\n",
    "                data_pts = pd.DataFrame() # dataframe to hold applicable data classes\n",
    "                # snow\n",
    "                if len([s for s in data_pts_site_fns if ('snow.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "                    data_pts_snow_fn = [s for s in data_pts_fns if ('snow.shp' in s) and (im_date in s)][0]\n",
    "                    data_pts_snow = gpd.read_file(data_pts_path_dataset + data_pts_snow_fn) # read file\n",
    "                    data_pts_snow['Class'] = 1 # determine class ID\n",
    "                    data_pts = pd.concat([data_pts, data_pts_snow], ignore_index=True) # concatenate to full data points df\n",
    "                    print(data_pts_snow_fn)\n",
    "                # shadowed snow\n",
    "                if len([s for s in data_pts_site_fns if ('snow-shadowed.shp' in s)  and (im_date in s)])>0: # check if class exists for site and date\n",
    "                    data_pts_snow_sh_fn = [s for s in data_pts_site_fns if ('snow-shadowed.shp' in s) and (im_date in s)][0]\n",
    "                    data_pts_snow_sh = gpd.read_file(data_pts_path_dataset + data_pts_snow_sh_fn) # read file\n",
    "                    data_pts_snow_sh['Class'] = 2 # determine class ID\n",
    "                    data_pts = pd.concat([data_pts, data_pts_snow_sh], ignore_index=True) # concatenate to full data points df\n",
    "                    print(data_pts_snow_sh_fn)\n",
    "                # ice\n",
    "                if len([s for s in data_pts_site_fns if ('ice.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "                    data_pts_ice_fn = [s for s in data_pts_site_fns if ('ice.shp' in s)  and (im_date in s)][0]\n",
    "                    data_pts_ice = gpd.read_file(data_pts_path_dataset + data_pts_ice_fn)  # read file\n",
    "                    data_pts_ice['Class'] = 3 # determine class ID\n",
    "                    data_pts = pd.concat([data_pts, data_pts_ice], ignore_index=True) # concatenate to full data points df\n",
    "                    print(data_pts_ice_fn)\n",
    "                # rock\n",
    "                if len([s for s in data_pts_site_fns if ('rock.shp' in s) and (im_date in s)])>0: # check if class exists for site and date\n",
    "                    data_pts_rock_fn = [s for s in data_pts_site_fns if ('rock.shp' in s)  and (im_date in s)][0]\n",
    "                    data_pts_rock = gpd.read_file(data_pts_path_dataset + data_pts_rock_fn) # read file\n",
    "                    data_pts_rock['Class'] = 4 # determine class ID\n",
    "                    data_pts = pd.concat([data_pts, data_pts_rock], ignore_index=True) # concatenate to full data points df\n",
    "                    print(data_pts_rock_fn)\n",
    "                # water\n",
    "                if len([s for s in data_pts_site_fns if ('water.shp' in s)  and (im_date in s)])>0: # check if class exists for site and date\n",
    "                    data_pts_water_fn = [s for s in data_pts_site_fns if ('water.shp' in s) and (im_date in s)][0]\n",
    "                    data_pts_water = gpd.read_file(data_pts_path_dataset + data_pts_water_fn) # read file\n",
    "                    data_pts_water['Class'] = 5 # determine class ID\n",
    "                    data_pts = pd.concat([data_pts, data_pts_water], ignore_index=True) # concatenate to full data points df\n",
    "                    print(data_pts_water_fn)\n",
    "                # remove 'id' column\n",
    "                data_pts.drop('id', axis=1, inplace=True)\n",
    "                # remove rows with empty geometries\n",
    "                data_pts.dropna(inplace=True)\n",
    "                # reformat MultiPoint objects as Point objects\n",
    "                data_pts['geometry'] = [x.geoms[0] for x in data_pts['geometry']]\n",
    "\n",
    "                # Load AOI\n",
    "                AOI_path = base_path + '../study-sites/' + site_name + '/AOIs/' + site_name + '_USGS_*.shp'\n",
    "                AOI_fn = glob.glob(AOI_path)[0]\n",
    "                AOI = gpd.read_file(AOI_fn)\n",
    "                # reproject AOI to WGS 84 for compatibility with images\n",
    "                AOI_WGS = AOI.to_crs('EPSG:4326')\n",
    "                # Determine optimal UTM zone EPSG code\n",
    "                epsg_UTM = f.convert_wgs_to_utm((AOI_WGS.geometry.bounds.maxx[0] - AOI_WGS.geometry.bounds.minx[0]) + AOI_WGS.geometry.bounds.minx[0],\n",
    "                                                (AOI_WGS.geometry.bounds.maxy[0] - AOI_WGS.geometry.bounds.miny[0]) + AOI_WGS.geometry.bounds.miny[0])\n",
    "                AOI_UTM = AOI.to_crs('EPSG:'+str(epsg_UTM))\n",
    "                \n",
    "                # Load image\n",
    "                im_fn_date = [im_fn for im_fn in im_fns if im_date in im_fn][0]\n",
    "                # Adjust image radiometry for PlanetScope images\n",
    "                if dataset=='PlanetScope':\n",
    "                    # load image\n",
    "                    im_ds = xr.open_dataset(data_pts_path_dataset + im_fn_date)\n",
    "                    # load DEM\n",
    "                    DEM_path = base_path + '../study-sites/' + site_name + '/DEMs/'\n",
    "                    DEM_fn = glob.glob(DEM_path+site_name+'*_DEM*.tif')[0]\n",
    "                    DEM = xr.open_dataset(DEM_fn)\n",
    "                    DEM = DEM.rename({'band_data':'elevation'})\n",
    "                    # remove unnecessary data (possible extra bands from ArcticDEM or other DEM)\n",
    "                    if len(np.shape(DEM.elevation.data))>2:\n",
    "                        DEM['elevation'] = DEM.elevation[0]\n",
    "                    DEM.rio.write_crs('EPSG:'+str(rxr.open_rasterio(DEM_fn).rio.crs.to_epsg()), inplace=True)\n",
    "                    DEM = DEM.rio.reproject('EPSG:'+epsg_UTM)\n",
    "                    DEM.rio.write_crs('EPSG:'+epsg_UTM, inplace=True)\n",
    "                    # create polygons of top and bottom 20th percentile elevations\n",
    "                    polygon_top, polygon_bottom = f.create_aoi_elev_polys(AOI_UTM, DEM)\n",
    "                    # adjust image radiometry\n",
    "                    im_ds = f.planetscope_adjust_image_radiometry(im_ds, np.datetime64(im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8]),\n",
    "                                                                  polygon_top, polygon_bottom, dataset_dict, skip_clipped=False)[0]\n",
    "                else:\n",
    "                    # read in xarray.DataArray\n",
    "                    im_da = rxr.open_rasterio(data_pts_path_dataset + im_fn_date)\n",
    "                    # reproject to optimal UTM zone (if necessary)\n",
    "                    im_da = im_da.rio.reproject('EPSG:'+str(epsg_UTM))\n",
    "                    # convert to xarray.DataSet\n",
    "                    im_ds = im_da.to_dataset('band')\n",
    "                    # account for image scalar and no data values\n",
    "                    im_ds = xr.where(im_ds != dataset_dict[dataset]['no_data_value'],\n",
    "                                     im_ds / dataset_dict[dataset]['image_scalar'], np.nan)\n",
    "                    # rename bands\n",
    "                    im_ds = im_ds.rename({i + 1: name for i, name in enumerate(band_names)})\n",
    "                    # set CRS\n",
    "                    im_ds.rio.write_crs('EPSG:'+str(im_da.rio.crs.to_epsg()), inplace=True)\n",
    "\n",
    "                # reproject data points to optimal UTM zone\n",
    "                data_pts = data_pts.to_crs('EPSG:'+epsg_UTM)\n",
    "\n",
    "                # sample band values at data points\n",
    "                band_names = [band_name for band_name in band_names if 'QA' not in band_name] # remove QA bands from band names\n",
    "                for band_name in band_names:\n",
    "                    data_pts[band_name] = [im_ds.sel(x=x.x, y=x.y, method=\"nearest\")[band_name].data \n",
    "                                           for x in data_pts['geometry'].values]\n",
    "                \n",
    "                # plot images and data points\n",
    "                fig1, ax1 = f.plot_xr_rgb_image(im_ds, dataset_dict[dataset]['RGB_bands'])\n",
    "                ax1.scatter([x.x/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==1].values], \n",
    "                            [x.y/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==1].values], c=colors[0], s=1, label='Snow')\n",
    "                ax1.scatter([x.x/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==2].values], \n",
    "                            [x.y/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==2].values], c=colors[1], s=1, label='Shadowed snow')\n",
    "                ax1.scatter([x.x/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==3].values], \n",
    "                            [x.y/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==3].values], c=colors[2], s=1, label='Ice')\n",
    "                ax1.scatter([x.x/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==4].values], \n",
    "                            [x.y/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==4].values], c=colors[3], s=1, label='Rock')\n",
    "                ax1.scatter([x.x/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==5].values], \n",
    "                            [x.y/1e3 for x in data_pts['geometry'].loc[data_pts['Class']==5].values], c=colors[4], s=1, label='Water')\n",
    "                ax1.set_xlabel('Easting [km]')\n",
    "                ax1.set_ylabel('Northing [km]')\n",
    "                ax1.legend(loc='best')\n",
    "                ax1.set_title(im_date)\n",
    "                plt.show() \n",
    "\n",
    "                # add data points to full data points data frame\n",
    "                data_pts['NDSI'] = ((data_pts[dataset_dict[dataset]['NDSI_bands'][0]] - data_pts[dataset_dict[dataset]['NDSI_bands'][1]]) \n",
    "                                    / (data_pts[dataset_dict[dataset]['NDSI_bands'][0]] + data_pts[dataset_dict[dataset]['NDSI_bands'][1]])) # add NDSI column\n",
    "                data_pts['image_date'] = im_date # add image date column\n",
    "                data_pts['site_name'] = site_name # add site name column\n",
    "                data_pts = data_pts.to_crs('EPSG:4326') # reproject back to WGS84 for compatibility\n",
    "                data_pts_full = pd.concat([data_pts_full, data_pts])\n",
    "    \n",
    "        # Reset dataframe indices\n",
    "        data_pts_full.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        # Plot spectral pairplot for training data\n",
    "        df = data_pts_full\n",
    "        df['Class'] = df['Class'].astype(int)\n",
    "        df = df.sort_values(by='Class')\n",
    "        # Assign labels to each class\n",
    "        df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "        df.loc[df['Class']==2, 'Class'] = 'Shadowed snow'\n",
    "        df.loc[df['Class']==3, 'Class'] = 'Ice'\n",
    "        df.loc[df['Class']==4, 'Class'] = 'Rock'\n",
    "        df.loc[df['Class']==5, 'Class'] = 'Water'\n",
    "        df[feature_cols] = df[feature_cols].astype(float)\n",
    "        # plot\n",
    "        fig = sns.pairplot(df, vars=feature_cols, corner=True, diag_kind='kde', hue='Class', \n",
    "                           palette=colors)\n",
    "        plt.show()\n",
    "    \n",
    "        # Save figure to file\n",
    "        fig_fn = figures_out_path + 'spectral_pairplot_' + dataset + '_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('Spectral pairplot figure saved to file: '+fig_fn)\n",
    "\n",
    "        # Save dataframe to file\n",
    "        data_pts_full = f.reduce_memory_usage(data_pts_full) # Reduce memory usage in df\n",
    "        data_pts_full.to_csv(out_path + training_data_fn, index=False)\n",
    "        print(dataset + ' training data saved to file:' + out_path + training_data_fn)\n",
    "\n",
    "        # Save feature columns\n",
    "        feature_cols_fn = out_path + dataset + '_feature_columns.json'\n",
    "        json.dump(feature_cols, open(feature_cols_fn, \"w\"))\n",
    "        print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Rename data points dataframe according to dataset\n",
    "    if dataset=='PlanetScope':\n",
    "        data_pts_full_PS = data_pts_full\n",
    "        feature_cols_PS = feature_cols\n",
    "    elif dataset=='Landsat':\n",
    "        data_pts_full_L = data_pts_full\n",
    "        feature_cols_L = feature_cols\n",
    "    elif dataset=='Sentinel-2_SR':\n",
    "        data_pts_full_S2_SR = data_pts_full\n",
    "        feature_cols_S2_SR = feature_cols\n",
    "    elif dataset=='Sentinel-2_TOA':\n",
    "        data_pts_full_S2_TOA = data_pts_full\n",
    "        feature_cols_S2_TOA = feature_cols\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c9544e",
   "metadata": {},
   "source": [
    "## 2. Develop supervised classifiers for each dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae3cd23",
   "metadata": {},
   "source": [
    "### Define supervised classification algorithms to test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc990c11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Classifier names\n",
    "names = [\n",
    "    \"Decision Tree\",\n",
    "    \"Random Forest\",\n",
    "    \"Neural Network\",\n",
    "    \"Ada Boost\",\n",
    "    \"Naive Bayes\",\n",
    "    \"QDA\",\n",
    "    \"Logistic Regression\",\n",
    "    \"Nearest Neighbors\",\n",
    "    \"Support Vector Machine\",\n",
    "\n",
    "]\n",
    "\n",
    "# -----Classifiers\n",
    "classifiers = [\n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1, max_iter=1000),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis(),\n",
    "    LogisticRegression(random_state = 0, max_iter = 1000),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(gamma=2, C=1),\n",
    "]\n",
    "\n",
    "# -----Define number of folds to use in K-folds cross-validation\n",
    "num_folds = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816cf50e",
   "metadata": {},
   "source": [
    "### Use classified points at each site to determine the best classifiers for EACH site and for ALL sites using K-folds cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a163f644",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "save_outputs = False\n",
    "\n",
    "# -----Loop through datasets\n",
    "for dataset in datasets:\n",
    "    \n",
    "    print('----------')\n",
    "    print(dataset)\n",
    "    print('----------')\n",
    "    \n",
    "    # Define variables and dataset prefix to use in file names based on dataset\n",
    "    if dataset=='PlanetScope':\n",
    "        data_pts_full = data_pts_full_PS.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_PS\n",
    "    elif dataset=='Landsat':\n",
    "        data_pts_full = data_pts_full_L.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_L\n",
    "    elif dataset=='Sentinel-2_SR':\n",
    "        data_pts_full = data_pts_full_S2_SR.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_S2_SR\n",
    "    elif dataset=='Sentinel-2_TOA':\n",
    "        data_pts_full = data_pts_full_S2_TOA.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_S2_TOA\n",
    "        \n",
    "    # Reformat string datatypes in dataframes\n",
    "    data_pts_full['geometry'] = data_pts_full['geometry'].apply(wkt.loads)\n",
    "        \n",
    "    # -----Test one classifier for all sites\n",
    "    print('Testing one classifier for ALL sites...')\n",
    "    # data_pts_full[feature_cols] = data_pts_full[feature_cols].astype(float)\n",
    "    data_pts_full = data_pts_full.dropna().reset_index(drop=True)\n",
    "    X = data_pts_full[feature_cols].astype(float) # features\n",
    "    y = data_pts_full['Class'].astype(int) # labels\n",
    "    \n",
    "    # Iterate over classifiers\n",
    "    num_folds = 10\n",
    "    accuracy = np.zeros(len(classifiers)) # mean accuracy\n",
    "    K = np.zeros(len(classifiers)) # mean Kappa score\n",
    "    CM = np.zeros((4, 4, len(classifiers))) # confusion matrix\n",
    "    j=0\n",
    "    for name, clf in zip(names, classifiers):\n",
    "\n",
    "        print(name)\n",
    "\n",
    "        # Conduct K-Fold cross-validation\n",
    "        kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "        accuracy_folds = np.zeros(num_folds) # accuracy for all simulations\n",
    "        K_folds = np.zeros(num_folds) # kappa score for all MC simulations\n",
    "        # CM_folds = np.zeros((4, 4, num_folds)) # confusion matrix for all folds\n",
    "        # enumerate the splits and summarize the distributions\n",
    "        k=0\n",
    "        for train_ix, test_ix in kfold.split(X):\n",
    "\n",
    "            # select rows\n",
    "            X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "            y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "            # Train classifier\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            # Predict class values using trained classifier\n",
    "            y_pred = clf.predict(X_test)\n",
    "\n",
    "            # Calculate overall accuracy\n",
    "            accuracy_folds[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "            # Calculate Kappa score\n",
    "            K_folds[k] = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "            \n",
    "            k+=1\n",
    "\n",
    "        # Calculate mean accuracy and Kappa score\n",
    "        accuracy[j] = np.nanmean(accuracy_folds)\n",
    "        K[j] = np.nanmean(K_folds)\n",
    "        \n",
    "        # Determine feature importance using Random Forest model\n",
    "        if name==\"Random Forest\":\n",
    "            importances = clf.feature_importances_\n",
    "            # print('    Feature importances:')\n",
    "            # [print('   ', x, y) for x, y in zip(feature_cols, importances)]\n",
    "            fig, ax = plt.subplots(1,1, figsize=(8,6))\n",
    "            bars = ax.bar(np.arange(len(feature_cols)), importances)\n",
    "            ax.bar_label(bars)\n",
    "            ax.set_xticks(np.arange(len(feature_cols)))\n",
    "            ax.set_xticklabels(feature_cols)\n",
    "            ax.set_xlabel('Features')\n",
    "            ax.set_ylabel('Importance')\n",
    "            ax.set_title(dataset+' Random Forest model feature importances')\n",
    "            plt.show()\n",
    "            fig.savefig(figures_out_path + 'RandomForest_feature_importances_'+dataset+'.png', \n",
    "                     dpi=300, facecolor='w')\n",
    "            print('    figure saved to file')\n",
    "        j+=1\n",
    "\n",
    "    # -----Determine best classifier based on Kappa score\n",
    "    results = pd.DataFrame()\n",
    "    results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "    clf_best_name = names[np.where(K==np.max(K))[0][0]]\n",
    "    clf_best = classifiers[np.where(K==np.max(K))[0][0]]\n",
    "    print(results)\n",
    "    print('')\n",
    "    print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "    # -----Save most accurate classifier\n",
    "    if save_outputs:\n",
    "        clf_fn = out_path + dataset + '_classifier_all_sites.joblib'\n",
    "        dump(clf_best, clf_fn)\n",
    "        print('Most accurate classifier saved to file: ',clf_fn)\n",
    "    \n",
    "    # -----Print confusion matrix\n",
    "    # CM_copy = pd.DataFrame(columns=['Snow', 'Shadowed_snow', 'Ice', 'Bare_ground', 'Water', 'All_snow'])\n",
    "    # CM_copy['Snow'] = CM[:,0,0]\n",
    "    # CM_copy['Shadowed_snow'] = CM[:,1,0]\n",
    "    # CM_copy['Ice'] = CM[:,2,0]\n",
    "    # CM_copy['Bare_ground'] = CM[:,3,0]\n",
    "    # CM_copy['Water'] = CM[:,4,0]\n",
    "    # CM_copy['All_snow'] = CM_copy['Snow'] + CM_copy['Shadowed_snow']\n",
    "    # CM_copy = CM_copy.drop(columns=['Snow', 'Shadowed_snow'])\n",
    "    # print(CM_copy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48066ae8",
   "metadata": {},
   "source": [
    "## 3. *Optional*\n",
    "\n",
    "### a. Test how the number of training points impacts model accuracies (i.e., calculate learning curves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97160a89",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve, ShuffleSplit\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Define scoring function\n",
    "scorer = make_scorer(metrics.accuracy_score)  \n",
    "\n",
    "# Create a function to plot learning curves\n",
    "def plot_learning_curve(estimator, estimator_name, X, y, cv=None, axis=None, color='k',\n",
    "                        label='_nolegend_', n_jobs=None, train_sizes=np.linspace(500, 6.5e3, 7).astype(int)):\n",
    "\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, scoring=scorer, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    \n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    axis.plot(train_sizes, test_scores_mean, 'o-', color=color, label=label)\n",
    "    axis.grid()\n",
    "\n",
    "    return plt\n",
    "\n",
    "# -----Set up figure\n",
    "plt.rcParams.update({'font.size':12, 'font.sans-serif':'Arial'})\n",
    "fig, ax = plt.subplots(2, 2, figsize=(12,8))\n",
    "plt.subplots_adjust(right=0.78)\n",
    "ax = ax.flatten()\n",
    "# define colors for plotting results for each model\n",
    "colors = ['#a6cee3', '#1f78b4', '#b2df8a', '#33a02c', '#fb9a99', \n",
    "          '#e31a1c', '#fdbf6f', '#ff7f00', '#6a3d9a']\n",
    "# set up titles\n",
    "text_labels = ['(a) ', '(b) ', '(c) ', '(d) ']\n",
    "\n",
    "# -----Iterate over datasets\n",
    "for i, dataset in enumerate(datasets):\n",
    "    \n",
    "    print(dataset)\n",
    "    print('----------')\n",
    "    \n",
    "    # Define variables and dataset prefix to use in file names based on dataset\n",
    "    if dataset=='PlanetScope':\n",
    "        data_pts_full = data_pts_full_PS.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_PS\n",
    "        train_sizes = np.linspace(1e3, 7e3, 4).astype(int)\n",
    "    elif dataset=='Landsat':\n",
    "        data_pts_full = data_pts_full_L.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_L\n",
    "        train_sizes = np.linspace(1e3, 6e3, 6).astype(int)\n",
    "    elif dataset=='Sentinel-2_SR':\n",
    "        data_pts_full = data_pts_full_S2_SR.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_S2_SR\n",
    "        train_sizes = np.linspace(1e3, 9e3, 5).astype(int)\n",
    "    elif dataset=='Sentinel-2_TOA':\n",
    "        data_pts_full = data_pts_full_S2_TOA.dropna().reset_index(drop=True)\n",
    "        feature_cols = feature_cols_S2_TOA\n",
    "        train_sizes = np.linspace(1e3, 9e3, 5).astype(int)\n",
    "\n",
    "    # Iterate over models\n",
    "    for name, clf, color in zip(names, classifiers, colors):\n",
    "\n",
    "        # Split training data into X and y\n",
    "        X = data_pts_full[feature_cols].astype(float) # features\n",
    "        y = data_pts_full['Class'].astype(int) # labels        \n",
    "        \n",
    "        # Plot learning curves\n",
    "        cv = ShuffleSplit(n_splits=10, test_size=0.2, random_state=0)  \n",
    "        plot_learning_curve(clf, name, X, y, cv=cv, axis=ax[i], color=color, label=name, n_jobs=-1)\n",
    "        \n",
    "    # adjust axes\n",
    "    if i >=2:\n",
    "        ax[i].set_xlabel('Number of training samples')\n",
    "    if (i==0) or (i==2):\n",
    "        ax[i].set_ylabel('Accuracy score')\n",
    "    ax[i].set_title(text_labels[i] + dataset.replace('_',' '))\n",
    "    ax[i].set_yticks([0.6, 0.7, 0.8, 0.9, 1.0])\n",
    "    ax[i].set_ylim(0.6, 1)\n",
    "\n",
    "    print(' ')\n",
    "\n",
    "handles, labels = ax[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='center right')        \n",
    "plt.show()\n",
    "\n",
    "# save figure\n",
    "fig_fn = os.path.join(base_path, 'figures', 'figS2_classifiers_learning_curves.png')\n",
    "fig.savefig(fig_fn, dpi=300, bbox_inches='tight')\n",
    "print('figure saved to file: ' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25b131b-08fa-4133-81b9-c67c2bdbd9d8",
   "metadata": {},
   "source": [
    "### b. MODIS (not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2debb56",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset='MODIS'\n",
    "ds_dict = dataset_dict[dataset]\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "# MODIS bands: \n",
    "# sur_refl_b01=Red, sur_refl_b04=Green, sur_refl_b03=Blue, sur_refl_b05=SWIR1, \n",
    "# sur_refl_b06=SWIR2, sur_refl_b07=SWIR3\n",
    "band_names = [band for band in ds_dict['bands'] if 'qc' not in band]\n",
    "feature_cols_M = [band for band in band_names] + ['NDSI']\n",
    "# buffer used for clipping images\n",
    "buffer = 3000 # [m]\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "M_training_data_fn = 'MODIS_training_data.pkl'\n",
    "if os.path.exists(out_path + M_training_data_fn):\n",
    "    \n",
    "    data_pts_full_M = pd.read_pickle(out_path + M_training_data_fn)\n",
    "    print('MODIS training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_M = data_pts_full_PS.copy(deep=True)\n",
    "    data_pts_full_M = data_pts_full_M.drop(columns=['blue', 'red', 'green', 'NIR', 'NDSI'])\n",
    "    data_pts_full_M[band_names] = \" \" # initialize band columns\n",
    "\n",
    "    # Loop through sites\n",
    "    for i, site_name in enumerate(site_names):\n",
    "    \n",
    "        print('----------')\n",
    "        print(site_name)\n",
    "        print('----------')\n",
    "\n",
    "        # Extract image dates from data point file names\n",
    "        im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "        # Load AOI\n",
    "        AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "        AOI_fn = glob.glob(AOI_fn)[0]\n",
    "        AOI = gpd.read_file(AOI_fn)\n",
    "        # reproject AOI to WGS 84 for compatibility with images\n",
    "        AOI_WGS = AOI.to_crs(4326)\n",
    "        # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "        AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                                [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                                  [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                                ])\n",
    "\n",
    "        # Load images from Earth Engine\n",
    "        if site_name=='Gulkana':\n",
    "            im1_fn = '2021_06_15'\n",
    "            im2_fn = '2021_08_06'\n",
    "        elif site_name=='SouthCascade':\n",
    "            im1_fn= '2021_07_03'\n",
    "            im2_fn = '2021_08_28'\n",
    "        elif site_name=='Sperry':\n",
    "            im1_fn = '2021_06_27'\n",
    "            im2_fn = '2021_08_01'\n",
    "        elif site_name=='Wolverine':\n",
    "            im1_fn = '2021_06_15'\n",
    "            im2_fn = '2021_08_15'\n",
    "        im1, im2 = ee.Image('MODIS/061/MOD09GA/'+im1_fn), ee.Image('MODIS/061/MOD09GA/'+im2_fn)\n",
    "    \n",
    "        # Clip images and select bands\n",
    "        im1_clip = im1.clip(AOI_WGS_bb_ee.buffer(buffer)).select(band_names)\n",
    "        im2_clip = im2.clip(AOI_WGS_bb_ee.buffer(buffer)).select(band_names)\n",
    "        # Convert images to xarray Datasets\n",
    "        im1_xr = im1_clip.wx.to_xarray(scale=ds_dict['resolution_m'], crs='EPSG:4326')\n",
    "        im2_xr = im2_clip.wx.to_xarray(scale=ds_dict['resolution_m'], crs='EPSG:4326')\n",
    "        # Determine optimal UTM zone EPSG code\n",
    "        epsg_UTM = f.convert_wgs_to_utm((AOI_WGS.geometry.bounds.maxx[0] - AOI_WGS.geometry.bounds.minx[0]) + AOI_WGS.geometry.bounds.minx[0],\n",
    "                                         (AOI_WGS.geometry.bounds.maxy[0] - AOI_WGS.geometry.bounds.miny[0]) + AOI_WGS.geometry.bounds.miny[0])\n",
    "        # Reproject to UTM\n",
    "        im1_xr = im1_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        im2_xr = im2_xr.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # Replace no data values with NaN, account for image scalar\n",
    "        im1_xr = xr.where((im1_xr!=ds_dict['no_data_value']) &  (im1_xr > 0), \n",
    "                          im1_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        im2_xr = xr.where((im2_xr!=ds_dict['no_data_value']) & (im2_xr > 0), \n",
    "                          im2_xr / ds_dict['SR_scalar'], \n",
    "                          np.nan)\n",
    "        # Create list of images\n",
    "        im_list = [im1_xr, im2_xr]\n",
    "        \n",
    "        # Loop through image dates\n",
    "        for j, im_date in enumerate(im_dates):\n",
    "\n",
    "            im = im_list[j]\n",
    "            \n",
    "            # select df columns for study site and image date\n",
    "            data_pts = data_pts_full_M.loc[(data_pts_full_M['site_name']==site_name) \n",
    "                                            & (data_pts_full_M['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "            # reproject to UTM\n",
    "            data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "            # grab x and y coordinates for data points at the site\n",
    "            data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                          for i in np.arange(0,len(data_pts))]\n",
    "            \n",
    "            # extract band values at data points \n",
    "            for band_name in band_names:\n",
    "                data_pts[band_name] = [im.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                       for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "            # plot images and data points\n",
    "            fig1, ax1 = plt.subplots(1, 1, figsize=(6,6))\n",
    "            ax1.imshow(np.dstack([im[ds_dict['RGB_bands'][0]].data[0],\n",
    "                                  im[ds_dict['RGB_bands'][1]].data[0],\n",
    "                                  im[ds_dict['RGB_bands'][2]].data[0]]),\n",
    "                      extent=(np.min(im.x.data)/1e3, np.max(im.x.data)/1e3, \n",
    "                              np.min(im.y.data)/1e3, np.max(im.y.data)/1e3))\n",
    "            # ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "            #             [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=2)\n",
    "            ax1.plot([x/1e3 for x in data_pts_x], [y/1e3 for y in data_pts_y], '.m', markersize=5)\n",
    "            ax1.set_xlabel('Easting [km]')\n",
    "            ax1.set_ylabel('Northing [km]')\n",
    "            plt.show()\n",
    "\n",
    "            # add data_pts back to full df\n",
    "            data_pts_full_M.loc[(data_pts_full_M['site_name']==site_name) \n",
    "                                 & (data_pts_full_M['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "    # Add NDSI column\n",
    "    data_pts_full_M['NDSI'] = ((data_pts_full_M[ds_dict['NDSI'][0]] - data_pts_full_M[ds_dict['NDSI'][1]]) / \n",
    "                                (data_pts_full_M[ds_dict['NDSI'][0]] + data_pts_full_M[ds_dict['NDSI'][1]]))\n",
    "    \n",
    "    # Remove no data points\n",
    "    data_pts_full_M = data_pts_full_M.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # don't use shadowed snow (where class==shadowed snow, make it snow)\n",
    "    data_pts_full_M.loc[data_pts_full_M['class']==2, 'class'] = 1\n",
    "    \n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_M = f.reduce_memory_usage(data_pts_full_M)\n",
    "    \n",
    "    # Save training data to file\n",
    "    data_pts_full_M.to_pickle(out_path + M_training_data_fn)\n",
    "    print('MODIS training data saved to file:' + out_path + M_training_data_fn)\n",
    "    \n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'MODIS_feature_columns.pkl'\n",
    "    pickle.dump(feature_cols_M, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: ', feature_cols_fn)\n",
    "    \n",
    "    # Plot spectral pairplot for training data\n",
    "    df = data_pts_full_M\n",
    "    df = df.sort_values(by='class')\n",
    "    df['Class'] = df['class'].astype(object)\n",
    "    # Assign labels to each class\n",
    "    df.loc[df['Class']==1, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==2, 'Class'] = 'Snow'\n",
    "    df.loc[df['Class']==3, 'Class'] = 'Ice'\n",
    "    df.loc[df['Class']==4, 'Class'] = 'Bare rock'\n",
    "    df.loc[df['Class']==5, 'Class'] = 'Water'\n",
    "    # Create colormap\n",
    "    color_snow = '#4eb3d3'\n",
    "    color_ice = '#084081'\n",
    "    color_rock = '#fdbb84'\n",
    "    color_water = '#bdbdbd'\n",
    "    color_contour = '#f768a1'\n",
    "    colors = [color_snow, color_ice, color_rock, color_water]\n",
    "    # plot\n",
    "    fig = sns.pairplot(df[['Class'] + feature_cols_M], corner=True, diag_kind='kde', hue='Class', palette=colors)\n",
    "    plt.show()\n",
    "    # save figure\n",
    "    if save_figures:\n",
    "        fig_fn = base_path + 'figures/spectral_pairplot_M_training_data.png'\n",
    "        fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "        print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0f3585",
   "metadata": {},
   "source": [
    "### c. Sentinel-1 (not recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880fa6b5",
   "metadata": {},
   "source": [
    "#### Set-up training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e92b2c19",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = 'Sentinel-1'\n",
    "\n",
    "# -----Add path to preprocessing functions\n",
    "# path to gee_s1_ard/python-api/\n",
    "ard_path = base_path + '../gee_s1_ard/python-api/'\n",
    "# add functions to path\n",
    "sys.path.insert(1, ard_path)\n",
    "import wrapper as w\n",
    "\n",
    "# -----Define bands and feature columns (predictors used in classification)\n",
    "band_names = ['VV', 'VH', 'angle']\n",
    "feature_cols_S1 = band_names + ['VHVV', 'VV-VH']\n",
    "# buffer used for clipping images\n",
    "buffer = 2000 # [m]\n",
    "\n",
    "# -----Load classified points\n",
    "os.chdir(data_pts_path)\n",
    "data_pts_fns = glob.glob('*.shp')\n",
    "data_pts_fns.sort()\n",
    "\n",
    "# -----Check if training data exist in file\n",
    "S1_training_data_fn = 'S1_training_data.pkl'\n",
    "if os.path.exists(out_path + S1_training_data_fn):\n",
    "    \n",
    "    data_pts_full_L = pd.read_pickle(out_path + S1_training_data_fn)\n",
    "    print('Sentinel-1 training data already exist... loaded from file.')\n",
    "    \n",
    "else: \n",
    "    \n",
    "    # Initialize full data points dataframe (for use in next step)\n",
    "    data_pts_full_S1 = data_pts_full_PS.copy(deep=True)\n",
    "    # remove PS bands\n",
    "    data_pts_full_S1 = data_pts_full_S1.drop(columns=['blue', 'green', 'red', 'NIR', 'NDSI'])\n",
    "    # initialize band columns\n",
    "    data_pts_full_S1['S1_im_date'] = ' '\n",
    "    data_pts_full_S1[feature_cols_S1] = 0\n",
    "\n",
    "    # Loop through sites\n",
    "    # for i, site_name in enumerate(site_names):\n",
    "    site_name = 'SouthCascade'\n",
    "    \n",
    "    print('----------')\n",
    "    print(site_name)\n",
    "    print('----------')\n",
    "\n",
    "    # Extract image dates from data point file names\n",
    "    im_dates = [s[len(site_name)+1:len(site_name)+9] for s in data_pts_fns if (site_name in s) and ('snow.shp' in s)]\n",
    "\n",
    "    # Load AOI\n",
    "    AOI_fn = AOI_path + site_name + '/glacier_outlines/' + site_name + '_USGS_*.shp'\n",
    "    AOI_fn = glob.glob(AOI_fn)[0]\n",
    "    AOI = gpd.read_file(AOI_fn)\n",
    "    # reproject AOI to WGS 84 for compatibility with images\n",
    "    AOI_WGS = AOI.to_crs(4326)\n",
    "    # reformat AOI_WGS bounding box as ee.Geometry for clipping images\n",
    "    AOI_WGS_bb_ee = ee.Geometry.Polygon(\n",
    "                            [[[AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                              [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.miny[0]],\n",
    "                              [AOI_WGS.geometry.bounds.maxx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                              [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.maxy[0]],\n",
    "                              [AOI_WGS.geometry.bounds.minx[0], AOI_WGS.geometry.bounds.miny[0]]]\n",
    "                            ]).buffer(3000)\n",
    "    # solve for optimal UTM zone for reprojection\n",
    "    AOI_WGS_centroid = [AOI_WGS.geometry[0].centroid.xy[0][0],\n",
    "                        AOI_WGS.geometry[0].centroid.xy[1][0]]\n",
    "    epsg_UTM = f.convert_wgs_to_utm(AOI_WGS_centroid[0], AOI_WGS_centroid[1])\n",
    "        \n",
    "        # # Load images from Earth Engine\n",
    "        # if site_name=='Gulkana':\n",
    "        #     im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067016_20210610'\n",
    "        #     im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_068016_20210804'\n",
    "    if site_name=='SouthCascade':\n",
    "        im1_date_range = ['2021-07-01', '2021-07-03']\n",
    "        im2_date_range = ['2021-08-27', '2021-08-29']\n",
    "        # elif site_name=='Sperry':\n",
    "            # im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_041026_20210706'\n",
    "            # im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_041026_20210722'\n",
    "        # elif site_name=='Wolverine':\n",
    "            # im1_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067018_20220629'\n",
    "            # im2_fn = 'LANDSAT/LC08/C02/T1_L2/LC08_067018_20210829'        \n",
    "    date_ranges = [im1_date_range, im2_date_range]\n",
    "\n",
    "    # loop through images\n",
    "    for j in [0, 1]:\n",
    "\n",
    "        # Define dictionary of parameters\n",
    "        params = {'APPLY_BORDER_NOISE_CORRECTION': True,\n",
    "                  'APPLY_TERRAIN_FLATTENING': True,\n",
    "                  'APPLY_SPECKLE_FILTERING': True, \n",
    "                  'POLARIZATION': 'VVVH',\n",
    "                  'PLATFORM_NUMBER': None,\n",
    "                  'ORBIT': None, \n",
    "                  'ORBIT_NUM': None, \n",
    "                  'SPECKLE_FILTER_FRAMEWORK': 'MULTI',\n",
    "                  'SPECKLE_FILTER': 'LEE',\n",
    "                  'SPECKLE_FILTER_KERNEL_SIZE': 9,\n",
    "                  'SPECKLE_FILTER_NR_OF_IMAGES': 10,\n",
    "                  'APPLY_TERRAIN_FLATTENING': True,\n",
    "                  'DEM': ee.Image(\"NASA/ASTER_GED/AG100_003\"),\n",
    "                  'TERRAIN_FLATTENING_MODEL': 'VOLUME',\n",
    "                  'TERRAIN_FLATTENING_ADDITIONAL_LAYOVER_SHADOW_BUFFER': 0,\n",
    "                  'FORMAT' : 'DB',\n",
    "                  'CLIP_TO_ROI': True,\n",
    "                  'SAVE_ASSET': False,\n",
    "                  'ASSET_ID': None,\n",
    "                  'START_DATE': date_ranges[j][0],\n",
    "                  'STOP_DATE': date_ranges[j][1],\n",
    "                  'ROI': AOI_WGS_bb_ee\n",
    "                 }\n",
    "\n",
    "        # run the gee_s1_ard wrapper\n",
    "        im = w.s1_preproc(params)\n",
    "        \n",
    "        im_date = im_dates[j]\n",
    "            \n",
    "        # Convert ee.ImageCollection to xarray.Dataset\n",
    "        im_ds = im.wx.to_xarray(scale=10, crs='EPSG:4326')\n",
    "        # reproject to UTM\n",
    "        im_ds = im_ds.rio.reproject('EPSG:'+epsg_UTM)\n",
    "        # replace no data values with NaN\n",
    "        im_ds = im_ds.where(im_ds!=-32768)\n",
    "        \n",
    "        # select df columns for study site and image date\n",
    "        data_pts = data_pts_full_S1.loc[(data_pts_full_S1['site_name']==site_name) \n",
    "                                        & (data_pts_full_S1['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])]\n",
    "        # add image date\n",
    "        data_pts['S1_im_date'] = str(im_ds.time.data[0])[0:10]\n",
    "        # reproject to UTM\n",
    "        data_pts = data_pts.to_crs(epsg_UTM)\n",
    "            \n",
    "        # grab x and y coordinates for data points at the site\n",
    "        data_pts_x = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].x\n",
    "                      for i in np.arange(0,len(data_pts))]\n",
    "        data_pts_y = [data_pts['geometry'].reset_index(drop=True)[i].geoms[0].y\n",
    "                      for i in np.arange(0,len(data_pts))]\n",
    "        # extract band values at data points \n",
    "        for band_name in band_names:\n",
    "            data_pts[band_name] = [im_ds.sel(x=x, y=y, method=\"nearest\")[band_name].data[0] \n",
    "                                   for x, y in list(zip(data_pts_x, data_pts_y))]\n",
    "\n",
    "        # add data_pts back to full df\n",
    "        data_pts_full_S1.loc[(data_pts_full_S1['site_name']==site_name) \n",
    "                             & (data_pts_full_S1['PS_im_date']==im_date[0:4]+'-'+im_date[4:6]+'-'+im_date[6:8])] = data_pts\n",
    "\n",
    "        # plot images and data points\n",
    "        fig1, ax1 = plt.subplots(1, 1, figsize=(10,10))\n",
    "        ax1.imshow(im_ds['VV'].data[0], cmap='Greys',\n",
    "                   extent=(np.min(im_ds.x.data)/1e3, np.max(im_ds.x.data)/1e3, \n",
    "                           np.min(im_ds.y.data)/1e3, np.max(im_ds.y.data)/1e3))\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==1]], c='cyan', s=1)\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==3]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==3]], c='blue', s=1)\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==4]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==4]], c='orange', s=1)\n",
    "        ax1.scatter([x.geoms[0].x/1e3 for x in data_pts['geometry'].loc[data_pts['class']==5]], \n",
    "                    [x.geoms[0].y/1e3 for x in data_pts['geometry'].loc[data_pts['class']==5]], c='grey', s=1)\n",
    "        ax1.set_xlabel('Easting [km]')\n",
    "        ax1.set_ylabel('Northing [km]')\n",
    "        plt.show()\n",
    "\n",
    "    # Remove no data points\n",
    "    data_pts_full_S1 = data_pts_full_S1.dropna().reset_index(drop=True)\n",
    "    \n",
    "    # Add VH/VV and VV-VH columns\n",
    "    data_pts_full_S1['VHVV'] = data_pts_full_S1['VH'] / data_pts_full_S1['VV']\n",
    "    data_pts_full_S1['VV-VH'] = data_pts_full_S1['VV'] - data_pts_full_S1['VH']\n",
    "\n",
    "    # Reduce memory usage in df\n",
    "    data_pts_full_S1 = f.reduce_memory_usage(data_pts_full_S1)\n",
    "\n",
    "    # Save training data to file\n",
    "    data_pts_full_S1.to_pickle(out_path + S1_training_data_fn)\n",
    "    print('Landsat training data saved to file:' + out_path + S1_training_data_fn)\n",
    "\n",
    "    # Save feature columns\n",
    "    feature_cols_fn = out_path + 'Sentinel-1_feature_columns.pkl'\n",
    "    pickle.dump(feature_cols_S1, open(feature_cols_fn, 'wb'))\n",
    "    print('Feature columns saved to file: '+ feature_cols_fn)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1043eb63",
   "metadata": {},
   "source": [
    "#### Plot pairplot of training data spectral characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95bfef7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_pts_full_S1['class'] = data_pts_full_S1['class'].astype(object)\n",
    "fig = sns.pairplot(data_pts_full_S1[['class'] + feature_cols_S1], markers='.',  \n",
    "             corner=True, diag_kind='kde', hue='class', palette=\"colorblind\");\n",
    "# save figure\n",
    "if save_figures:\n",
    "    fig_fn = base_path + 'figures/spectral_pairplot_Sentinel1.png'\n",
    "    fig.savefig(fig_fn, facecolor='w', dpi=300)\n",
    "    print('figure saved to file:' + fig_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4295170",
   "metadata": {},
   "source": [
    "#### Test one classifier for _each_ site"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86472b5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Test supervised classification algorithms\n",
    "# Loop through sites\n",
    "# for i, site_name in enumerate(site_names):\n",
    "site_name = 'SouthCascade'\n",
    "\n",
    "print('----------')\n",
    "print(site_name)\n",
    "print('----------')\n",
    "\n",
    "# Select all columns in data_pts_full_PS for site\n",
    "data_pts = data_pts_full_S1.loc[data_pts_full_S1['site_name']==site_name]\n",
    "data_pts = data_pts.reset_index(drop=True)\n",
    "\n",
    "# Split data points into features (band values / terrain parameters) and target variable (class)\n",
    "X = data_pts[feature_cols_S1] # features\n",
    "y = data_pts['class'] # target variable\n",
    "y = y.astype(int)\n",
    "\n",
    "# Iterate over classifiers\n",
    "accuracy = np.zeros(len(classifiers)) # mean accuracy\n",
    "K = np.zeros(len(classifiers)) # mean Kappa score\n",
    "j=0\n",
    "for name, clf in zip(names, classifiers):\n",
    "\n",
    "    print(name)\n",
    "\n",
    "    # Conduct K-Fold cross-validation\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "    accuracy_folds = np.zeros(num_folds) # accuracy for all simulations\n",
    "    K_folds = np.zeros(num_folds) # kappa score for all MC simulations\n",
    "    k=0 # iteration counter\n",
    "    # enumerate the splits and summarize the distributions\n",
    "    for train_ix, test_ix in kfold.split(X):\n",
    "\n",
    "        # select rows\n",
    "        X_train, X_test = X.loc[train_ix], X.loc[test_ix]\n",
    "        y_train, y_test = y[train_ix], y[test_ix]\n",
    "\n",
    "        # Train classifier\n",
    "        clf.fit(X_train, y_train)\n",
    "\n",
    "        # Predict class values using trained classifier\n",
    "        y_pred = clf.predict(X_test)\n",
    "\n",
    "        # Calculate overall accuracy\n",
    "        accuracy_folds[k] = metrics.accuracy_score(y_test, y_pred)\n",
    "        # Calculate Kappa score\n",
    "        K_folds[k] = metrics.cohen_kappa_score(y_test, y_pred)\n",
    "\n",
    "        k+=1\n",
    "\n",
    "    # Calculate mean accuracy and Kappa score\n",
    "    accuracy[j] = np.nanmean(accuracy_folds)\n",
    "    K[j] = np.nanmean(K_folds)\n",
    "\n",
    "    j+=1\n",
    "\n",
    "# Determine best classifier based on accuracy\n",
    "results = pd.DataFrame()\n",
    "results['Classifier'], results['Accuracy'], results['Kappa_score'] = names, accuracy, K\n",
    "clf_best_name = names[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "clf_best = classifiers[np.where(accuracy==np.max(accuracy))[0][0]]\n",
    "print(results)\n",
    "print('')\n",
    "print('Best accuracy classifier: ' + clf_best_name)\n",
    "\n",
    "# -----Save most accurate classifier\n",
    "if save_outputs==True:\n",
    "    clf_fn = out_path + 'S1_classifier_'+site_names[i]+'.sav'\n",
    "    pickle.dump(clf_best, open(clf_fn, 'wb'))\n",
    "    print('Most accurate classifier saved to file: ',clf_fn)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c059d3",
   "metadata": {},
   "source": [
    "#### Test one classifier for _all_ sites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255e58cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# -----Define image bands\n",
    "bands = [x for x in im_ds.data_vars]\n",
    "# bands = [band for band in bands if (band != 'QA_PIXEL') and ('B' in band)]\n",
    "        \n",
    "im_date = str(im_ds.time.data[0])[0:10]\n",
    "print(im_date)\n",
    "        \n",
    "im_AOI = im_ds\n",
    "\n",
    "# add VHVV and VV-VH columns\n",
    "im_ds['VHVV'] = im_ds['VH'] / im_ds['VV']\n",
    "im_ds['VV-VH'] = im_ds['VV'] - im_ds['VH']\n",
    "\n",
    "# find indices of real numbers (no NaNs allowed in classification)\n",
    "ix = [np.where(np.isnan(im_AOI[band].data), False, True) for band in bands]\n",
    "I_real = np.full(np.shape(im_AOI[bands[0]].data), True)\n",
    "for ixx in ix:\n",
    "    I_real = I_real & ixx\n",
    "            \n",
    "# create df of image band values\n",
    "df = pd.DataFrame(columns=feature_cols_S1)\n",
    "for col in feature_cols_S1:\n",
    "    df[col] = np.ravel(im_AOI[col].data[I_real])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# -----Classify image\n",
    "if len(df)>1:\n",
    "    array_classified = clf.predict(df[feature_cols_S1])\n",
    "else:\n",
    "    print(\"No real values found to classify, skipping...\")\n",
    "    # continue\n",
    "\n",
    "# reshape from flat array to original shape\n",
    "im_classified = np.zeros(im_AOI.to_array().data[0].shape)\n",
    "im_classified[:] = np.nan\n",
    "im_classified[I_real] = array_classified\n",
    "            \n",
    "# -----Plot results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10,6))\n",
    "ax = ax.flatten()\n",
    "# define x and y limits\n",
    "xmin, xmax = np.min(im_ds.x.data)/1e3, np.max(im_ds.x.data)/1e3\n",
    "ymin, ymax = np.min(im_ds.y.data)/1e3, np.max(im_ds.y.data)/1e3\n",
    "# define colors for plotting\n",
    "color_snow = '#4eb3d3'\n",
    "color_ice = '#084081'\n",
    "color_rock = '#fdbb84'\n",
    "color_water = '#bdbdbd'\n",
    "color_contour = '#f768a1'\n",
    "# create colormap\n",
    "colors = [color_snow, color_snow, color_ice, color_rock, color_water]\n",
    "cmp = matplotlib.colors.ListedColormap(colors)\n",
    "# RGB image\n",
    "ax[0].imshow(im_ds['VV'].data[0], cmap='Greys',\n",
    "             extent=(xmin, xmax, ymin, ymax))\n",
    "ax[0].set_xlabel(\"Easting [km]\")\n",
    "ax[0].set_ylabel(\"Northing [km]\")\n",
    "ax[0].set_title('RGB image')\n",
    "# classified image\n",
    "ax[1].imshow(im_classified[0], cmap=cmp, vmin=1, vmax=5,\n",
    "             extent=(np.min(im_AOI.x.data)/1e3, np.max(im_AOI.x.data)/1e3,\n",
    "                     np.min(im_AOI.y.data)/1e3, np.max(im_AOI.y.data)/1e3))\n",
    "# plot dummy points for legend\n",
    "ax[1].scatter(0, 0, color=color_snow, s=50, label='snow')\n",
    "ax[1].scatter(0, 0, color=color_ice, s=50, label='ice')\n",
    "ax[1].scatter(0, 0, color=color_rock, s=50, label='rock')\n",
    "ax[1].scatter(0, 0, color=color_water, s=50, label='water')\n",
    "ax[1].set_title('Classified image')\n",
    "ax[1].set_xlabel('Easting [km]')\n",
    "ax[1].legend(loc='best')\n",
    "# AOI\n",
    "if AOI.geometry[0].geom_type=='MultiPolygon': # loop through geoms if AOI = MultiPolygon\n",
    "    for j, poly in enumerate(AOI.geometry[0].geoms):\n",
    "        # only include legend label for first geom\n",
    "        if j==0:\n",
    "            ax[0].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "        else:\n",
    "            ax[0].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "        ax[1].plot([x/1e3 for x in poly.exterior.coords.xy[0]], [y/1e3 for y in poly.exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "else:\n",
    "    ax[0].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='AOI')\n",
    "    ax[1].plot([x/1e3 for x in AOI.geometry[0].exterior.coords.xy[0]], [y/1e3 for y in AOI.geometry[0].exterior.coords.xy[1]], '-k', linewidth=1, label='_nolegend_')\n",
    "# reset x and y limits\n",
    "ax[0].set_xlim(xmin, xmax)\n",
    "ax[0].set_ylim(ymin, ymax)\n",
    "ax[1].set_xlim(xmin, xmax)\n",
    "ax[1].set_ylim(ymin, ymax)\n",
    "fig.suptitle(im_date)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snow-cover-mapping",
   "language": "python",
   "name": "snow-cover-mapping"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
